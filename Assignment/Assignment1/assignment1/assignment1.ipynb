{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM4513-6513] Assignment 1: Text Classification with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test two text classification systems: \n",
    "\n",
    "- **Task 1:** sentiment analysis, in particular to predict the sentiment of movie review, i.e. positive or negative (binary classification).\n",
    "- **Task 2:** topic classification, to predict whether a news article is about International issues, Sports or Business (multiclass classification).\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using (1) unigrams, bigrams and trigrams to obtain vector representations of documents. Two vector weighting schemes should be tested: (1) raw frequencies (**3 marks; 1 for each ngram type**); (2) tf.idf (**1 marks**). \n",
    "- Binary Logistic Regression classifiers that will be able to accurately classify movie reviews trained with (1) BOW-count (raw frequencies); and (2) BOW-tfidf (tf.idf weighted) for Task 1. \n",
    "- Multiclass Logistic Regression classifiers that will be able to accurately classify news articles trained with (1) BOW-count (raw frequencies); and (2) BOW-tfidf (tf.idf weighted) for Task 2. \n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function for Task 1 (**3 marks**)\n",
    "    - Minimise the Categorical Cross-entropy loss function for Task 2 (**3 marks**)\n",
    "    - Use L2 regularisation (both tasks) (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous validation loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength)?  (**2 marks; 0.5 for each model in each task**).\n",
    "- After training the LR models, plot the learning process (i.e. training and validation loss in each epoch) using a line plot (**1 mark; 0.5 for both BOW-count and BOW-tfidf LR models in each task**) and discuss if your model overfits/underfits/is about right.\n",
    "- Model interpretability by showing the most important features for each class (i.e. most positive/negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!).  If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks; 0.5 for BOW-count and BOW-tfidf LR models respectively in each task**)\n",
    "\n",
    "\n",
    "### Data - Task 1 \n",
    "\n",
    "The data you will use for Task 1 are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "### Data - Task 2\n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc..\n",
    "\n",
    "Please make sure to comment your code. You should also mention if you've used Windows (not recommended) to write and test your code. There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Fri, 20 Mar 2020** and it needs to be submitted via MOLE. Standard departmental penalties for lateness will be applied. We use a range of strategies to detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index), including Turnitin which helps detect plagiarism, so make sure you do not plagiarise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:31:36.292691Z",
     "start_time": "2020-02-15T14:31:35.549108Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import math\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.145788Z",
     "start_time": "2020-02-15T14:17:28.066100Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "data_dev = pd.read_csv(\"./data_sentiment/dev.csv\",header=None, names=['text','label'])\n",
    "data_test = pd.read_csv(\"./data_sentiment/test.csv\",header=None, names=['text','label'])\n",
    "data_tr = pd.read_csv(\"./data_sentiment/train.csv\",header=None, names=['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.900892Z",
     "start_time": "2020-02-15T14:17:28.891221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>note : some may consider portions of the follo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>every once in a while you see a film that is s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i was growing up in 1970s , boys in my sc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the muppet movie is the first , and the best m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  note : some may consider portions of the follo...      1\n",
       "1  note : some may consider portions of the follo...      1\n",
       "2  every once in a while you see a film that is s...      1\n",
       "3  when i was growing up in 1970s , boys in my sc...      1\n",
       "4  the muppet movie is the first , and the best m...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.115577Z",
     "start_time": "2020-02-15T14:17:31.108038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put the raw texts into list\n",
    "# training dataset\n",
    "data_tr_list = data_tr['text'].tolist()\n",
    "data_tr_label = np.array(data_tr['label'])\n",
    "# development dataset\n",
    "data_dev_list = data_dev['text'].tolist()\n",
    "data_dev_label = np.array(data_dev['label'])\n",
    "# test dataset\n",
    "data_test_list = data_test['text'].tolist()\n",
    "data_test_label = np.array(data_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Representation \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (you can keep top N if you encounter memory issues).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.860420Z",
     "start_time": "2020-02-15T14:17:31.855439Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they' 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:33.169090Z",
     "start_time": "2020-02-15T14:17:33.161268Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', stop_words=[], vocab=set()):\n",
    "    # Next it is my part to finish the this part\n",
    "    # Create a new blank list\n",
    "    x = list()\n",
    "    ngram_list = list()\n",
    "    # judge the variable type\n",
    "    if type(x_raw) == type(\"string\"):\n",
    "        x_raw = list([x_raw])\n",
    "    \n",
    "    for x_each in x_raw:\n",
    "        x_findword = re.findall(token_pattern,x_each)\n",
    "        # Now we have obtained the new word list without the stop_word \n",
    "        x_rawall = [word for word in x_findword if word not in stop_words]\n",
    "        store_ngram = list()\n",
    "        for n_g in range(min(ngram_range),max(ngram_range)+1):\n",
    "            for i in range(len(x_rawall)-n_g+1):\n",
    "                if n_g == 1:\n",
    "                    ngramTemp = x_rawall[i]\n",
    "                    x.append(ngramTemp)\n",
    "                    store_ngram.append(ngramTemp)\n",
    "                elif n_g == 2 or n_g == 3:\n",
    "                    ngramTemp = tuple(x_rawall[i:i+n_g])\n",
    "                    x.append(ngramTemp)\n",
    "                    store_ngram.append(ngramTemp)\n",
    "        ngram_list.append(store_ngram)      \n",
    "    # Now we should make the x into a set and \n",
    "    if vocab: \n",
    "        x = set(x) & vocab   \n",
    "        return x\n",
    "    else:\n",
    "        return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:33.680114Z",
     "start_time": "2020-02-15T14:17:33.675339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['great',\n",
       "  'movie',\n",
       "  'watch',\n",
       "  ('great', 'movie'),\n",
       "  ('movie', 'watch'),\n",
       "  ('great', 'movie', 'watch')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ngrams(\"this is a great movie to watch\", \n",
    "               ngram_range=(1,3), \n",
    "               stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:34.278189Z",
     "start_time": "2020-02-15T14:17:34.273722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('great', 'movie'), 'great'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ngrams(\"this is a great movie to watch\", \n",
    "               ngram_range=(1,2), \n",
    "               stop_words=stop_words, \n",
    "               vocab=set(['great',  ('great','movie')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:35.821240Z",
     "start_time": "2020-02-15T14:17:35.814722Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
    "    \n",
    "    # Firstly we should obtain the all the ngram list \n",
    "    # From the previous function this function will choose the putput type due to the input variables\n",
    "    vocab = extract_ngrams(x_raw=X_raw, stop_words=stop_words)\n",
    "    \n",
    "    df = dict() #dict that contains ngrams as keys and their corresponding document frequency as values.\n",
    "    ngram_counts= list() # type is the list and stores the number of words which occur at the text\n",
    "    \n",
    "    for each_ngram in vocab:\n",
    "        for ngram in set(each_ngram):\n",
    "            if ngram not in df.keys():\n",
    "                df[ngram] = 1\n",
    "            else:\n",
    "                df[ngram] +=1\n",
    "\n",
    "    df = dict(sorted(df.items(), key=lambda count:count[1], reverse=True)[:keep_topN])\n",
    "    vocab = [name for name in df.keys()]\n",
    "    \n",
    "    for item in df.keys():\n",
    "        ngram_counts.append(item)\n",
    "    \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.319793Z",
     "start_time": "2020-02-15T14:17:36.836545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "\n",
      "['but', 'one', 'film', 'not', 'all', 'movie', 'out', 'so', 'there', 'like', 'more', 'up', 'about', 'when', 'some', 'if', 'just', 'only', 'into', 'than', 'even', 'their', 'time', 'most', 'no', 'good', 'much', 'him', 'would', 'other', 'get', 'story', 'well', 'will', 'also', 'two', 'after', 'first', 'character', 'make', 'way', 'characters', 'off', 'see', 'very', 'while', 'does', 'any', 'where', 'too', 'little', 'plot', 'because', 'over', 'director', 'had', 'how', 'then', 'best', 'being', 'people', 'doesn', 'really', 'man', 'never', 'life', 'through', 'films', 'here', 'don', 'many', 'another', 'such', 'scene', 'me', 'bad', 'know', 'made', 'scenes', 'my', 'end', 'new', 'go', 'before', 'back', 'makes', 'something', 'great', 'work', 'movies', 'still', 'better', 'now', 'few', 'down', 'seems', 'around', 'every', 're', 'enough']\n",
      "\n",
      "[('but', 1334), ('one', 1247), ('film', 1231), ('not', 1170), ('all', 1117), ('movie', 1095), ('out', 1080), ('so', 1047), ('there', 1046), ('like', 1043)]\n"
     ]
    }
   ],
   "source": [
    "X_tr_raw = data_tr_list\n",
    "vocab, df, ngram_counts = get_vocab(X_tr_raw, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "print(len(vocab))\n",
    "print()\n",
    "print(list(vocab)[:100])\n",
    "print()\n",
    "print(list(df.items())[:10])\n",
    "# print(df.most_common()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.326811Z",
     "start_time": "2020-02-15T14:17:39.322256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'but'), (1, 'one'), (2, 'film'), (3, 'not'), (4, 'all')]\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "# We need to number the each words and store them in a dictionary \n",
    "voca_dict = dict()\n",
    "for i in range(len(vocab)):\n",
    "    voca_dict[i] = vocab[i]\n",
    "\n",
    "print(list(voca_dict.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.213253Z",
     "start_time": "2020-02-15T14:17:39.329147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finish the process of extracting the n-grams for feature\n",
    "tr_feature, tr_df, tr_ngramcount = get_vocab(data_tr_list,ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "dev_feature, dev_df, dev_ngramcount = get_vocab(data_dev_list,ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "test_feature, test_df, test_ngramcount = get_vocab(data_test_list,ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "\n",
    "# The part of extracting the ngram from the extract_ngrams\n",
    "tr_ngram = extract_ngrams(data_tr_list, ngram_range=(1,3), stop_words=stop_words)\n",
    "dev_ngram = extract_ngrams(data_dev_list, ngram_range=(1,3), stop_words=stop_words)\n",
    "test_ngram = extract_ngrams(data_test_list, ngram_range=(1,3), stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.219201Z",
     "start_time": "2020-02-15T14:17:40.215129Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab):\n",
    "    # Now we have obtain all the ngram and the type is list \n",
    "    # Use the nparray to establish a new matrix\n",
    "    X_matrix = np.zeros((len(X_ngram),len(vocab)))\n",
    "    for sid,ngram in enumerate(X_ngram):\n",
    "        for sid_v,ngram_top in enumerate(vocab):\n",
    "            X_matrix[sid][sid_v] = ngram.count(ngram_top)  \n",
    "    return X_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:41.999574Z",
     "start_time": "2020-02-15T14:17:40.376534Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "X_tr_count = vectorise(tr_ngram,tr_feature)\n",
    "X_dev_count = vectorise(dev_ngram,tr_feature)\n",
    "X_test_count = vectorise(test_ngram,tr_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  8., 20.,  4.,  1.,  0.,  1.,  3.,  1.,  0.,  1.,  0.,  1.,\n",
       "         0.,  1.,  1.,  2.,  4.,  2.,  1.,  3.,  6.,  0.,  4.,  1.,  1.,\n",
       "         1.,  0.,  3.,  0.,  0.,  2.,  0.,  1.,  0.,  1.,  0.,  2.,  3.,\n",
       "         0.,  0.,  4.,  1.,  1.,  0.,  3.,  0.,  1.,  1.,  0.],\n",
       "       [ 2.,  5.,  6.,  2.,  4.,  0.,  2.,  3.,  2.,  3.,  3.,  4.,  2.,\n",
       "         0.,  2.,  2.,  0.,  0.,  2.,  3.,  0.,  0.,  2.,  2.,  1.,  1.,\n",
       "         1.,  5.,  1.,  1.,  1.,  2.,  0.,  4.,  1.,  1.,  0.,  0.,  5.,\n",
       "         1.,  2.,  0.,  0.,  1.,  0.,  3.,  1.,  1.,  2.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_count[:2,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.004808Z",
     "start_time": "2020-02-15T14:17:42.001555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.022692Z",
     "start_time": "2020-02-15T14:17:42.012315Z"
    }
   },
   "outputs": [],
   "source": [
    "def idf_function(ngram_list,vocab):\n",
    "\n",
    "    # establish a new dictionary datastructure to store the idf value\n",
    "    idf_dict = dict()\n",
    "    num_N = len(ngram_list)\n",
    "    for sid,voc in enumerate(vocab):\n",
    "        occur_sum = 0\n",
    "        for ngram in ngram_list:\n",
    "            if voc in ngram:\n",
    "                occur_sum +=1\n",
    "        idf_dict[voc]= np.log((num_N+1)/(occur_sum+1))\n",
    "    return idf_dict               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.802265Z",
     "start_time": "2020-02-15T14:17:42.752448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we have obtained the dictionary about the idf value\n",
    "# X_tr_count - the original vector matrix\n",
    "# x_raw - the raw data\n",
    "def tfidf_function(X_count,ngram_list,vocab):\n",
    "    tfidf = list()\n",
    "    # obtain the new idf_dict\n",
    "    idf_dict = idf_function(ngram_list,vocab)\n",
    "    for sid,word in enumerate(idf_dict.keys()):\n",
    "        result = X_count[:,sid] * idf_dict[word]\n",
    "        tfidf.append(result)\n",
    "\n",
    "    X_tfidf = np.transpose(np.asarray(tfidf))\n",
    "    \n",
    "    return X_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_tfidf = tfidf_function(X_count=X_tr_count, ngram_list = tr_ngram, vocab = tr_feature)\n",
    "X_dev_tfidf = tfidf_function(X_count=X_dev_count, ngram_list = dev_ngram, vocab = tr_feature)\n",
    "X_test_tfidf = tfidf_function(X_count=X_test_count, ngram_list = test_ngram, vocab = tr_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09650995, 0.57821999, 0.77128441, 0.35865637, 0.90257957,\n",
       "       0.        , 0.51859946, 0.87090804, 0.58251467, 0.88238033,\n",
       "       0.89101343, 1.26561491, 0.65249265, 0.        , 0.74355542,\n",
       "       0.79812334, 0.        , 0.        , 0.83896302, 1.284617  ,\n",
       "       0.        , 0.        , 0.94143532, 0.99471004, 0.50442219,\n",
       "       0.52834677, 0.55658683, 2.864543  , 0.5895012 , 0.59336967,\n",
       "       0.59985058, 1.20230363, 0.        , 2.45175908, 0.61690017,\n",
       "       0.62087632, 0.        , 0.        , 3.16450256, 0.68108602,\n",
       "       1.36499491, 0.        , 0.        , 0.70535559, 0.        ,\n",
       "       2.17296126, 0.74365188, 0.74515451, 1.50843602, 0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_tfidf[1,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.160661Z",
     "start_time": "2020-02-15T14:17:44.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = 1.0 / (1 + np.exp(-z))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.351292Z",
     "start_time": "2020-02-15T14:17:44.346822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.00669285 0.76852478]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0)) \n",
    "print(sigmoid(np.array([-5., 1.2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.718566Z",
     "start_time": "2020-02-15T14:17:44.715017Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    # Calculate inner product of the matrix \n",
    "    preds_proba = sigmoid(np.dot(X,weights))\n",
    "    \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.002125Z",
     "start_time": "2020-02-15T14:17:44.998668Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    # Calculate the prediction probabilities of X given the weights\n",
    "    proba_value = predict_proba(X,weights)\n",
    "    # Change the value into 1 or 0\n",
    "    proba_value = np.where(proba_value<0.5, 0, 1)\n",
    "    preds_class = proba_value.astype(np.int64)\n",
    "    \n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.455533Z",
     "start_time": "2020-02-15T14:17:45.451475Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_loss(X, Y, weights, alpha=0.00001):\n",
    "    \n",
    "    # Firstly we should calculate the prediction probabilities of X used with weight\n",
    "    y_proba = predict_proba(X,weights)\n",
    "    \n",
    "    # Restrict the value range from the alpha  to  1-alpha\n",
    "    y_proba  = np.clip(y_proba,alpha,1-alpha)\n",
    "    loss = - Y * np.log(y_proba) - (1 - Y) * np.log(1-y_proba)\n",
    "    \n",
    "    # Look the X scope\n",
    "    if len(X.shape)>1:\n",
    "        L2_regularization = (1/len(X))*(alpha/2)*(np.sum(np.square(weights)))\n",
    "    else:\n",
    "        L2_regularization = (alpha/2)*(np.sum(np.square(weights)))\n",
    "    \n",
    "    l = loss + L2_regularization\n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.968510Z",
     "start_time": "2020-02-15T14:17:45.958185Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], loss=\"binary\", lr=0.1, alpha=0.00001, epochs=5, tolerance=0.0001, print_progress=True):\n",
    "    \n",
    "    cur_loss_dev = 1.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    # Obtain the weights\n",
    "    weights = np.zeros(X_tr.shape[1])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        loss_list = list()\n",
    "        seed_number = random.randint(0,100)\n",
    "        np.random.seed(seed_number)\n",
    "        # Shuffle the X_tr and Y_tr  \n",
    "        per_X_tr = np.random.permutation(X_tr)\n",
    "        np.random.seed(seed_number)\n",
    "        per_Y_tr = np.random.permutation(Y_tr)\n",
    "        for j,row in enumerate(per_X_tr):\n",
    "            # Caculate the Binary Loss and store the loss into loss_list\n",
    "            loss_tr = binary_loss(row, per_Y_tr[j],weights, alpha)\n",
    "            loss_list.append(loss_tr)\n",
    "            y_pred = predict_proba(row,weights) \n",
    "            error = y_pred - per_Y_tr[j]\n",
    "            # update weights\n",
    "            weights = weights - lr*error*row\n",
    "        # Obtain the mean\n",
    "        tr_loss_mean = np.mean(loss_list)\n",
    "        training_loss_history.append(tr_loss_mean)\n",
    "        \n",
    "        dev_loss = binary_loss(X_dev, Y_dev, weights, alpha)\n",
    "        dev_loss = sum(dev_loss)/len(dev_loss)\n",
    "        validation_loss_history.append(dev_loss)\n",
    "        print('Epoch: %d' % i, '| Training loss: %f' %tr_loss_mean, '| Validation loss: %f' %dev_loss)\n",
    "        \n",
    "        if (cur_loss_dev-dev_loss)<tolerance:\n",
    "            break\n",
    "            \n",
    "        cur_loss_dev = dev_loss\n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.667690 | Validation loss: 0.647094\n",
      "Epoch: 1 | Training loss: 0.613934 | Validation loss: 0.615373\n",
      "Epoch: 2 | Training loss: 0.575116 | Validation loss: 0.595635\n",
      "Epoch: 3 | Training loss: 0.545252 | Validation loss: 0.575331\n",
      "Epoch: 4 | Training loss: 0.520551 | Validation loss: 0.562030\n",
      "Epoch: 5 | Training loss: 0.499586 | Validation loss: 0.548129\n",
      "Epoch: 6 | Training loss: 0.481713 | Validation loss: 0.538025\n",
      "Epoch: 7 | Training loss: 0.465533 | Validation loss: 0.530481\n",
      "Epoch: 8 | Training loss: 0.451564 | Validation loss: 0.519784\n",
      "Epoch: 9 | Training loss: 0.439090 | Validation loss: 0.512905\n",
      "Epoch: 10 | Training loss: 0.427257 | Validation loss: 0.506482\n",
      "Epoch: 11 | Training loss: 0.416128 | Validation loss: 0.501852\n",
      "Epoch: 12 | Training loss: 0.406814 | Validation loss: 0.494111\n",
      "Epoch: 13 | Training loss: 0.397518 | Validation loss: 0.488875\n",
      "Epoch: 14 | Training loss: 0.388979 | Validation loss: 0.484013\n",
      "Epoch: 15 | Training loss: 0.380997 | Validation loss: 0.480896\n",
      "Epoch: 16 | Training loss: 0.373599 | Validation loss: 0.475360\n",
      "Epoch: 17 | Training loss: 0.366493 | Validation loss: 0.471957\n",
      "Epoch: 18 | Training loss: 0.359441 | Validation loss: 0.469523\n",
      "Epoch: 19 | Training loss: 0.353198 | Validation loss: 0.465293\n",
      "Epoch: 20 | Training loss: 0.347355 | Validation loss: 0.462088\n",
      "Epoch: 21 | Training loss: 0.341564 | Validation loss: 0.457970\n",
      "Epoch: 22 | Training loss: 0.336115 | Validation loss: 0.455706\n",
      "Epoch: 23 | Training loss: 0.330757 | Validation loss: 0.453449\n",
      "Epoch: 24 | Training loss: 0.325950 | Validation loss: 0.450681\n",
      "Epoch: 25 | Training loss: 0.320996 | Validation loss: 0.448717\n",
      "Epoch: 26 | Training loss: 0.316457 | Validation loss: 0.445406\n",
      "Epoch: 27 | Training loss: 0.311947 | Validation loss: 0.442609\n",
      "Epoch: 28 | Training loss: 0.307677 | Validation loss: 0.441136\n",
      "Epoch: 29 | Training loss: 0.303613 | Validation loss: 0.439073\n",
      "Epoch: 30 | Training loss: 0.299752 | Validation loss: 0.436814\n",
      "Epoch: 31 | Training loss: 0.295824 | Validation loss: 0.434723\n",
      "Epoch: 32 | Training loss: 0.291876 | Validation loss: 0.433425\n",
      "Epoch: 33 | Training loss: 0.288540 | Validation loss: 0.431482\n",
      "Epoch: 34 | Training loss: 0.285090 | Validation loss: 0.429750\n",
      "Epoch: 35 | Training loss: 0.281504 | Validation loss: 0.429179\n",
      "Epoch: 36 | Training loss: 0.278308 | Validation loss: 0.426556\n",
      "Epoch: 37 | Training loss: 0.275335 | Validation loss: 0.425370\n",
      "Epoch: 38 | Training loss: 0.272124 | Validation loss: 0.424008\n",
      "Epoch: 39 | Training loss: 0.269183 | Validation loss: 0.423003\n",
      "Epoch: 40 | Training loss: 0.266316 | Validation loss: 0.421179\n",
      "Epoch: 41 | Training loss: 0.263219 | Validation loss: 0.420743\n",
      "Epoch: 42 | Training loss: 0.260617 | Validation loss: 0.418752\n",
      "Epoch: 43 | Training loss: 0.257886 | Validation loss: 0.417834\n",
      "Epoch: 44 | Training loss: 0.255196 | Validation loss: 0.416404\n",
      "Epoch: 45 | Training loss: 0.252851 | Validation loss: 0.415398\n",
      "Epoch: 46 | Training loss: 0.250213 | Validation loss: 0.415025\n",
      "Epoch: 47 | Training loss: 0.247883 | Validation loss: 0.413822\n",
      "Epoch: 48 | Training loss: 0.245570 | Validation loss: 0.412777\n",
      "Epoch: 49 | Training loss: 0.243239 | Validation loss: 0.411473\n",
      "Epoch: 50 | Training loss: 0.240952 | Validation loss: 0.410816\n",
      "Epoch: 51 | Training loss: 0.238663 | Validation loss: 0.409894\n",
      "Epoch: 52 | Training loss: 0.236534 | Validation loss: 0.408841\n",
      "Epoch: 53 | Training loss: 0.234449 | Validation loss: 0.408064\n",
      "Epoch: 54 | Training loss: 0.232368 | Validation loss: 0.407237\n",
      "Epoch: 55 | Training loss: 0.230297 | Validation loss: 0.406769\n",
      "Epoch: 56 | Training loss: 0.228253 | Validation loss: 0.406385\n",
      "Epoch: 57 | Training loss: 0.226466 | Validation loss: 0.405153\n",
      "Epoch: 58 | Training loss: 0.224542 | Validation loss: 0.404563\n",
      "Epoch: 59 | Training loss: 0.222540 | Validation loss: 0.403616\n",
      "Epoch: 60 | Training loss: 0.220682 | Validation loss: 0.403421\n",
      "Epoch: 61 | Training loss: 0.218944 | Validation loss: 0.403166\n",
      "Epoch: 62 | Training loss: 0.217157 | Validation loss: 0.401730\n",
      "Epoch: 63 | Training loss: 0.215344 | Validation loss: 0.401756\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr_count, data_tr_label, \n",
    "                                             X_dev=X_dev_count, \n",
    "                                             Y_dev=data_dev_label, \n",
    "                                             lr=0.0001, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,len(loss_tr_count),len(loss_tr_count))\n",
    "\n",
    "plt.plot(x, loss_tr_count,label='Training loss')\n",
    "plt.plot(x, dev_loss_count, label='Validation loss')\n",
    "\n",
    "plt.title('Training Monitoring')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:52:26.583150Z",
     "start_time": "2020-01-21T16:52:26.578754Z"
    }
   },
   "source": [
    "Explain here...\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "The model have reached at the overfit situation,because the gap between the Training loss and Validation loss is very large. The over-training data result in optimization. In addition, the model's generalization ability performs not well. These conditions cause the error rate of verification set is higher than the traininng set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "def accuracy_score(Y_trainerror,pre_te):  \n",
    "    result = 0\n",
    "    for i,label in enumerate(Y_trainerror):\n",
    "        if pre_te[i] == label:\n",
    "            result = result+1\n",
    "    score = result/len(Y_trainerror)\n",
    "    return score\n",
    "\n",
    "def precision_score(Y_trainerror,pre_te):\n",
    "    Y_cor = (Y_trainerror==1)\n",
    "    pre_te_cor = (pre_te==1)\n",
    "    cor_count = (Y_cor * pre_te_cor).sum()\n",
    "    all_count = (pre_te==1).sum()\n",
    "    \n",
    "    return cor_count/all_count\n",
    "\n",
    "def recall_score(Y_trainerror,pre_te):\n",
    "    Y_cor = (Y_trainerror==1)\n",
    "    pre_te_cor = (pre_te==1)\n",
    "    cor_count = (Y_cor * pre_te_cor).sum()\n",
    "    all_count = (Y_cor==1).sum()\n",
    "    \n",
    "    return cor_count/all_count\n",
    "\n",
    "def f1_score(Y_trainerror,pre_te):\n",
    "    cor_count = 2 * precision_score(Y_trainerror, pre_te) * recall_score(Y_trainerror, pre_te)\n",
    "    all_count = precision_score(Y_trainerror, pre_te) + recall_score(Y_trainerror, pre_te)\n",
    "    \n",
    "    return cor_count/all_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:37:40.569499Z",
     "start_time": "2020-02-15T14:37:40.566796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8325\n",
      "Precision: 0.824390243902439\n",
      "Recall: 0.845\n",
      "F1-Score: 0.8345679012345678\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(X_test_count, w_count)\n",
    "Y_te = data_test_label\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:37:07.638179Z",
     "start_time": "2020-02-15T14:37:07.635838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 positive words are ['great', 'well', 'also', 'seen', 'life', 'fun', 'world', 'many', 'movies', 'see'] respectively.\n",
      "The top 10 negative words are ['bad', 'only', 'worst', 'unfortunately', 'script', 'why', 'plot', 'boring', 'any', 'nothing'] respectively.\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "# Obtain a new dictionary to store the weight\n",
    "dic_weight = dict()\n",
    "for i,weight in enumerate(w_count):\n",
    "    dic_weight[vocab[i]] = weight\n",
    "    \n",
    "# Obtain the positive words\n",
    "dic_resultP = dict()\n",
    "dic_resultN = dict()\n",
    "\n",
    "for word,weight in dic_weight.items():\n",
    "    if weight >= 0:\n",
    "        dic_resultP[word] = weight\n",
    "    else:\n",
    "        dic_resultN[word] = weight\n",
    "\n",
    "dic_resultP = dict(sorted(dic_resultP.items(),key=lambda weight:weight[1], reverse=True)[:10])\n",
    "dic_resultN = dict(sorted(dic_resultN.items(),key=lambda weight:weight[1], reverse=False)[:10])\n",
    "\n",
    "print(\"The top 10 positive words are %s respectively.\"% list(dic_resultP.keys()))\n",
    "print(\"The top 10 negative words are %s respectively.\"% list(dic_resultN.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your answer here...\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "We could use this trained features into the different domain such as the shopping reviews or restaurant reviews.\n",
    "\n",
    "The features like positive category such as 'well' 'good' and negative category such as 'nothing' 'worst' 'unfortunately'.\n",
    "\n",
    "But we should remove some irrelevant words from the data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:54.414637Z",
     "start_time": "2020-02-15T14:17:51.625934Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.630729 | Validation loss: 0.597229\n",
      "Epoch: 1 | Training loss: 0.493460 | Validation loss: 0.549631\n",
      "Epoch: 2 | Training loss: 0.416168 | Validation loss: 0.518829\n",
      "Epoch: 3 | Training loss: 0.364288 | Validation loss: 0.496018\n",
      "Epoch: 4 | Training loss: 0.326064 | Validation loss: 0.478729\n",
      "Epoch: 5 | Training loss: 0.295907 | Validation loss: 0.464445\n",
      "Epoch: 6 | Training loss: 0.271674 | Validation loss: 0.453058\n",
      "Epoch: 7 | Training loss: 0.251322 | Validation loss: 0.443290\n",
      "Epoch: 8 | Training loss: 0.234251 | Validation loss: 0.434850\n",
      "Epoch: 9 | Training loss: 0.219420 | Validation loss: 0.427794\n",
      "Epoch: 10 | Training loss: 0.206445 | Validation loss: 0.421439\n",
      "Epoch: 11 | Training loss: 0.195075 | Validation loss: 0.415901\n",
      "Epoch: 12 | Training loss: 0.184735 | Validation loss: 0.411163\n",
      "Epoch: 13 | Training loss: 0.175831 | Validation loss: 0.406658\n",
      "Epoch: 14 | Training loss: 0.167590 | Validation loss: 0.402873\n",
      "Epoch: 15 | Training loss: 0.160046 | Validation loss: 0.399387\n",
      "Epoch: 16 | Training loss: 0.153221 | Validation loss: 0.396397\n",
      "Epoch: 17 | Training loss: 0.147066 | Validation loss: 0.393370\n",
      "Epoch: 18 | Training loss: 0.141323 | Validation loss: 0.390855\n",
      "Epoch: 19 | Training loss: 0.136002 | Validation loss: 0.388535\n",
      "Epoch: 20 | Training loss: 0.131096 | Validation loss: 0.386326\n",
      "Epoch: 21 | Training loss: 0.126505 | Validation loss: 0.384335\n",
      "Epoch: 22 | Training loss: 0.122269 | Validation loss: 0.382568\n",
      "Epoch: 23 | Training loss: 0.118276 | Validation loss: 0.380904\n",
      "Epoch: 24 | Training loss: 0.114532 | Validation loss: 0.379329\n",
      "Epoch: 25 | Training loss: 0.111039 | Validation loss: 0.377924\n",
      "Epoch: 26 | Training loss: 0.107706 | Validation loss: 0.376605\n",
      "Epoch: 27 | Training loss: 0.104634 | Validation loss: 0.375432\n",
      "Epoch: 28 | Training loss: 0.101692 | Validation loss: 0.374373\n",
      "Epoch: 29 | Training loss: 0.098921 | Validation loss: 0.373359\n",
      "Epoch: 30 | Training loss: 0.096292 | Validation loss: 0.372366\n",
      "Epoch: 31 | Training loss: 0.093803 | Validation loss: 0.371521\n",
      "Epoch: 32 | Training loss: 0.091426 | Validation loss: 0.370673\n",
      "Epoch: 33 | Training loss: 0.089156 | Validation loss: 0.369987\n",
      "Epoch: 34 | Training loss: 0.087034 | Validation loss: 0.369267\n",
      "Epoch: 35 | Training loss: 0.084975 | Validation loss: 0.368527\n",
      "Epoch: 36 | Training loss: 0.083033 | Validation loss: 0.367945\n",
      "Epoch: 37 | Training loss: 0.081161 | Validation loss: 0.367391\n",
      "Epoch: 38 | Training loss: 0.079381 | Validation loss: 0.366891\n",
      "Epoch: 39 | Training loss: 0.077667 | Validation loss: 0.366370\n",
      "Epoch: 40 | Training loss: 0.076026 | Validation loss: 0.365968\n",
      "Epoch: 41 | Training loss: 0.074460 | Validation loss: 0.365520\n",
      "Epoch: 42 | Training loss: 0.072951 | Validation loss: 0.365144\n",
      "Epoch: 43 | Training loss: 0.071504 | Validation loss: 0.364762\n",
      "Epoch: 44 | Training loss: 0.070098 | Validation loss: 0.364381\n",
      "Epoch: 45 | Training loss: 0.068767 | Validation loss: 0.364058\n",
      "Epoch: 46 | Training loss: 0.067476 | Validation loss: 0.363774\n",
      "Epoch: 47 | Training loss: 0.066228 | Validation loss: 0.363475\n",
      "Epoch: 48 | Training loss: 0.065026 | Validation loss: 0.363235\n",
      "Epoch: 49 | Training loss: 0.063871 | Validation loss: 0.362977\n"
     ]
    }
   ],
   "source": [
    "w_tfidf, trl, devl = SGD(X_tr_tfidf, data_tr_label, \n",
    "                         X_dev=X_dev_tfidf, \n",
    "                         Y_dev=data_dev_label, \n",
    "                         lr=0.0001, \n",
    "                         alpha=0.00001, \n",
    "                         epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:54.517668Z",
     "start_time": "2020-02-15T14:17:54.417118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,len(trl),len(trl))\n",
    "\n",
    "plt.plot(x, trl,label='Training loss')\n",
    "plt.plot(x, devl, label='Validation loss')\n",
    "\n",
    "plt.title('Training Monitoring')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model have reached at the overfit situation,because the gap between the Training loss and Validation loss is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:37:56.489814Z",
     "start_time": "2020-02-15T14:37:56.487014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8675\n",
      "Precision: 0.8516746411483254\n",
      "Recall: 0.89\n",
      "F1-Score: 0.8704156479217604\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "preds_te = predict_class(X_test_tfidf, w_tfidf)\n",
    "Y_te = data_test_label\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te))\n",
    "print('Recall:', recall_score(Y_te,preds_te))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top-10 most positive and negative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:38:17.845485Z",
     "start_time": "2020-02-15T14:38:17.842557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 positive words are ['great', 'hilarious', 'fun', 'terrific', 'overall', 'perfectly', 'definitely', 'memorable', 'life', 'simple'] respectively.\n",
      "The top 10 negative words are ['bad', 'worst', 'boring', 'supposed', 'unfortunately', 'waste', 'awful', 'poor', 'script', 'nothing'] respectively.\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "# Obtain a new dictionary to store the weight\n",
    "dic_weight = dict()\n",
    "for i,weight in enumerate(w_tfidf):\n",
    "    dic_weight[vocab[i]] = weight\n",
    "    \n",
    "# Obtain the positive words\n",
    "dic_resultP = dict()\n",
    "dic_resultN = dict()\n",
    "\n",
    "for word,weight in dic_weight.items():\n",
    "    if weight >= 0:\n",
    "        dic_resultP[word] = weight\n",
    "    else:\n",
    "        dic_resultN[word] = weight\n",
    "\n",
    "dic_resultP = dict(sorted(dic_resultP.items(),key=lambda weight:weight[1], reverse=True)[:10])\n",
    "dic_resultN = dict(sorted(dic_resultN.items(),key=lambda weight:weight[1], reverse=False)[:10])\n",
    "\n",
    "print(\"The top 10 positive words are %s respectively.\"% list(dic_resultP.keys()))\n",
    "print(\"The top 10 negative words are %s respectively.\"% list(dic_resultN.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.490848 | Validation loss: 0.422014\n",
      "Epoch: 1 | Training loss: 0.179338 | Validation loss: 0.378674\n",
      "Epoch: 2 | Training loss: 0.109270 | Validation loss: 0.371132\n",
      "Epoch: 3 | Training loss: 0.081879 | Validation loss: 0.365232\n",
      "Epoch: 4 | Training loss: 0.066032 | Validation loss: 0.363066\n",
      "Epoch: 5 | Training loss: 0.055511 | Validation loss: 0.363308\n",
      "lr = 0.001000, alpha= 0.001000; Predict_Class:0.861314\n",
      "Epoch: 0 | Training loss: 0.492331 | Validation loss: 0.425297\n",
      "Epoch: 1 | Training loss: 0.176187 | Validation loss: 0.387362\n",
      "Epoch: 2 | Training loss: 0.109291 | Validation loss: 0.383238\n",
      "Epoch: 3 | Training loss: 0.081937 | Validation loss: 0.368167\n",
      "Epoch: 4 | Training loss: 0.065579 | Validation loss: 0.364402\n",
      "Epoch: 5 | Training loss: 0.055040 | Validation loss: 0.363961\n",
      "Epoch: 6 | Training loss: 0.047454 | Validation loss: 0.364355\n",
      "lr = 0.001000, alpha= 0.000500; Predict_Class:0.866337\n",
      "Epoch: 0 | Training loss: 0.494068 | Validation loss: 0.426704\n",
      "Epoch: 1 | Training loss: 0.173315 | Validation loss: 0.390385\n",
      "Epoch: 2 | Training loss: 0.109115 | Validation loss: 0.378187\n",
      "Epoch: 3 | Training loss: 0.081341 | Validation loss: 0.369791\n",
      "Epoch: 4 | Training loss: 0.065311 | Validation loss: 0.367865\n",
      "Epoch: 5 | Training loss: 0.054376 | Validation loss: 0.365924\n",
      "Epoch: 6 | Training loss: 0.046644 | Validation loss: 0.369589\n",
      "lr = 0.001000, alpha= 0.000100; Predict_Class:0.864078\n",
      "Epoch: 0 | Training loss: 0.530480 | Validation loss: 0.473739\n",
      "Epoch: 1 | Training loss: 0.268423 | Validation loss: 0.423822\n",
      "Epoch: 2 | Training loss: 0.189719 | Validation loss: 0.400644\n",
      "Epoch: 3 | Training loss: 0.148375 | Validation loss: 0.391764\n",
      "Epoch: 4 | Training loss: 0.122753 | Validation loss: 0.379869\n",
      "Epoch: 5 | Training loss: 0.104440 | Validation loss: 0.374024\n",
      "Epoch: 6 | Training loss: 0.091104 | Validation loss: 0.370380\n",
      "Epoch: 7 | Training loss: 0.080774 | Validation loss: 0.367905\n",
      "Epoch: 8 | Training loss: 0.072655 | Validation loss: 0.365872\n",
      "Epoch: 9 | Training loss: 0.066095 | Validation loss: 0.364131\n",
      "Epoch: 10 | Training loss: 0.060632 | Validation loss: 0.363386\n",
      "Epoch: 11 | Training loss: 0.056028 | Validation loss: 0.363041\n",
      "Epoch: 12 | Training loss: 0.052141 | Validation loss: 0.362894\n",
      "Epoch: 13 | Training loss: 0.048758 | Validation loss: 0.362838\n",
      "lr = 0.000500, alpha= 0.001000; Predict_Class:0.864198\n",
      "Epoch: 0 | Training loss: 0.525952 | Validation loss: 0.475222\n",
      "Epoch: 1 | Training loss: 0.270389 | Validation loss: 0.421016\n",
      "Epoch: 2 | Training loss: 0.189989 | Validation loss: 0.397285\n",
      "Epoch: 3 | Training loss: 0.148396 | Validation loss: 0.385271\n",
      "Epoch: 4 | Training loss: 0.121986 | Validation loss: 0.377747\n",
      "Epoch: 5 | Training loss: 0.104137 | Validation loss: 0.371402\n",
      "Epoch: 6 | Training loss: 0.090657 | Validation loss: 0.367627\n",
      "Epoch: 7 | Training loss: 0.080333 | Validation loss: 0.365394\n",
      "Epoch: 8 | Training loss: 0.072127 | Validation loss: 0.363618\n",
      "Epoch: 9 | Training loss: 0.065504 | Validation loss: 0.362583\n",
      "Epoch: 10 | Training loss: 0.059988 | Validation loss: 0.361900\n",
      "Epoch: 11 | Training loss: 0.055323 | Validation loss: 0.361397\n",
      "Epoch: 12 | Training loss: 0.051402 | Validation loss: 0.361224\n",
      "Epoch: 13 | Training loss: 0.047974 | Validation loss: 0.361317\n",
      "lr = 0.000500, alpha= 0.000500; Predict_Class:0.866995\n",
      "Epoch: 0 | Training loss: 0.530445 | Validation loss: 0.464269\n",
      "Epoch: 1 | Training loss: 0.268302 | Validation loss: 0.416873\n",
      "Epoch: 2 | Training loss: 0.189291 | Validation loss: 0.401766\n",
      "Epoch: 3 | Training loss: 0.148315 | Validation loss: 0.383875\n",
      "Epoch: 4 | Training loss: 0.121990 | Validation loss: 0.376471\n",
      "Epoch: 5 | Training loss: 0.103667 | Validation loss: 0.370455\n",
      "Epoch: 6 | Training loss: 0.090249 | Validation loss: 0.367373\n",
      "Epoch: 7 | Training loss: 0.079840 | Validation loss: 0.365303\n",
      "Epoch: 8 | Training loss: 0.071625 | Validation loss: 0.363942\n",
      "Epoch: 9 | Training loss: 0.064853 | Validation loss: 0.364041\n",
      "lr = 0.000500, alpha= 0.000100; Predict_Class:0.866180\n",
      "Epoch: 0 | Training loss: 0.630648 | Validation loss: 0.597692\n",
      "Epoch: 1 | Training loss: 0.492887 | Validation loss: 0.549601\n",
      "Epoch: 2 | Training loss: 0.416346 | Validation loss: 0.518746\n",
      "Epoch: 3 | Training loss: 0.364555 | Validation loss: 0.496064\n",
      "Epoch: 4 | Training loss: 0.326253 | Validation loss: 0.478117\n",
      "Epoch: 5 | Training loss: 0.296074 | Validation loss: 0.464310\n",
      "Epoch: 6 | Training loss: 0.271874 | Validation loss: 0.452527\n",
      "Epoch: 7 | Training loss: 0.251645 | Validation loss: 0.442786\n",
      "Epoch: 8 | Training loss: 0.234474 | Validation loss: 0.434592\n",
      "Epoch: 9 | Training loss: 0.219702 | Validation loss: 0.427568\n",
      "Epoch: 10 | Training loss: 0.206752 | Validation loss: 0.421386\n",
      "Epoch: 11 | Training loss: 0.195317 | Validation loss: 0.415953\n",
      "Epoch: 12 | Training loss: 0.185297 | Validation loss: 0.410943\n",
      "Epoch: 13 | Training loss: 0.176200 | Validation loss: 0.406562\n",
      "Epoch: 14 | Training loss: 0.167997 | Validation loss: 0.402688\n",
      "Epoch: 15 | Training loss: 0.160526 | Validation loss: 0.399364\n",
      "Epoch: 16 | Training loss: 0.153758 | Validation loss: 0.396149\n",
      "Epoch: 17 | Training loss: 0.147560 | Validation loss: 0.393273\n",
      "Epoch: 18 | Training loss: 0.141840 | Validation loss: 0.390690\n",
      "Epoch: 19 | Training loss: 0.136570 | Validation loss: 0.388328\n",
      "Epoch: 20 | Training loss: 0.131660 | Validation loss: 0.386190\n",
      "Epoch: 21 | Training loss: 0.127068 | Validation loss: 0.384394\n",
      "Epoch: 22 | Training loss: 0.122892 | Validation loss: 0.382542\n",
      "Epoch: 23 | Training loss: 0.118922 | Validation loss: 0.380867\n",
      "Epoch: 24 | Training loss: 0.115206 | Validation loss: 0.379253\n",
      "Epoch: 25 | Training loss: 0.111732 | Validation loss: 0.377842\n",
      "Epoch: 26 | Training loss: 0.108456 | Validation loss: 0.376516\n",
      "Epoch: 27 | Training loss: 0.105381 | Validation loss: 0.375320\n",
      "Epoch: 28 | Training loss: 0.102460 | Validation loss: 0.374212\n",
      "Epoch: 29 | Training loss: 0.099726 | Validation loss: 0.373198\n",
      "Epoch: 30 | Training loss: 0.097110 | Validation loss: 0.372263\n",
      "Epoch: 31 | Training loss: 0.094642 | Validation loss: 0.371367\n",
      "Epoch: 32 | Training loss: 0.092285 | Validation loss: 0.370559\n",
      "Epoch: 33 | Training loss: 0.090071 | Validation loss: 0.369790\n",
      "Epoch: 34 | Training loss: 0.087940 | Validation loss: 0.369093\n",
      "Epoch: 35 | Training loss: 0.085902 | Validation loss: 0.368481\n",
      "Epoch: 36 | Training loss: 0.083992 | Validation loss: 0.367818\n",
      "Epoch: 37 | Training loss: 0.082147 | Validation loss: 0.367236\n",
      "Epoch: 38 | Training loss: 0.080383 | Validation loss: 0.366715\n",
      "Epoch: 39 | Training loss: 0.078691 | Validation loss: 0.366188\n",
      "Epoch: 40 | Training loss: 0.077071 | Validation loss: 0.365700\n",
      "Epoch: 41 | Training loss: 0.075524 | Validation loss: 0.365278\n",
      "Epoch: 42 | Training loss: 0.074032 | Validation loss: 0.364920\n",
      "Epoch: 43 | Training loss: 0.072605 | Validation loss: 0.364529\n",
      "Epoch: 44 | Training loss: 0.071217 | Validation loss: 0.364192\n",
      "Epoch: 45 | Training loss: 0.069910 | Validation loss: 0.363865\n",
      "Epoch: 46 | Training loss: 0.068645 | Validation loss: 0.363564\n",
      "Epoch: 47 | Training loss: 0.067389 | Validation loss: 0.363356\n",
      "Epoch: 48 | Training loss: 0.066227 | Validation loss: 0.363043\n",
      "Epoch: 49 | Training loss: 0.065097 | Validation loss: 0.362808\n",
      "lr = 0.000100, alpha= 0.001000; Predict_Class:0.869779\n",
      "Epoch: 0 | Training loss: 0.629683 | Validation loss: 0.598444\n",
      "Epoch: 1 | Training loss: 0.492813 | Validation loss: 0.549464\n",
      "Epoch: 2 | Training loss: 0.416894 | Validation loss: 0.518050\n",
      "Epoch: 3 | Training loss: 0.364683 | Validation loss: 0.496071\n",
      "Epoch: 4 | Training loss: 0.325984 | Validation loss: 0.478819\n",
      "Epoch: 5 | Training loss: 0.296142 | Validation loss: 0.464652\n",
      "Epoch: 6 | Training loss: 0.271887 | Validation loss: 0.452846\n",
      "Epoch: 7 | Training loss: 0.251594 | Validation loss: 0.443159\n",
      "Epoch: 8 | Training loss: 0.234339 | Validation loss: 0.434699\n",
      "Epoch: 9 | Training loss: 0.219605 | Validation loss: 0.427679\n",
      "Epoch: 10 | Training loss: 0.206682 | Validation loss: 0.421415\n",
      "Epoch: 11 | Training loss: 0.195259 | Validation loss: 0.415930\n",
      "Epoch: 12 | Training loss: 0.185059 | Validation loss: 0.411120\n",
      "Epoch: 13 | Training loss: 0.176049 | Validation loss: 0.406791\n",
      "Epoch: 14 | Training loss: 0.167775 | Validation loss: 0.402894\n",
      "Epoch: 15 | Training loss: 0.160169 | Validation loss: 0.399585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Training loss: 0.153589 | Validation loss: 0.396217\n",
      "Epoch: 17 | Training loss: 0.147341 | Validation loss: 0.393365\n",
      "Epoch: 18 | Training loss: 0.141567 | Validation loss: 0.390807\n",
      "Epoch: 19 | Training loss: 0.136311 | Validation loss: 0.388407\n",
      "Epoch: 20 | Training loss: 0.131382 | Validation loss: 0.386209\n",
      "Epoch: 21 | Training loss: 0.126832 | Validation loss: 0.384290\n",
      "Epoch: 22 | Training loss: 0.122541 | Validation loss: 0.382424\n",
      "Epoch: 23 | Training loss: 0.118607 | Validation loss: 0.380771\n",
      "Epoch: 24 | Training loss: 0.114888 | Validation loss: 0.379330\n",
      "Epoch: 25 | Training loss: 0.111400 | Validation loss: 0.377933\n",
      "Epoch: 26 | Training loss: 0.108108 | Validation loss: 0.376662\n",
      "Epoch: 27 | Training loss: 0.105007 | Validation loss: 0.375498\n",
      "Epoch: 28 | Training loss: 0.102087 | Validation loss: 0.374284\n",
      "Epoch: 29 | Training loss: 0.099319 | Validation loss: 0.373285\n",
      "Epoch: 30 | Training loss: 0.096713 | Validation loss: 0.372282\n",
      "Epoch: 31 | Training loss: 0.094228 | Validation loss: 0.371378\n",
      "Epoch: 32 | Training loss: 0.091871 | Validation loss: 0.370556\n",
      "Epoch: 33 | Training loss: 0.089624 | Validation loss: 0.369776\n",
      "Epoch: 34 | Training loss: 0.087486 | Validation loss: 0.369059\n",
      "Epoch: 35 | Training loss: 0.085456 | Validation loss: 0.368410\n",
      "Epoch: 36 | Training loss: 0.083508 | Validation loss: 0.367790\n",
      "Epoch: 37 | Training loss: 0.081658 | Validation loss: 0.367220\n",
      "Epoch: 38 | Training loss: 0.079853 | Validation loss: 0.366734\n",
      "Epoch: 39 | Training loss: 0.078182 | Validation loss: 0.366165\n",
      "Epoch: 40 | Training loss: 0.076554 | Validation loss: 0.365695\n",
      "Epoch: 41 | Training loss: 0.074992 | Validation loss: 0.365289\n",
      "Epoch: 42 | Training loss: 0.073493 | Validation loss: 0.364882\n",
      "Epoch: 43 | Training loss: 0.072051 | Validation loss: 0.364500\n",
      "Epoch: 44 | Training loss: 0.070670 | Validation loss: 0.364161\n",
      "Epoch: 45 | Training loss: 0.069328 | Validation loss: 0.363809\n",
      "Epoch: 46 | Training loss: 0.068054 | Validation loss: 0.363505\n",
      "Epoch: 47 | Training loss: 0.066814 | Validation loss: 0.363223\n",
      "Epoch: 48 | Training loss: 0.065618 | Validation loss: 0.363024\n",
      "Epoch: 49 | Training loss: 0.064479 | Validation loss: 0.362802\n",
      "lr = 0.000100, alpha= 0.000500; Predict_Class:0.868293\n",
      "Epoch: 0 | Training loss: 0.631021 | Validation loss: 0.598696\n",
      "Epoch: 1 | Training loss: 0.493515 | Validation loss: 0.549354\n",
      "Epoch: 2 | Training loss: 0.416807 | Validation loss: 0.517926\n",
      "Epoch: 3 | Training loss: 0.364625 | Validation loss: 0.495753\n",
      "Epoch: 4 | Training loss: 0.326144 | Validation loss: 0.478580\n",
      "Epoch: 5 | Training loss: 0.296179 | Validation loss: 0.464246\n",
      "Epoch: 6 | Training loss: 0.271745 | Validation loss: 0.452898\n",
      "Epoch: 7 | Training loss: 0.251291 | Validation loss: 0.443203\n",
      "Epoch: 8 | Training loss: 0.234339 | Validation loss: 0.434737\n",
      "Epoch: 9 | Training loss: 0.219472 | Validation loss: 0.427603\n",
      "Epoch: 10 | Training loss: 0.206497 | Validation loss: 0.421511\n",
      "Epoch: 11 | Training loss: 0.195106 | Validation loss: 0.416031\n",
      "Epoch: 12 | Training loss: 0.184951 | Validation loss: 0.411158\n",
      "Epoch: 13 | Training loss: 0.175868 | Validation loss: 0.406794\n",
      "Epoch: 14 | Training loss: 0.167600 | Validation loss: 0.402912\n",
      "Epoch: 15 | Training loss: 0.160141 | Validation loss: 0.399466\n",
      "Epoch: 16 | Training loss: 0.153337 | Validation loss: 0.396372\n",
      "Epoch: 17 | Training loss: 0.147122 | Validation loss: 0.393490\n",
      "Epoch: 18 | Training loss: 0.141353 | Validation loss: 0.390911\n",
      "Epoch: 19 | Training loss: 0.136057 | Validation loss: 0.388517\n",
      "Epoch: 20 | Training loss: 0.131130 | Validation loss: 0.386454\n",
      "Epoch: 21 | Training loss: 0.126575 | Validation loss: 0.384496\n",
      "Epoch: 22 | Training loss: 0.122295 | Validation loss: 0.382750\n",
      "Epoch: 23 | Training loss: 0.118314 | Validation loss: 0.381120\n",
      "Epoch: 24 | Training loss: 0.114600 | Validation loss: 0.379520\n",
      "Epoch: 25 | Training loss: 0.111085 | Validation loss: 0.378076\n",
      "Epoch: 26 | Training loss: 0.107765 | Validation loss: 0.376791\n",
      "Epoch: 27 | Training loss: 0.104701 | Validation loss: 0.375590\n",
      "Epoch: 28 | Training loss: 0.101762 | Validation loss: 0.374484\n",
      "Epoch: 29 | Training loss: 0.098995 | Validation loss: 0.373433\n",
      "Epoch: 30 | Training loss: 0.096369 | Validation loss: 0.372402\n",
      "Epoch: 31 | Training loss: 0.093876 | Validation loss: 0.371507\n",
      "Epoch: 32 | Training loss: 0.091479 | Validation loss: 0.370758\n",
      "Epoch: 33 | Training loss: 0.089257 | Validation loss: 0.369907\n",
      "Epoch: 34 | Training loss: 0.087105 | Validation loss: 0.369222\n",
      "Epoch: 35 | Training loss: 0.085068 | Validation loss: 0.368578\n",
      "Epoch: 36 | Training loss: 0.083113 | Validation loss: 0.367987\n",
      "Epoch: 37 | Training loss: 0.081254 | Validation loss: 0.367411\n",
      "Epoch: 38 | Training loss: 0.079468 | Validation loss: 0.366887\n",
      "Epoch: 39 | Training loss: 0.077760 | Validation loss: 0.366374\n",
      "Epoch: 40 | Training loss: 0.076124 | Validation loss: 0.365914\n",
      "Epoch: 41 | Training loss: 0.074544 | Validation loss: 0.365460\n",
      "Epoch: 42 | Training loss: 0.073049 | Validation loss: 0.365051\n",
      "Epoch: 43 | Training loss: 0.071593 | Validation loss: 0.364705\n",
      "Epoch: 44 | Training loss: 0.070206 | Validation loss: 0.364347\n",
      "Epoch: 45 | Training loss: 0.068860 | Validation loss: 0.364000\n",
      "Epoch: 46 | Training loss: 0.067574 | Validation loss: 0.363682\n",
      "Epoch: 47 | Training loss: 0.066335 | Validation loss: 0.363417\n",
      "Epoch: 48 | Training loss: 0.065132 | Validation loss: 0.363194\n",
      "Epoch: 49 | Training loss: 0.063978 | Validation loss: 0.362955\n",
      "lr = 0.000100, alpha= 0.000100; Predict_Class:0.870416\n"
     ]
    }
   ],
   "source": [
    "lr_set = [0.001,0.0005,0.0001]\n",
    "alpha_set = [0.001,0.0005,0.0001]\n",
    "\n",
    "for lr_sid in range(len(lr_set)):\n",
    "    for alpha_sid in range(len(alpha_set)):\n",
    "        w_tfidf, trl, devl = SGD(X_tr_tfidf, data_tr_label, \n",
    "                         X_dev=X_dev_tfidf, \n",
    "                         Y_dev=data_dev_label, \n",
    "                         lr=lr_set[lr_sid], \n",
    "                         alpha=alpha_set[alpha_sid], \n",
    "                         epochs=50)\n",
    "        preds_te = predict_class(X_test_tfidf, w_tfidf)\n",
    "        print(\"lr = %f, alpha= %f;\"%(lr_set[lr_sid],alpha_set[alpha_sid]),'Predict_Class:%f'%(f1_score(Y_te,preds_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous result, we can find that whtn the lr=0.0001,alpha=0.0001, the value of the F1-Score has reached at the peak.\n",
    "\n",
    "So we conclude that the lr has more influence on the model training.\n",
    "\n",
    "If we want to avoid the overfitting, we should apply the regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |  0.825 | 0.85  |  0.83744 |\n",
    "| BOW-tfidf  |   0.85167 |  0.89 | 0.8704156  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Logistic Regression \n",
    "\n",
    "Now you need to train a Multiclass Logistic Regression (MLR) Classifier by extending the Binary model you developed above. You will use the MLR model to perform topic classification on the AG news dataset consisting of three classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class 1: World\n",
    "- Class 2: Sports\n",
    "- Class 3: Business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to follow the same process as in Task 1 for data processing and feature extraction by reusing the functions you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:03.212229Z",
     "start_time": "2020-02-15T14:18:03.185261Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill in your code...\n",
    "data_tr = pd.read_csv(\"./data_topic/train.csv\",header=None, names=['label','text'])\n",
    "data_dev = pd.read_csv(\"./data_topic/dev.csv\",header=None, names=['label','text'])\n",
    "data_test = pd.read_csv(\"./data_topic/test.csv\",header=None, names=['label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:03.515585Z",
     "start_time": "2020-02-15T14:18:03.508299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:03.806523Z",
     "start_time": "2020-02-15T14:18:03.798279Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can obtain the all the file into \n",
    "data_tr_list = [word.lower() for word in data_tr['text'].tolist()]\n",
    "data_tr_label = np.array(data_tr['label'])\n",
    "# development dataset\n",
    "data_dev_list = [word.lower() for word in data_dev['text'].tolist()]\n",
    "data_dev_label = np.array(data_dev['label'])\n",
    "# test dataset\n",
    "data_test_list = [word.lower() for word in data_test['text'].tolist()]\n",
    "data_test_label = np.array(data_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they' 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ngram = extract_ngrams(data_tr_list, ngram_range=(1,3), stop_words=stop_words)\n",
    "dev_ngram = extract_ngrams(data_dev_list, ngram_range=(1,3), stop_words=stop_words)\n",
    "test_ngram = extract_ngrams(data_test_list, ngram_range=(1,3), stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:04.508938Z",
     "start_time": "2020-02-15T14:18:04.171071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "\n",
      "['reuters', 'said', 'tuesday', 'wednesday', 'new', 'after', 'ap', 'athens', 'monday', 'first', 'two', 'york', 'over', ('new', 'york'), 'us', 'olympic', 'but', 'their', 'will', 'inc', 'more', 'year', 'oil', 'prices', 'company', 'world', 'than', 'aug', 'about', 'had', 'united', 'one', 'sunday', 'out', 'into', 'against', 'up', 'second', 'last', 'president', 'stocks', 'gold', 'team', ('york', 'reuters'), ('new', 'york', 'reuters'), 'when', 'three', 'night', 'time', 'no', 'yesterday', 'games', 'olympics', 'not', 'states', 'greece', 'off', 'iraq', 'washington', 'percent', ('united', 'states'), ('oil', 'prices'), 'home', 'day', 'google', 'public', ('athens', 'reuters'), 'record', 'week', 'men', 'government', 'win', ('said', 'tuesday'), 'american', 'won', 'years', 'all', 'billion', 'shares', 'city', 'offering', 'officials', 'would', 'today', 'final', 'afp', 'gt', 'people', 'lt', 'medal', 'corp', 'sales', 'country', 'back', 'four', 'high', 'investor', 'com', 'minister', 'reported']\n",
      "\n",
      "[('reuters', 631), ('said', 432), ('tuesday', 413), ('wednesday', 344), ('new', 325), ('after', 295), ('ap', 275), ('athens', 245), ('monday', 221), ('first', 210)]\n"
     ]
    }
   ],
   "source": [
    "vocab, df, ngram_counts = get_vocab(data_tr_list, ngram_range=(1,3), keep_topN=5000, stop_words=stop_words)\n",
    "print(len(vocab))\n",
    "print()\n",
    "print(list(vocab)[:100])\n",
    "print()\n",
    "print(list(df.items())[:10])\n",
    "# print(df.most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:04.706802Z",
     "start_time": "2020-02-15T14:18:04.511061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorise\n",
    "X_tr_count = vectorise(tr_ngram,vocab)\n",
    "X_dev_count = vectorise(dev_ngram,vocab)\n",
    "X_test_count = vectorise(test_ngram,vocab)\n",
    "\n",
    "#Show\n",
    "X_tr_count[:2,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf.idf Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_tfidf = tfidf_function(X_count=X_tr_count, ngram_list = tr_ngram, vocab = vocab)\n",
    "X_dev_tfidf = tfidf_function(X_count=X_dev_count, ngram_list = dev_ngram, vocab = vocab)\n",
    "X_test_tfidf = tfidf_function(X_count=X_test_count, ngram_list = test_ngram, vocab = vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to change `SGD` to support multiclass datasets. First you need to develop a `softmax` function. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `smax`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.440998Z",
     "start_time": "2020-02-15T14:18:07.437915Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "\n",
    "    if len(z.shape) >1:\n",
    "        x_sum = np.sum(np.exp(z), axis = 1, keepdims = True)\n",
    "    else:\n",
    "        x_sum = np.sum(np.exp(z), axis = 0)\n",
    "\n",
    "    smax = np.exp(z) / x_sum    \n",
    "    return smax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then modify `predict_proba` and `predict_class` functions for the multiclass case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.445451Z",
     "start_time": "2020-02-15T14:18:07.442851Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \n",
    "    P_value = np.dot(X,weights.T)\n",
    "    preds_proba = softmax(P_value)\n",
    "    \n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:07.449814Z",
     "start_time": "2020-02-15T14:18:07.447145Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \n",
    "    pro_value = predict_proba(X,weights)\n",
    "    if len(pro_value.shape)>1:\n",
    "        feature_list = list()\n",
    "        for value in pro_value:\n",
    "            value[value ==  max(value)] = 1\n",
    "            for i,row in enumerate(value):\n",
    "                if row == 1:\n",
    "                    feature_list.append(i+1)\n",
    "                    break;\n",
    "        feature_list = np.asarray(feature_list)\n",
    "        \n",
    "    else:\n",
    "        pro_value[pro_value==max(pro_value)] = 1\n",
    "        for i,value in enumerate(pro_value):\n",
    "            if value == 1:\n",
    "                feature_list = i+1\n",
    "                break;\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example and expected functionality of the functions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:08.059902Z",
     "start_time": "2020-02-15T14:18:08.056774Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.1,0.2],[0.2,0.1],[0.1,-0.2]])\n",
    "w = np.array([[2,-5],[-5,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:08.495464Z",
     "start_time": "2020-02-15T14:18:08.491074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33181223, 0.66818777],\n",
       "       [0.66818777, 0.33181223],\n",
       "       [0.89090318, 0.10909682]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:08.714215Z",
     "start_time": "2020-02-15T14:18:08.710098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to compute the categorical cross entropy loss (extending the binary loss to support multiple classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:30:48.047338Z",
     "start_time": "2020-02-15T14:30:48.044395Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(X, Y, weights, num_classes=5, alpha=0.00001):\n",
    "    \n",
    "    y_proba = predict_proba(X,weights)\n",
    "    # Use the np.clip to restrict the value range\n",
    "    y_proba = np.clip(y_proba,alpha,1-alpha)\n",
    "    \n",
    "    if len(X.shape)>1:\n",
    "        loss_value = -np.log(y_proba[range(len(Y)),Y-1])\n",
    "        L2_reg_value = (1/len(X))*(alpha/2)*(np.sum(np.square(weights)))\n",
    "    else:\n",
    "        loss_value = -np.log(y_proba[Y-1])\n",
    "        L2_reg_value = (alpha/2)*(np.sum(np.square(weights)))\n",
    "    \n",
    "    l = loss_value + L2_reg_value\n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_regularize(X,class_nums):\n",
    "    \n",
    "    return np.delete(np.eye(class_nums+1)[X],0,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support the categorical cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:10.176885Z",
     "start_time": "2020-02-15T14:18:10.165021Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, vocab, X_dev=[], Y_dev=[], num_classes=5, lr=0.01, alpha=0.00001, epochs=5, tolerance=0.001, print_progress=True):\n",
    "        \n",
    "    cur_loss_dev = 2.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "\n",
    "    # Obtain the weights\n",
    "    weights = np.zeros((3, len(vocab)))\n",
    "    Y_tr_onehot = one_regularize(Y_tr,num_classes)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        loss_list = list()\n",
    "        seed_number = random.randint(50,100)\n",
    "        np.random.seed(seed_number)\n",
    "        # Shuffle the X_tr and Y_tr  \n",
    "        per_X_tr = np.random.permutation(X_tr)\n",
    "        np.random.seed(seed_number)\n",
    "        per_Y_tr = np.random.permutation(Y_tr)\n",
    "        np.random.seed(seed_number)\n",
    "        per_Y_tr_onehot = np.random.permutation(Y_tr_onehot)\n",
    "\n",
    "        for j,row in enumerate(per_X_tr):\n",
    "            # Caculate the categorical_loss and store the loss into loss_list\n",
    "            loss_tr = categorical_loss(row, per_Y_tr[j], weights, alpha)\n",
    "            loss_list.append(loss_tr)\n",
    "            y_pred = predict_proba(row,weights) \n",
    "            error = y_pred - per_Y_tr_onehot[j]\n",
    "            # update weights\n",
    "            weights_list = [weights[i]-lr*row*error[i] for i in range(num_classes)]\n",
    "            weights = np.asarray(weights_list)\n",
    "        \n",
    "        # Obtain the mean\n",
    "        tr_loss_mean = np.mean(loss_list)\n",
    "        training_loss_history.append(tr_loss_mean)\n",
    "        \n",
    "        dev_loss = categorical_loss(X_dev, Y_dev, weights, alpha)\n",
    "        dev_loss = sum(dev_loss)/len(dev_loss)\n",
    "        validation_loss_history.append(dev_loss)\n",
    "        print('Epoch: %d' % i, '| Training loss: %f' %tr_loss_mean, '| Validation loss: %f' %dev_loss)\n",
    "        \n",
    "        if (cur_loss_dev-dev_loss)<tolerance:\n",
    "            break       \n",
    "        cur_loss_dev = dev_loss\n",
    "\n",
    "\n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you MLR following the same steps as in Task 1 for both Count and tfidf features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:18:55.324956Z",
     "start_time": "2020-02-15T14:18:11.720952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 1.084229 | Validation loss: 1.083282\n",
      "Epoch: 1 | Training loss: 1.056024 | Validation loss: 1.069042\n",
      "Epoch: 2 | Training loss: 1.030918 | Validation loss: 1.055699\n",
      "Epoch: 3 | Training loss: 1.008251 | Validation loss: 1.043099\n",
      "Epoch: 4 | Training loss: 0.987531 | Validation loss: 1.031134\n",
      "Epoch: 5 | Training loss: 0.968390 | Validation loss: 1.019712\n",
      "Epoch: 6 | Training loss: 0.950563 | Validation loss: 1.008770\n",
      "Epoch: 7 | Training loss: 0.933864 | Validation loss: 0.998258\n",
      "Epoch: 8 | Training loss: 0.918148 | Validation loss: 0.988138\n",
      "Epoch: 9 | Training loss: 0.903305 | Validation loss: 0.978378\n",
      "Epoch: 10 | Training loss: 0.889241 | Validation loss: 0.968950\n",
      "Epoch: 11 | Training loss: 0.875882 | Validation loss: 0.959835\n",
      "Epoch: 12 | Training loss: 0.863166 | Validation loss: 0.951012\n",
      "Epoch: 13 | Training loss: 0.851038 | Validation loss: 0.942466\n",
      "Epoch: 14 | Training loss: 0.839455 | Validation loss: 0.934182\n",
      "Epoch: 15 | Training loss: 0.828375 | Validation loss: 0.926146\n",
      "Epoch: 16 | Training loss: 0.817762 | Validation loss: 0.918347\n",
      "Epoch: 17 | Training loss: 0.807583 | Validation loss: 0.910774\n",
      "Epoch: 18 | Training loss: 0.797808 | Validation loss: 0.903415\n",
      "Epoch: 19 | Training loss: 0.788410 | Validation loss: 0.896262\n",
      "Epoch: 20 | Training loss: 0.779368 | Validation loss: 0.889305\n",
      "Epoch: 21 | Training loss: 0.770658 | Validation loss: 0.882538\n",
      "Epoch: 22 | Training loss: 0.762262 | Validation loss: 0.875949\n",
      "Epoch: 23 | Training loss: 0.754161 | Validation loss: 0.869534\n",
      "Epoch: 24 | Training loss: 0.746338 | Validation loss: 0.863282\n",
      "Epoch: 25 | Training loss: 0.738776 | Validation loss: 0.857191\n",
      "Epoch: 26 | Training loss: 0.731461 | Validation loss: 0.851252\n",
      "Epoch: 27 | Training loss: 0.724379 | Validation loss: 0.845460\n",
      "Epoch: 28 | Training loss: 0.717520 | Validation loss: 0.839810\n",
      "Epoch: 29 | Training loss: 0.710871 | Validation loss: 0.834295\n",
      "Epoch: 30 | Training loss: 0.704422 | Validation loss: 0.828912\n",
      "Epoch: 31 | Training loss: 0.698162 | Validation loss: 0.823653\n",
      "Epoch: 32 | Training loss: 0.692083 | Validation loss: 0.818516\n",
      "Epoch: 33 | Training loss: 0.686175 | Validation loss: 0.813497\n",
      "Epoch: 34 | Training loss: 0.680432 | Validation loss: 0.808592\n",
      "Epoch: 35 | Training loss: 0.674846 | Validation loss: 0.803794\n",
      "Epoch: 36 | Training loss: 0.669408 | Validation loss: 0.799102\n",
      "Epoch: 37 | Training loss: 0.664112 | Validation loss: 0.794512\n",
      "Epoch: 38 | Training loss: 0.658952 | Validation loss: 0.790020\n",
      "Epoch: 39 | Training loss: 0.653923 | Validation loss: 0.785622\n",
      "Epoch: 40 | Training loss: 0.649019 | Validation loss: 0.781318\n",
      "Epoch: 41 | Training loss: 0.644235 | Validation loss: 0.777101\n",
      "Epoch: 42 | Training loss: 0.639565 | Validation loss: 0.772970\n",
      "Epoch: 43 | Training loss: 0.635005 | Validation loss: 0.768922\n",
      "Epoch: 44 | Training loss: 0.630551 | Validation loss: 0.764955\n",
      "Epoch: 45 | Training loss: 0.626198 | Validation loss: 0.761065\n",
      "Epoch: 46 | Training loss: 0.621943 | Validation loss: 0.757252\n",
      "Epoch: 47 | Training loss: 0.617783 | Validation loss: 0.753511\n",
      "Epoch: 48 | Training loss: 0.613712 | Validation loss: 0.749842\n",
      "Epoch: 49 | Training loss: 0.609729 | Validation loss: 0.746240\n",
      "Epoch: 50 | Training loss: 0.605829 | Validation loss: 0.742705\n",
      "Epoch: 51 | Training loss: 0.602010 | Validation loss: 0.739235\n",
      "Epoch: 52 | Training loss: 0.598270 | Validation loss: 0.735828\n",
      "Epoch: 53 | Training loss: 0.594605 | Validation loss: 0.732482\n",
      "Epoch: 54 | Training loss: 0.591012 | Validation loss: 0.729195\n",
      "Epoch: 55 | Training loss: 0.587489 | Validation loss: 0.725966\n",
      "Epoch: 56 | Training loss: 0.584035 | Validation loss: 0.722793\n",
      "Epoch: 57 | Training loss: 0.580646 | Validation loss: 0.719675\n",
      "Epoch: 58 | Training loss: 0.577320 | Validation loss: 0.716608\n",
      "Epoch: 59 | Training loss: 0.574056 | Validation loss: 0.713594\n",
      "Epoch: 60 | Training loss: 0.570851 | Validation loss: 0.710630\n",
      "Epoch: 61 | Training loss: 0.567705 | Validation loss: 0.707714\n",
      "Epoch: 62 | Training loss: 0.564613 | Validation loss: 0.704846\n",
      "Epoch: 63 | Training loss: 0.561577 | Validation loss: 0.702024\n",
      "Epoch: 64 | Training loss: 0.558592 | Validation loss: 0.699248\n",
      "Epoch: 65 | Training loss: 0.555659 | Validation loss: 0.696515\n",
      "Epoch: 66 | Training loss: 0.552775 | Validation loss: 0.693825\n",
      "Epoch: 67 | Training loss: 0.549939 | Validation loss: 0.691177\n",
      "Epoch: 68 | Training loss: 0.547150 | Validation loss: 0.688569\n",
      "Epoch: 69 | Training loss: 0.544405 | Validation loss: 0.686001\n",
      "Epoch: 70 | Training loss: 0.541706 | Validation loss: 0.683472\n",
      "Epoch: 71 | Training loss: 0.539049 | Validation loss: 0.680980\n",
      "Epoch: 72 | Training loss: 0.536433 | Validation loss: 0.678524\n",
      "Epoch: 73 | Training loss: 0.533858 | Validation loss: 0.676105\n",
      "Epoch: 74 | Training loss: 0.531322 | Validation loss: 0.673721\n",
      "Epoch: 75 | Training loss: 0.528825 | Validation loss: 0.671372\n",
      "Epoch: 76 | Training loss: 0.526365 | Validation loss: 0.669054\n",
      "Epoch: 77 | Training loss: 0.523941 | Validation loss: 0.666770\n",
      "Epoch: 78 | Training loss: 0.521553 | Validation loss: 0.664518\n",
      "Epoch: 79 | Training loss: 0.519199 | Validation loss: 0.662297\n",
      "Epoch: 80 | Training loss: 0.516879 | Validation loss: 0.660107\n",
      "Epoch: 81 | Training loss: 0.514592 | Validation loss: 0.657946\n",
      "Epoch: 82 | Training loss: 0.512337 | Validation loss: 0.655814\n",
      "Epoch: 83 | Training loss: 0.510113 | Validation loss: 0.653712\n",
      "Epoch: 84 | Training loss: 0.507920 | Validation loss: 0.651636\n",
      "Epoch: 85 | Training loss: 0.505756 | Validation loss: 0.649589\n",
      "Epoch: 86 | Training loss: 0.503622 | Validation loss: 0.647569\n",
      "Epoch: 87 | Training loss: 0.501516 | Validation loss: 0.645573\n",
      "Epoch: 88 | Training loss: 0.499437 | Validation loss: 0.643603\n",
      "Epoch: 89 | Training loss: 0.497386 | Validation loss: 0.641658\n",
      "Epoch: 90 | Training loss: 0.495361 | Validation loss: 0.639739\n",
      "Epoch: 91 | Training loss: 0.493362 | Validation loss: 0.637843\n",
      "Epoch: 92 | Training loss: 0.491388 | Validation loss: 0.635971\n",
      "Epoch: 93 | Training loss: 0.489439 | Validation loss: 0.634122\n",
      "Epoch: 94 | Training loss: 0.487514 | Validation loss: 0.632296\n",
      "Epoch: 95 | Training loss: 0.485613 | Validation loss: 0.630491\n",
      "Epoch: 96 | Training loss: 0.483735 | Validation loss: 0.628708\n",
      "Epoch: 97 | Training loss: 0.481880 | Validation loss: 0.626946\n",
      "Epoch: 98 | Training loss: 0.480047 | Validation loss: 0.625206\n",
      "Epoch: 99 | Training loss: 0.478235 | Validation loss: 0.623486\n",
      "Epoch: 100 | Training loss: 0.476445 | Validation loss: 0.621786\n",
      "Epoch: 101 | Training loss: 0.474676 | Validation loss: 0.620106\n",
      "Epoch: 102 | Training loss: 0.472927 | Validation loss: 0.618445\n",
      "Epoch: 103 | Training loss: 0.471198 | Validation loss: 0.616803\n",
      "Epoch: 104 | Training loss: 0.469489 | Validation loss: 0.615180\n",
      "Epoch: 105 | Training loss: 0.467799 | Validation loss: 0.613575\n",
      "Epoch: 106 | Training loss: 0.466128 | Validation loss: 0.611987\n",
      "Epoch: 107 | Training loss: 0.464475 | Validation loss: 0.610417\n",
      "Epoch: 108 | Training loss: 0.462840 | Validation loss: 0.608866\n",
      "Epoch: 109 | Training loss: 0.461223 | Validation loss: 0.607330\n",
      "Epoch: 110 | Training loss: 0.459624 | Validation loss: 0.605810\n",
      "Epoch: 111 | Training loss: 0.458041 | Validation loss: 0.604308\n",
      "Epoch: 112 | Training loss: 0.456476 | Validation loss: 0.602821\n",
      "Epoch: 113 | Training loss: 0.454927 | Validation loss: 0.601352\n",
      "Epoch: 114 | Training loss: 0.453394 | Validation loss: 0.599897\n",
      "Epoch: 115 | Training loss: 0.451877 | Validation loss: 0.598457\n",
      "Epoch: 116 | Training loss: 0.450375 | Validation loss: 0.597033\n",
      "Epoch: 117 | Training loss: 0.448889 | Validation loss: 0.595623\n",
      "Epoch: 118 | Training loss: 0.447417 | Validation loss: 0.594228\n",
      "Epoch: 119 | Training loss: 0.445961 | Validation loss: 0.592847\n",
      "Epoch: 120 | Training loss: 0.444519 | Validation loss: 0.591480\n",
      "Epoch: 121 | Training loss: 0.443091 | Validation loss: 0.590127\n",
      "Epoch: 122 | Training loss: 0.441677 | Validation loss: 0.588787\n",
      "Epoch: 123 | Training loss: 0.440277 | Validation loss: 0.587460\n",
      "Epoch: 124 | Training loss: 0.438891 | Validation loss: 0.586147\n",
      "Epoch: 125 | Training loss: 0.437518 | Validation loss: 0.584847\n",
      "Epoch: 126 | Training loss: 0.436158 | Validation loss: 0.583559\n",
      "Epoch: 127 | Training loss: 0.434810 | Validation loss: 0.582284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128 | Training loss: 0.433476 | Validation loss: 0.581021\n",
      "Epoch: 129 | Training loss: 0.432154 | Validation loss: 0.579771\n",
      "Epoch: 130 | Training loss: 0.430844 | Validation loss: 0.578532\n",
      "Epoch: 131 | Training loss: 0.429546 | Validation loss: 0.577305\n",
      "Epoch: 132 | Training loss: 0.428260 | Validation loss: 0.576089\n",
      "Epoch: 133 | Training loss: 0.426985 | Validation loss: 0.574884\n",
      "Epoch: 134 | Training loss: 0.425722 | Validation loss: 0.573691\n",
      "Epoch: 135 | Training loss: 0.424471 | Validation loss: 0.572509\n",
      "Epoch: 136 | Training loss: 0.423230 | Validation loss: 0.571338\n",
      "Epoch: 137 | Training loss: 0.422000 | Validation loss: 0.570178\n",
      "Epoch: 138 | Training loss: 0.420782 | Validation loss: 0.569028\n",
      "Epoch: 139 | Training loss: 0.419573 | Validation loss: 0.567889\n",
      "Epoch: 140 | Training loss: 0.418376 | Validation loss: 0.566760\n",
      "Epoch: 141 | Training loss: 0.417188 | Validation loss: 0.565641\n",
      "Epoch: 142 | Training loss: 0.416011 | Validation loss: 0.564532\n",
      "Epoch: 143 | Training loss: 0.414843 | Validation loss: 0.563431\n",
      "Epoch: 144 | Training loss: 0.413686 | Validation loss: 0.562342\n",
      "Epoch: 145 | Training loss: 0.412538 | Validation loss: 0.561261\n",
      "Epoch: 146 | Training loss: 0.411399 | Validation loss: 0.560190\n",
      "Epoch: 147 | Training loss: 0.410270 | Validation loss: 0.559128\n",
      "Epoch: 148 | Training loss: 0.409151 | Validation loss: 0.558075\n",
      "Epoch: 149 | Training loss: 0.408040 | Validation loss: 0.557031\n",
      "Epoch: 150 | Training loss: 0.406939 | Validation loss: 0.555996\n",
      "Epoch: 151 | Training loss: 0.405846 | Validation loss: 0.554970\n",
      "Epoch: 152 | Training loss: 0.404762 | Validation loss: 0.553953\n",
      "Epoch: 153 | Training loss: 0.403687 | Validation loss: 0.552944\n",
      "Epoch: 154 | Training loss: 0.402620 | Validation loss: 0.551943\n",
      "Epoch: 155 | Training loss: 0.401562 | Validation loss: 0.550951\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr_count, data_tr_label,vocab,\n",
    "                                             X_dev=X_dev_count, \n",
    "                                             Y_dev=data_dev_label,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.0001, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation process and explain if your model overfit, underfit or is about right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,len(loss_tr_count),len(loss_tr_count))\n",
    "y1, y2 = loss_tr_count, dev_loss_count\n",
    " \n",
    "plt.plot(x, y1,label='Training loss')\n",
    "plt.plot(x, y2, label='Validation loss')\n",
    " \n",
    "plt.title('Training Monitoring')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:32:12.606498Z",
     "start_time": "2020-02-15T14:32:12.604164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8533333333333334\n",
      "Precision: 0.8233333333333334\n",
      "Recall: 0.8233333333333334\n",
      "F1-Score: 0.8233333333333334\n"
     ]
    }
   ],
   "source": [
    "# fill in your code...\n",
    "preds_te_count = predict_class(X = X_test_count, weights = w_count)\n",
    "Y_te = data_test_label\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top-10 words for each class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:32:26.224693Z",
     "start_time": "2020-02-15T14:32:26.221886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words for class 1 are ['afp', 'said', 'president', 'minister', 'najaf', 'people', 'monday', 'iraq', 'troops', 'al'] respectively.\n",
      "The top 10 words for class 2 are ['athens', 'olympic', 'ap', 'team', ('athens', 'reuters'), 'win', 'games', 'olympics', 'game', 'when'] respectively.\n",
      "The top 10 words for class 3 are ['company', 'inc', 'oil', 'corp', 'billion', 'prices', 'business', 'sales', 'million', 'market'] respectively.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(w_count)):\n",
    "    \n",
    "    dict_store = {vocab[sid]:weight for sid,weight in enumerate(w_count[i])}\n",
    "    # Order By\n",
    "    dict_store = dict(sorted(dict_store.items(), key=lambda col:col[1],reverse=True)[:10])\n",
    "\n",
    "    print(\"The top 10 words for class %d are %s respectively.\"%(i+1,list(dict_store.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:16:19.856538Z",
     "start_time": "2020-02-15T14:16:19.852547Z"
    }
   },
   "source": [
    "Explain here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.990362 | Validation loss: 0.977769\n",
      "Epoch: 1 | Training loss: 0.840230 | Validation loss: 0.895535\n",
      "Epoch: 2 | Training loss: 0.747093 | Validation loss: 0.833415\n",
      "Epoch: 3 | Training loss: 0.681181 | Validation loss: 0.784788\n",
      "Epoch: 4 | Training loss: 0.631315 | Validation loss: 0.745370\n",
      "Epoch: 5 | Training loss: 0.591804 | Validation loss: 0.712715\n",
      "Epoch: 6 | Training loss: 0.559436 | Validation loss: 0.685196\n",
      "Epoch: 7 | Training loss: 0.532167 | Validation loss: 0.661514\n",
      "Epoch: 8 | Training loss: 0.508784 | Validation loss: 0.640886\n",
      "Epoch: 9 | Training loss: 0.488371 | Validation loss: 0.622766\n",
      "Epoch: 10 | Training loss: 0.470339 | Validation loss: 0.606647\n",
      "Epoch: 11 | Training loss: 0.454241 | Validation loss: 0.592179\n",
      "Epoch: 12 | Training loss: 0.439729 | Validation loss: 0.579122\n",
      "Epoch: 13 | Training loss: 0.426550 | Validation loss: 0.567233\n",
      "Epoch: 14 | Training loss: 0.414496 | Validation loss: 0.556338\n",
      "Epoch: 15 | Training loss: 0.403425 | Validation loss: 0.546391\n",
      "Epoch: 16 | Training loss: 0.393194 | Validation loss: 0.537241\n",
      "Epoch: 17 | Training loss: 0.383698 | Validation loss: 0.528788\n",
      "Epoch: 18 | Training loss: 0.374844 | Validation loss: 0.520894\n",
      "Epoch: 19 | Training loss: 0.366565 | Validation loss: 0.513538\n",
      "Epoch: 20 | Training loss: 0.358788 | Validation loss: 0.506665\n",
      "Epoch: 21 | Training loss: 0.351492 | Validation loss: 0.500204\n",
      "Epoch: 22 | Training loss: 0.344595 | Validation loss: 0.494105\n",
      "Epoch: 23 | Training loss: 0.338077 | Validation loss: 0.488391\n",
      "Epoch: 24 | Training loss: 0.331905 | Validation loss: 0.482995\n",
      "Epoch: 25 | Training loss: 0.326036 | Validation loss: 0.477865\n",
      "Epoch: 26 | Training loss: 0.320453 | Validation loss: 0.473042\n",
      "Epoch: 27 | Training loss: 0.315135 | Validation loss: 0.468453\n",
      "Epoch: 28 | Training loss: 0.310057 | Validation loss: 0.464070\n",
      "Epoch: 29 | Training loss: 0.305203 | Validation loss: 0.459905\n",
      "Epoch: 30 | Training loss: 0.300554 | Validation loss: 0.455920\n",
      "Epoch: 31 | Training loss: 0.296096 | Validation loss: 0.452127\n",
      "Epoch: 32 | Training loss: 0.291808 | Validation loss: 0.448500\n",
      "Epoch: 33 | Training loss: 0.287703 | Validation loss: 0.445029\n",
      "Epoch: 34 | Training loss: 0.283741 | Validation loss: 0.441705\n",
      "Epoch: 35 | Training loss: 0.279931 | Validation loss: 0.438493\n",
      "Epoch: 36 | Training loss: 0.276256 | Validation loss: 0.435415\n",
      "Epoch: 37 | Training loss: 0.272708 | Validation loss: 0.432458\n",
      "Epoch: 38 | Training loss: 0.269280 | Validation loss: 0.429607\n",
      "Epoch: 39 | Training loss: 0.265966 | Validation loss: 0.426877\n",
      "Epoch: 40 | Training loss: 0.262757 | Validation loss: 0.424220\n",
      "Epoch: 41 | Training loss: 0.259653 | Validation loss: 0.421674\n",
      "Epoch: 42 | Training loss: 0.256642 | Validation loss: 0.419198\n",
      "Epoch: 43 | Training loss: 0.253725 | Validation loss: 0.416828\n",
      "Epoch: 44 | Training loss: 0.250892 | Validation loss: 0.414537\n",
      "Epoch: 45 | Training loss: 0.248141 | Validation loss: 0.412309\n",
      "Epoch: 46 | Training loss: 0.245468 | Validation loss: 0.410146\n",
      "Epoch: 47 | Training loss: 0.242869 | Validation loss: 0.408079\n",
      "Epoch: 48 | Training loss: 0.240341 | Validation loss: 0.406046\n",
      "Epoch: 49 | Training loss: 0.237881 | Validation loss: 0.404072\n",
      "Epoch: 50 | Training loss: 0.235484 | Validation loss: 0.402146\n",
      "Epoch: 51 | Training loss: 0.233150 | Validation loss: 0.400296\n",
      "Epoch: 52 | Training loss: 0.230874 | Validation loss: 0.398483\n",
      "Epoch: 53 | Training loss: 0.228655 | Validation loss: 0.396735\n",
      "Epoch: 54 | Training loss: 0.226488 | Validation loss: 0.395040\n",
      "Epoch: 55 | Training loss: 0.224372 | Validation loss: 0.393373\n",
      "Epoch: 56 | Training loss: 0.222308 | Validation loss: 0.391747\n",
      "Epoch: 57 | Training loss: 0.220290 | Validation loss: 0.390183\n",
      "Epoch: 58 | Training loss: 0.218317 | Validation loss: 0.388678\n",
      "Epoch: 59 | Training loss: 0.216391 | Validation loss: 0.387191\n",
      "Epoch: 60 | Training loss: 0.214506 | Validation loss: 0.385733\n",
      "Epoch: 61 | Training loss: 0.212661 | Validation loss: 0.384317\n",
      "Epoch: 62 | Training loss: 0.210855 | Validation loss: 0.382937\n",
      "Epoch: 63 | Training loss: 0.209088 | Validation loss: 0.381568\n",
      "Epoch: 64 | Training loss: 0.207354 | Validation loss: 0.380251\n",
      "Epoch: 65 | Training loss: 0.205660 | Validation loss: 0.378945\n",
      "Epoch: 66 | Training loss: 0.203994 | Validation loss: 0.377652\n",
      "Epoch: 67 | Training loss: 0.202371 | Validation loss: 0.376424\n",
      "Epoch: 68 | Training loss: 0.200771 | Validation loss: 0.375198\n",
      "Epoch: 69 | Training loss: 0.199209 | Validation loss: 0.374035\n",
      "Epoch: 70 | Training loss: 0.197672 | Validation loss: 0.372909\n",
      "Epoch: 71 | Training loss: 0.196163 | Validation loss: 0.371799\n",
      "Epoch: 72 | Training loss: 0.194681 | Validation loss: 0.370720\n",
      "Epoch: 73 | Training loss: 0.193228 | Validation loss: 0.369628\n",
      "Epoch: 74 | Training loss: 0.191803 | Validation loss: 0.368586\n",
      "Epoch: 75 | Training loss: 0.190398 | Validation loss: 0.367553\n",
      "Epoch: 76 | Training loss: 0.189024 | Validation loss: 0.366531\n",
      "Epoch: 77 | Training loss: 0.187672 | Validation loss: 0.365546\n",
      "lr = 0.001000, alpha= 0.001000; Predict_Class:0.827243\n",
      "Epoch: 0 | Training loss: 0.990602 | Validation loss: 0.977745\n",
      "Epoch: 1 | Training loss: 0.840381 | Validation loss: 0.895537\n",
      "Epoch: 2 | Training loss: 0.747175 | Validation loss: 0.833714\n",
      "Epoch: 3 | Training loss: 0.681331 | Validation loss: 0.785051\n",
      "Epoch: 4 | Training loss: 0.631415 | Validation loss: 0.745609\n",
      "Epoch: 5 | Training loss: 0.591891 | Validation loss: 0.713017\n",
      "Epoch: 6 | Training loss: 0.559500 | Validation loss: 0.685475\n",
      "Epoch: 7 | Training loss: 0.532220 | Validation loss: 0.661820\n",
      "Epoch: 8 | Training loss: 0.508801 | Validation loss: 0.641125\n",
      "Epoch: 9 | Training loss: 0.488391 | Validation loss: 0.622933\n",
      "Epoch: 10 | Training loss: 0.470366 | Validation loss: 0.606820\n",
      "Epoch: 11 | Training loss: 0.454259 | Validation loss: 0.592334\n",
      "Epoch: 12 | Training loss: 0.439742 | Validation loss: 0.579290\n",
      "Epoch: 13 | Training loss: 0.426555 | Validation loss: 0.567436\n",
      "Epoch: 14 | Training loss: 0.414507 | Validation loss: 0.556571\n",
      "Epoch: 15 | Training loss: 0.403429 | Validation loss: 0.546640\n",
      "Epoch: 16 | Training loss: 0.393187 | Validation loss: 0.537505\n",
      "Epoch: 17 | Training loss: 0.383688 | Validation loss: 0.528986\n",
      "Epoch: 18 | Training loss: 0.374835 | Validation loss: 0.521064\n",
      "Epoch: 19 | Training loss: 0.366554 | Validation loss: 0.513686\n",
      "Epoch: 20 | Training loss: 0.358786 | Validation loss: 0.506807\n",
      "Epoch: 21 | Training loss: 0.351480 | Validation loss: 0.500334\n",
      "Epoch: 22 | Training loss: 0.344582 | Validation loss: 0.494234\n",
      "Epoch: 23 | Training loss: 0.338066 | Validation loss: 0.488493\n",
      "Epoch: 24 | Training loss: 0.331892 | Validation loss: 0.483100\n",
      "Epoch: 25 | Training loss: 0.326026 | Validation loss: 0.477988\n",
      "Epoch: 26 | Training loss: 0.320446 | Validation loss: 0.473141\n",
      "Epoch: 27 | Training loss: 0.315127 | Validation loss: 0.468554\n",
      "Epoch: 28 | Training loss: 0.310047 | Validation loss: 0.464165\n",
      "Epoch: 29 | Training loss: 0.305193 | Validation loss: 0.459977\n",
      "Epoch: 30 | Training loss: 0.300541 | Validation loss: 0.455997\n",
      "Epoch: 31 | Training loss: 0.296083 | Validation loss: 0.452214\n",
      "Epoch: 32 | Training loss: 0.291801 | Validation loss: 0.448618\n",
      "Epoch: 33 | Training loss: 0.287690 | Validation loss: 0.445115\n",
      "Epoch: 34 | Training loss: 0.283731 | Validation loss: 0.441778\n",
      "Epoch: 35 | Training loss: 0.279917 | Validation loss: 0.438607\n",
      "Epoch: 36 | Training loss: 0.276242 | Validation loss: 0.435522\n",
      "Epoch: 37 | Training loss: 0.272686 | Validation loss: 0.432503\n",
      "Epoch: 38 | Training loss: 0.269267 | Validation loss: 0.429671\n",
      "Epoch: 39 | Training loss: 0.265954 | Validation loss: 0.426931\n",
      "Epoch: 40 | Training loss: 0.262749 | Validation loss: 0.424274\n",
      "Epoch: 41 | Training loss: 0.259644 | Validation loss: 0.421728\n",
      "Epoch: 42 | Training loss: 0.256632 | Validation loss: 0.419247\n",
      "Epoch: 43 | Training loss: 0.253716 | Validation loss: 0.416879\n",
      "Epoch: 44 | Training loss: 0.250875 | Validation loss: 0.414529\n",
      "Epoch: 45 | Training loss: 0.248134 | Validation loss: 0.412316\n",
      "Epoch: 46 | Training loss: 0.245460 | Validation loss: 0.410173\n",
      "Epoch: 47 | Training loss: 0.242860 | Validation loss: 0.408112\n",
      "Epoch: 48 | Training loss: 0.240329 | Validation loss: 0.406081\n",
      "Epoch: 49 | Training loss: 0.237871 | Validation loss: 0.404099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Training loss: 0.235477 | Validation loss: 0.402190\n",
      "Epoch: 51 | Training loss: 0.233141 | Validation loss: 0.400315\n",
      "Epoch: 52 | Training loss: 0.230861 | Validation loss: 0.398472\n",
      "Epoch: 53 | Training loss: 0.228646 | Validation loss: 0.396730\n",
      "Epoch: 54 | Training loss: 0.226478 | Validation loss: 0.395014\n",
      "Epoch: 55 | Training loss: 0.224364 | Validation loss: 0.393375\n",
      "Epoch: 56 | Training loss: 0.222299 | Validation loss: 0.391754\n",
      "Epoch: 57 | Training loss: 0.220282 | Validation loss: 0.390183\n",
      "Epoch: 58 | Training loss: 0.218312 | Validation loss: 0.388654\n",
      "Epoch: 59 | Training loss: 0.216383 | Validation loss: 0.387165\n",
      "Epoch: 60 | Training loss: 0.214498 | Validation loss: 0.385720\n",
      "Epoch: 61 | Training loss: 0.212653 | Validation loss: 0.384299\n",
      "Epoch: 62 | Training loss: 0.210850 | Validation loss: 0.382920\n",
      "Epoch: 63 | Training loss: 0.209081 | Validation loss: 0.381562\n",
      "Epoch: 64 | Training loss: 0.207351 | Validation loss: 0.380262\n",
      "Epoch: 65 | Training loss: 0.205654 | Validation loss: 0.378977\n",
      "Epoch: 66 | Training loss: 0.203992 | Validation loss: 0.377725\n",
      "Epoch: 67 | Training loss: 0.202363 | Validation loss: 0.376486\n",
      "Epoch: 68 | Training loss: 0.200763 | Validation loss: 0.375290\n",
      "Epoch: 69 | Training loss: 0.199198 | Validation loss: 0.374098\n",
      "Epoch: 70 | Training loss: 0.197663 | Validation loss: 0.372941\n",
      "Epoch: 71 | Training loss: 0.196156 | Validation loss: 0.371809\n",
      "Epoch: 72 | Training loss: 0.194675 | Validation loss: 0.370703\n",
      "Epoch: 73 | Training loss: 0.193223 | Validation loss: 0.369627\n",
      "Epoch: 74 | Training loss: 0.191797 | Validation loss: 0.368581\n",
      "Epoch: 75 | Training loss: 0.190396 | Validation loss: 0.367554\n",
      "Epoch: 76 | Training loss: 0.189018 | Validation loss: 0.366540\n",
      "Epoch: 77 | Training loss: 0.187664 | Validation loss: 0.365540\n",
      "Epoch: 78 | Training loss: 0.186336 | Validation loss: 0.364574\n",
      "lr = 0.001000, alpha= 0.000500; Predict_Class:0.825871\n",
      "Epoch: 0 | Training loss: 0.990624 | Validation loss: 0.978114\n",
      "Epoch: 1 | Training loss: 0.840351 | Validation loss: 0.895639\n",
      "Epoch: 2 | Training loss: 0.747046 | Validation loss: 0.833517\n",
      "Epoch: 3 | Training loss: 0.681165 | Validation loss: 0.784850\n",
      "Epoch: 4 | Training loss: 0.631286 | Validation loss: 0.745488\n",
      "Epoch: 5 | Training loss: 0.591803 | Validation loss: 0.712826\n",
      "Epoch: 6 | Training loss: 0.559423 | Validation loss: 0.685282\n",
      "Epoch: 7 | Training loss: 0.532150 | Validation loss: 0.661541\n",
      "Epoch: 8 | Training loss: 0.508776 | Validation loss: 0.640966\n",
      "Epoch: 9 | Training loss: 0.488375 | Validation loss: 0.622821\n",
      "Epoch: 10 | Training loss: 0.470354 | Validation loss: 0.606717\n",
      "Epoch: 11 | Training loss: 0.454250 | Validation loss: 0.592227\n",
      "Epoch: 12 | Training loss: 0.439731 | Validation loss: 0.579159\n",
      "Epoch: 13 | Training loss: 0.426543 | Validation loss: 0.567244\n",
      "Epoch: 14 | Training loss: 0.414509 | Validation loss: 0.556433\n",
      "Epoch: 15 | Training loss: 0.403433 | Validation loss: 0.546507\n",
      "Epoch: 16 | Training loss: 0.393199 | Validation loss: 0.537325\n",
      "Epoch: 17 | Training loss: 0.383697 | Validation loss: 0.528842\n",
      "Epoch: 18 | Training loss: 0.374851 | Validation loss: 0.520943\n",
      "Epoch: 19 | Training loss: 0.366564 | Validation loss: 0.513530\n",
      "Epoch: 20 | Training loss: 0.358802 | Validation loss: 0.506639\n",
      "Epoch: 21 | Training loss: 0.351496 | Validation loss: 0.500190\n",
      "Epoch: 22 | Training loss: 0.344606 | Validation loss: 0.494132\n",
      "Epoch: 23 | Training loss: 0.338074 | Validation loss: 0.488374\n",
      "Epoch: 24 | Training loss: 0.331905 | Validation loss: 0.482999\n",
      "Epoch: 25 | Training loss: 0.326041 | Validation loss: 0.477916\n",
      "Epoch: 26 | Training loss: 0.320458 | Validation loss: 0.473087\n",
      "Epoch: 27 | Training loss: 0.315139 | Validation loss: 0.468505\n",
      "Epoch: 28 | Training loss: 0.310059 | Validation loss: 0.464118\n",
      "Epoch: 29 | Training loss: 0.305206 | Validation loss: 0.459956\n",
      "Epoch: 30 | Training loss: 0.300554 | Validation loss: 0.455970\n",
      "Epoch: 31 | Training loss: 0.296095 | Validation loss: 0.452166\n",
      "Epoch: 32 | Training loss: 0.291814 | Validation loss: 0.448550\n",
      "Epoch: 33 | Training loss: 0.287701 | Validation loss: 0.445067\n",
      "Epoch: 34 | Training loss: 0.283743 | Validation loss: 0.441729\n",
      "Epoch: 35 | Training loss: 0.279931 | Validation loss: 0.438541\n",
      "Epoch: 36 | Training loss: 0.276255 | Validation loss: 0.435483\n",
      "Epoch: 37 | Training loss: 0.272703 | Validation loss: 0.432507\n",
      "Epoch: 38 | Training loss: 0.269279 | Validation loss: 0.429660\n",
      "Epoch: 39 | Training loss: 0.265962 | Validation loss: 0.426897\n",
      "Epoch: 40 | Training loss: 0.262756 | Validation loss: 0.424255\n",
      "Epoch: 41 | Training loss: 0.259651 | Validation loss: 0.421696\n",
      "Epoch: 42 | Training loss: 0.256642 | Validation loss: 0.419235\n",
      "Epoch: 43 | Training loss: 0.253724 | Validation loss: 0.416840\n",
      "Epoch: 44 | Training loss: 0.250891 | Validation loss: 0.414531\n",
      "Epoch: 45 | Training loss: 0.248137 | Validation loss: 0.412267\n",
      "Epoch: 46 | Training loss: 0.245468 | Validation loss: 0.410108\n",
      "Epoch: 47 | Training loss: 0.242871 | Validation loss: 0.408030\n",
      "Epoch: 48 | Training loss: 0.240343 | Validation loss: 0.406017\n",
      "Epoch: 49 | Training loss: 0.237879 | Validation loss: 0.404054\n",
      "Epoch: 50 | Training loss: 0.235485 | Validation loss: 0.402156\n",
      "Epoch: 51 | Training loss: 0.233149 | Validation loss: 0.400306\n",
      "Epoch: 52 | Training loss: 0.230872 | Validation loss: 0.398512\n",
      "Epoch: 53 | Training loss: 0.228652 | Validation loss: 0.396762\n",
      "Epoch: 54 | Training loss: 0.226486 | Validation loss: 0.395064\n",
      "Epoch: 55 | Training loss: 0.224372 | Validation loss: 0.393414\n",
      "Epoch: 56 | Training loss: 0.222303 | Validation loss: 0.391805\n",
      "Epoch: 57 | Training loss: 0.220290 | Validation loss: 0.390232\n",
      "Epoch: 58 | Training loss: 0.218316 | Validation loss: 0.388716\n",
      "Epoch: 59 | Training loss: 0.216390 | Validation loss: 0.387208\n",
      "Epoch: 60 | Training loss: 0.214501 | Validation loss: 0.385718\n",
      "Epoch: 61 | Training loss: 0.212660 | Validation loss: 0.384310\n",
      "Epoch: 62 | Training loss: 0.210848 | Validation loss: 0.382895\n",
      "Epoch: 63 | Training loss: 0.209088 | Validation loss: 0.381551\n",
      "Epoch: 64 | Training loss: 0.207357 | Validation loss: 0.380252\n",
      "Epoch: 65 | Training loss: 0.205657 | Validation loss: 0.378976\n",
      "Epoch: 66 | Training loss: 0.203998 | Validation loss: 0.377726\n",
      "Epoch: 67 | Training loss: 0.202368 | Validation loss: 0.376496\n",
      "Epoch: 68 | Training loss: 0.200772 | Validation loss: 0.375307\n",
      "Epoch: 69 | Training loss: 0.199205 | Validation loss: 0.374157\n",
      "Epoch: 70 | Training loss: 0.197669 | Validation loss: 0.373007\n",
      "Epoch: 71 | Training loss: 0.196161 | Validation loss: 0.371868\n",
      "Epoch: 72 | Training loss: 0.194679 | Validation loss: 0.370757\n",
      "Epoch: 73 | Training loss: 0.193228 | Validation loss: 0.369690\n",
      "Epoch: 74 | Training loss: 0.191801 | Validation loss: 0.368638\n",
      "Epoch: 75 | Training loss: 0.190399 | Validation loss: 0.367612\n",
      "Epoch: 76 | Training loss: 0.189023 | Validation loss: 0.366593\n",
      "Epoch: 77 | Training loss: 0.187670 | Validation loss: 0.365601\n",
      "lr = 0.001000, alpha= 0.000100; Predict_Class:0.827243\n",
      "Epoch: 0 | Training loss: 1.036428 | Validation loss: 1.030935\n",
      "Epoch: 1 | Training loss: 0.937166 | Validation loss: 0.978131\n",
      "Epoch: 2 | Training loss: 0.865730 | Validation loss: 0.933858\n",
      "Epoch: 3 | Training loss: 0.809748 | Validation loss: 0.895964\n",
      "Epoch: 4 | Training loss: 0.764142 | Validation loss: 0.862914\n",
      "Epoch: 5 | Training loss: 0.726043 | Validation loss: 0.833930\n",
      "Epoch: 6 | Training loss: 0.693571 | Validation loss: 0.808239\n",
      "Epoch: 7 | Training loss: 0.665479 | Validation loss: 0.785257\n",
      "Epoch: 8 | Training loss: 0.640838 | Validation loss: 0.764574\n",
      "Epoch: 9 | Training loss: 0.618968 | Validation loss: 0.745874\n",
      "Epoch: 10 | Training loss: 0.599390 | Validation loss: 0.728837\n",
      "Epoch: 11 | Training loss: 0.581705 | Validation loss: 0.713242\n",
      "Epoch: 12 | Training loss: 0.565625 | Validation loss: 0.698921\n",
      "Epoch: 13 | Training loss: 0.550912 | Validation loss: 0.685682\n",
      "Epoch: 14 | Training loss: 0.537370 | Validation loss: 0.673415\n",
      "Epoch: 15 | Training loss: 0.524845 | Validation loss: 0.661996\n",
      "Epoch: 16 | Training loss: 0.513212 | Validation loss: 0.651357\n",
      "Epoch: 17 | Training loss: 0.502361 | Validation loss: 0.641390\n",
      "Epoch: 18 | Training loss: 0.492206 | Validation loss: 0.632025\n",
      "Epoch: 19 | Training loss: 0.482674 | Validation loss: 0.623212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Training loss: 0.473697 | Validation loss: 0.614907\n",
      "Epoch: 21 | Training loss: 0.465223 | Validation loss: 0.607072\n",
      "Epoch: 22 | Training loss: 0.457203 | Validation loss: 0.599632\n",
      "Epoch: 23 | Training loss: 0.449596 | Validation loss: 0.592581\n",
      "Epoch: 24 | Training loss: 0.442369 | Validation loss: 0.585893\n",
      "Epoch: 25 | Training loss: 0.435484 | Validation loss: 0.579515\n",
      "Epoch: 26 | Training loss: 0.428920 | Validation loss: 0.573444\n",
      "Epoch: 27 | Training loss: 0.422646 | Validation loss: 0.567651\n",
      "Epoch: 28 | Training loss: 0.416641 | Validation loss: 0.562120\n",
      "Epoch: 29 | Training loss: 0.410890 | Validation loss: 0.556818\n",
      "Epoch: 30 | Training loss: 0.405368 | Validation loss: 0.551737\n",
      "Epoch: 31 | Training loss: 0.400064 | Validation loss: 0.546861\n",
      "Epoch: 32 | Training loss: 0.394961 | Validation loss: 0.542181\n",
      "Epoch: 33 | Training loss: 0.390050 | Validation loss: 0.537682\n",
      "Epoch: 34 | Training loss: 0.385314 | Validation loss: 0.533345\n",
      "Epoch: 35 | Training loss: 0.380743 | Validation loss: 0.529179\n",
      "Epoch: 36 | Training loss: 0.376331 | Validation loss: 0.525155\n",
      "Epoch: 37 | Training loss: 0.372064 | Validation loss: 0.521264\n",
      "Epoch: 38 | Training loss: 0.367936 | Validation loss: 0.517512\n",
      "Epoch: 39 | Training loss: 0.363938 | Validation loss: 0.513890\n",
      "Epoch: 40 | Training loss: 0.360065 | Validation loss: 0.510386\n",
      "Epoch: 41 | Training loss: 0.356312 | Validation loss: 0.506993\n",
      "Epoch: 42 | Training loss: 0.352668 | Validation loss: 0.503701\n",
      "Epoch: 43 | Training loss: 0.349129 | Validation loss: 0.500518\n",
      "Epoch: 44 | Training loss: 0.345694 | Validation loss: 0.497432\n",
      "Epoch: 45 | Training loss: 0.342351 | Validation loss: 0.494437\n",
      "Epoch: 46 | Training loss: 0.339101 | Validation loss: 0.491528\n",
      "Epoch: 47 | Training loss: 0.335938 | Validation loss: 0.488708\n",
      "Epoch: 48 | Training loss: 0.332858 | Validation loss: 0.485962\n",
      "Epoch: 49 | Training loss: 0.329856 | Validation loss: 0.483300\n",
      "Epoch: 50 | Training loss: 0.326931 | Validation loss: 0.480705\n",
      "Epoch: 51 | Training loss: 0.324077 | Validation loss: 0.478176\n",
      "Epoch: 52 | Training loss: 0.321293 | Validation loss: 0.475712\n",
      "Epoch: 53 | Training loss: 0.318577 | Validation loss: 0.473318\n",
      "Epoch: 54 | Training loss: 0.315924 | Validation loss: 0.470982\n",
      "Epoch: 55 | Training loss: 0.313333 | Validation loss: 0.468712\n",
      "Epoch: 56 | Training loss: 0.310800 | Validation loss: 0.466500\n",
      "Epoch: 57 | Training loss: 0.308323 | Validation loss: 0.464339\n",
      "Epoch: 58 | Training loss: 0.305900 | Validation loss: 0.462221\n",
      "Epoch: 59 | Training loss: 0.303532 | Validation loss: 0.460162\n",
      "Epoch: 60 | Training loss: 0.301212 | Validation loss: 0.458153\n",
      "Epoch: 61 | Training loss: 0.298942 | Validation loss: 0.456184\n",
      "Epoch: 62 | Training loss: 0.296718 | Validation loss: 0.454262\n",
      "Epoch: 63 | Training loss: 0.294541 | Validation loss: 0.452380\n",
      "Epoch: 64 | Training loss: 0.292406 | Validation loss: 0.450539\n",
      "Epoch: 65 | Training loss: 0.290312 | Validation loss: 0.448733\n",
      "Epoch: 66 | Training loss: 0.288261 | Validation loss: 0.446969\n",
      "Epoch: 67 | Training loss: 0.286249 | Validation loss: 0.445241\n",
      "Epoch: 68 | Training loss: 0.284275 | Validation loss: 0.443552\n",
      "Epoch: 69 | Training loss: 0.282338 | Validation loss: 0.441900\n",
      "Epoch: 70 | Training loss: 0.280436 | Validation loss: 0.440286\n",
      "Epoch: 71 | Training loss: 0.278568 | Validation loss: 0.438695\n",
      "Epoch: 72 | Training loss: 0.276735 | Validation loss: 0.437139\n",
      "Epoch: 73 | Training loss: 0.274934 | Validation loss: 0.435620\n",
      "Epoch: 74 | Training loss: 0.273164 | Validation loss: 0.434120\n",
      "Epoch: 75 | Training loss: 0.271424 | Validation loss: 0.432649\n",
      "Epoch: 76 | Training loss: 0.269715 | Validation loss: 0.431214\n",
      "Epoch: 77 | Training loss: 0.268032 | Validation loss: 0.429789\n",
      "Epoch: 78 | Training loss: 0.266381 | Validation loss: 0.428409\n",
      "Epoch: 79 | Training loss: 0.264754 | Validation loss: 0.427049\n",
      "Epoch: 80 | Training loss: 0.263155 | Validation loss: 0.425723\n",
      "Epoch: 81 | Training loss: 0.261580 | Validation loss: 0.424418\n",
      "Epoch: 82 | Training loss: 0.260031 | Validation loss: 0.423127\n",
      "Epoch: 83 | Training loss: 0.258506 | Validation loss: 0.421854\n",
      "Epoch: 84 | Training loss: 0.257004 | Validation loss: 0.420597\n",
      "Epoch: 85 | Training loss: 0.255527 | Validation loss: 0.419371\n",
      "Epoch: 86 | Training loss: 0.254071 | Validation loss: 0.418169\n",
      "Epoch: 87 | Training loss: 0.252637 | Validation loss: 0.416989\n",
      "Epoch: 88 | Training loss: 0.251224 | Validation loss: 0.415821\n",
      "Epoch: 89 | Training loss: 0.249831 | Validation loss: 0.414678\n",
      "Epoch: 90 | Training loss: 0.248459 | Validation loss: 0.413542\n",
      "Epoch: 91 | Training loss: 0.247107 | Validation loss: 0.412434\n",
      "Epoch: 92 | Training loss: 0.245774 | Validation loss: 0.411348\n",
      "Epoch: 93 | Training loss: 0.244460 | Validation loss: 0.410278\n",
      "Epoch: 94 | Training loss: 0.243163 | Validation loss: 0.409224\n",
      "Epoch: 95 | Training loss: 0.241884 | Validation loss: 0.408180\n",
      "Epoch: 96 | Training loss: 0.240623 | Validation loss: 0.407164\n",
      "Epoch: 97 | Training loss: 0.239379 | Validation loss: 0.406154\n",
      "Epoch: 98 | Training loss: 0.238150 | Validation loss: 0.405153\n",
      "Epoch: 99 | Training loss: 0.236940 | Validation loss: 0.404181\n",
      "lr = 0.000500, alpha= 0.001000; Predict_Class:0.831947\n",
      "Epoch: 0 | Training loss: 1.036607 | Validation loss: 1.031025\n",
      "Epoch: 1 | Training loss: 0.937346 | Validation loss: 0.978162\n",
      "Epoch: 2 | Training loss: 0.865740 | Validation loss: 0.933890\n",
      "Epoch: 3 | Training loss: 0.809700 | Validation loss: 0.895950\n",
      "Epoch: 4 | Training loss: 0.764098 | Validation loss: 0.862943\n",
      "Epoch: 5 | Training loss: 0.726014 | Validation loss: 0.833947\n",
      "Epoch: 6 | Training loss: 0.693564 | Validation loss: 0.808250\n",
      "Epoch: 7 | Training loss: 0.665469 | Validation loss: 0.785245\n",
      "Epoch: 8 | Training loss: 0.640832 | Validation loss: 0.764589\n",
      "Epoch: 9 | Training loss: 0.618970 | Validation loss: 0.745884\n",
      "Epoch: 10 | Training loss: 0.599379 | Validation loss: 0.728839\n",
      "Epoch: 11 | Training loss: 0.581700 | Validation loss: 0.713229\n",
      "Epoch: 12 | Training loss: 0.565614 | Validation loss: 0.698886\n",
      "Epoch: 13 | Training loss: 0.550894 | Validation loss: 0.685649\n",
      "Epoch: 14 | Training loss: 0.537349 | Validation loss: 0.673373\n",
      "Epoch: 15 | Training loss: 0.524820 | Validation loss: 0.661959\n",
      "Epoch: 16 | Training loss: 0.513190 | Validation loss: 0.651314\n",
      "Epoch: 17 | Training loss: 0.502345 | Validation loss: 0.641350\n",
      "Epoch: 18 | Training loss: 0.492196 | Validation loss: 0.631998\n",
      "Epoch: 19 | Training loss: 0.482665 | Validation loss: 0.623191\n",
      "Epoch: 20 | Training loss: 0.473688 | Validation loss: 0.614889\n",
      "Epoch: 21 | Training loss: 0.465215 | Validation loss: 0.607053\n",
      "Epoch: 22 | Training loss: 0.457197 | Validation loss: 0.599628\n",
      "Epoch: 23 | Training loss: 0.449594 | Validation loss: 0.592582\n",
      "Epoch: 24 | Training loss: 0.442365 | Validation loss: 0.585885\n",
      "Epoch: 25 | Training loss: 0.435479 | Validation loss: 0.579505\n",
      "Epoch: 26 | Training loss: 0.428915 | Validation loss: 0.573437\n",
      "Epoch: 27 | Training loss: 0.422642 | Validation loss: 0.567640\n",
      "Epoch: 28 | Training loss: 0.416638 | Validation loss: 0.562104\n",
      "Epoch: 29 | Training loss: 0.410887 | Validation loss: 0.556794\n",
      "Epoch: 30 | Training loss: 0.405368 | Validation loss: 0.551712\n",
      "Epoch: 31 | Training loss: 0.400063 | Validation loss: 0.546829\n",
      "Epoch: 32 | Training loss: 0.394959 | Validation loss: 0.542142\n",
      "Epoch: 33 | Training loss: 0.390046 | Validation loss: 0.537637\n",
      "Epoch: 34 | Training loss: 0.385312 | Validation loss: 0.533313\n",
      "Epoch: 35 | Training loss: 0.380741 | Validation loss: 0.529135\n",
      "Epoch: 36 | Training loss: 0.376329 | Validation loss: 0.525112\n",
      "Epoch: 37 | Training loss: 0.372062 | Validation loss: 0.521240\n",
      "Epoch: 38 | Training loss: 0.367934 | Validation loss: 0.517491\n",
      "Epoch: 39 | Training loss: 0.363937 | Validation loss: 0.513870\n",
      "Epoch: 40 | Training loss: 0.360065 | Validation loss: 0.510361\n",
      "Epoch: 41 | Training loss: 0.356310 | Validation loss: 0.506973\n",
      "Epoch: 42 | Training loss: 0.352667 | Validation loss: 0.503691\n",
      "Epoch: 43 | Training loss: 0.349130 | Validation loss: 0.500506\n",
      "Epoch: 44 | Training loss: 0.345693 | Validation loss: 0.497424\n",
      "Epoch: 45 | Training loss: 0.342352 | Validation loss: 0.494433\n",
      "Epoch: 46 | Training loss: 0.339102 | Validation loss: 0.491528\n",
      "Epoch: 47 | Training loss: 0.335938 | Validation loss: 0.488702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Training loss: 0.332858 | Validation loss: 0.485956\n",
      "Epoch: 49 | Training loss: 0.329858 | Validation loss: 0.483297\n",
      "Epoch: 50 | Training loss: 0.326931 | Validation loss: 0.480699\n",
      "Epoch: 51 | Training loss: 0.324078 | Validation loss: 0.478177\n",
      "Epoch: 52 | Training loss: 0.321292 | Validation loss: 0.475705\n",
      "Epoch: 53 | Training loss: 0.318579 | Validation loss: 0.473316\n",
      "Epoch: 54 | Training loss: 0.315924 | Validation loss: 0.470977\n",
      "Epoch: 55 | Training loss: 0.313334 | Validation loss: 0.468708\n",
      "Epoch: 56 | Training loss: 0.310800 | Validation loss: 0.466493\n",
      "Epoch: 57 | Training loss: 0.308325 | Validation loss: 0.464332\n",
      "Epoch: 58 | Training loss: 0.305903 | Validation loss: 0.462223\n",
      "Epoch: 59 | Training loss: 0.303533 | Validation loss: 0.460165\n",
      "Epoch: 60 | Training loss: 0.301214 | Validation loss: 0.458150\n",
      "Epoch: 61 | Training loss: 0.298943 | Validation loss: 0.456184\n",
      "Epoch: 62 | Training loss: 0.296720 | Validation loss: 0.454257\n",
      "Epoch: 63 | Training loss: 0.294540 | Validation loss: 0.452369\n",
      "Epoch: 64 | Training loss: 0.292407 | Validation loss: 0.450526\n",
      "Epoch: 65 | Training loss: 0.290314 | Validation loss: 0.448725\n",
      "Epoch: 66 | Training loss: 0.288262 | Validation loss: 0.446961\n",
      "Epoch: 67 | Training loss: 0.286251 | Validation loss: 0.445241\n",
      "Epoch: 68 | Training loss: 0.284276 | Validation loss: 0.443550\n",
      "Epoch: 69 | Training loss: 0.282338 | Validation loss: 0.441905\n",
      "Epoch: 70 | Training loss: 0.280437 | Validation loss: 0.440292\n",
      "Epoch: 71 | Training loss: 0.278569 | Validation loss: 0.438712\n",
      "Epoch: 72 | Training loss: 0.276736 | Validation loss: 0.437157\n",
      "Epoch: 73 | Training loss: 0.274935 | Validation loss: 0.435635\n",
      "Epoch: 74 | Training loss: 0.273165 | Validation loss: 0.434137\n",
      "Epoch: 75 | Training loss: 0.271425 | Validation loss: 0.432678\n",
      "Epoch: 76 | Training loss: 0.269716 | Validation loss: 0.431237\n",
      "Epoch: 77 | Training loss: 0.268034 | Validation loss: 0.429823\n",
      "Epoch: 78 | Training loss: 0.266380 | Validation loss: 0.428440\n",
      "Epoch: 79 | Training loss: 0.264755 | Validation loss: 0.427073\n",
      "Epoch: 80 | Training loss: 0.263156 | Validation loss: 0.425735\n",
      "Epoch: 81 | Training loss: 0.261582 | Validation loss: 0.424422\n",
      "Epoch: 82 | Training loss: 0.260033 | Validation loss: 0.423132\n",
      "Epoch: 83 | Training loss: 0.258507 | Validation loss: 0.421858\n",
      "Epoch: 84 | Training loss: 0.257007 | Validation loss: 0.420613\n",
      "Epoch: 85 | Training loss: 0.255528 | Validation loss: 0.419391\n",
      "Epoch: 86 | Training loss: 0.254072 | Validation loss: 0.418183\n",
      "Epoch: 87 | Training loss: 0.252638 | Validation loss: 0.417001\n",
      "Epoch: 88 | Training loss: 0.251224 | Validation loss: 0.415832\n",
      "Epoch: 89 | Training loss: 0.249832 | Validation loss: 0.414685\n",
      "Epoch: 90 | Training loss: 0.248461 | Validation loss: 0.413561\n",
      "Epoch: 91 | Training loss: 0.247108 | Validation loss: 0.412458\n",
      "Epoch: 92 | Training loss: 0.245775 | Validation loss: 0.411364\n",
      "Epoch: 93 | Training loss: 0.244460 | Validation loss: 0.410293\n",
      "Epoch: 94 | Training loss: 0.243164 | Validation loss: 0.409241\n",
      "Epoch: 95 | Training loss: 0.241885 | Validation loss: 0.408204\n",
      "Epoch: 96 | Training loss: 0.240624 | Validation loss: 0.407183\n",
      "Epoch: 97 | Training loss: 0.239380 | Validation loss: 0.406177\n",
      "Epoch: 98 | Training loss: 0.238153 | Validation loss: 0.405182\n",
      "lr = 0.000500, alpha= 0.000500; Predict_Class:0.831947\n",
      "Epoch: 0 | Training loss: 1.036498 | Validation loss: 1.031044\n",
      "Epoch: 1 | Training loss: 0.937248 | Validation loss: 0.978159\n",
      "Epoch: 2 | Training loss: 0.865804 | Validation loss: 0.933931\n",
      "Epoch: 3 | Training loss: 0.809795 | Validation loss: 0.895972\n",
      "Epoch: 4 | Training loss: 0.764150 | Validation loss: 0.862927\n",
      "Epoch: 5 | Training loss: 0.726047 | Validation loss: 0.833944\n",
      "Epoch: 6 | Training loss: 0.693583 | Validation loss: 0.808211\n",
      "Epoch: 7 | Training loss: 0.665485 | Validation loss: 0.785238\n",
      "Epoch: 8 | Training loss: 0.640833 | Validation loss: 0.764583\n",
      "Epoch: 9 | Training loss: 0.618958 | Validation loss: 0.745870\n",
      "Epoch: 10 | Training loss: 0.599385 | Validation loss: 0.728836\n",
      "Epoch: 11 | Training loss: 0.581704 | Validation loss: 0.713238\n",
      "Epoch: 12 | Training loss: 0.565631 | Validation loss: 0.698915\n",
      "Epoch: 13 | Training loss: 0.550911 | Validation loss: 0.685677\n",
      "Epoch: 14 | Training loss: 0.537360 | Validation loss: 0.673395\n",
      "Epoch: 15 | Training loss: 0.524832 | Validation loss: 0.661974\n",
      "Epoch: 16 | Training loss: 0.513192 | Validation loss: 0.651311\n",
      "Epoch: 17 | Training loss: 0.502344 | Validation loss: 0.641344\n",
      "Epoch: 18 | Training loss: 0.492190 | Validation loss: 0.632003\n",
      "Epoch: 19 | Training loss: 0.482661 | Validation loss: 0.623207\n",
      "Epoch: 20 | Training loss: 0.473686 | Validation loss: 0.614913\n",
      "Epoch: 21 | Training loss: 0.465215 | Validation loss: 0.607076\n",
      "Epoch: 22 | Training loss: 0.457196 | Validation loss: 0.599640\n",
      "Epoch: 23 | Training loss: 0.449588 | Validation loss: 0.592583\n",
      "Epoch: 24 | Training loss: 0.442361 | Validation loss: 0.585888\n",
      "Epoch: 25 | Training loss: 0.435476 | Validation loss: 0.579511\n",
      "Epoch: 26 | Training loss: 0.428911 | Validation loss: 0.573434\n",
      "Epoch: 27 | Training loss: 0.422638 | Validation loss: 0.567633\n",
      "Epoch: 28 | Training loss: 0.416632 | Validation loss: 0.562091\n",
      "Epoch: 29 | Training loss: 0.410880 | Validation loss: 0.556786\n",
      "Epoch: 30 | Training loss: 0.405360 | Validation loss: 0.551696\n",
      "Epoch: 31 | Training loss: 0.400057 | Validation loss: 0.546828\n",
      "Epoch: 32 | Training loss: 0.394954 | Validation loss: 0.542136\n",
      "Epoch: 33 | Training loss: 0.390043 | Validation loss: 0.537635\n",
      "Epoch: 34 | Training loss: 0.385308 | Validation loss: 0.533303\n",
      "Epoch: 35 | Training loss: 0.380738 | Validation loss: 0.529142\n",
      "Epoch: 36 | Training loss: 0.376326 | Validation loss: 0.525120\n",
      "Epoch: 37 | Training loss: 0.372061 | Validation loss: 0.521237\n",
      "Epoch: 38 | Training loss: 0.367932 | Validation loss: 0.517484\n",
      "Epoch: 39 | Training loss: 0.363936 | Validation loss: 0.513856\n",
      "Epoch: 40 | Training loss: 0.360062 | Validation loss: 0.510358\n",
      "Epoch: 41 | Training loss: 0.356309 | Validation loss: 0.506968\n",
      "Epoch: 42 | Training loss: 0.352665 | Validation loss: 0.503681\n",
      "Epoch: 43 | Training loss: 0.349128 | Validation loss: 0.500499\n",
      "Epoch: 44 | Training loss: 0.345690 | Validation loss: 0.497410\n",
      "Epoch: 45 | Training loss: 0.342349 | Validation loss: 0.494420\n",
      "Epoch: 46 | Training loss: 0.339098 | Validation loss: 0.491517\n",
      "Epoch: 47 | Training loss: 0.335934 | Validation loss: 0.488701\n",
      "Epoch: 48 | Training loss: 0.332854 | Validation loss: 0.485968\n",
      "Epoch: 49 | Training loss: 0.329853 | Validation loss: 0.483293\n",
      "Epoch: 50 | Training loss: 0.326928 | Validation loss: 0.480701\n",
      "Epoch: 51 | Training loss: 0.324074 | Validation loss: 0.478169\n",
      "Epoch: 52 | Training loss: 0.321293 | Validation loss: 0.475715\n",
      "Epoch: 53 | Training loss: 0.318576 | Validation loss: 0.473318\n",
      "Epoch: 54 | Training loss: 0.315923 | Validation loss: 0.470984\n",
      "Epoch: 55 | Training loss: 0.313331 | Validation loss: 0.468714\n",
      "Epoch: 56 | Training loss: 0.310798 | Validation loss: 0.466500\n",
      "Epoch: 57 | Training loss: 0.308322 | Validation loss: 0.464336\n",
      "Epoch: 58 | Training loss: 0.305897 | Validation loss: 0.462209\n",
      "Epoch: 59 | Training loss: 0.303530 | Validation loss: 0.460148\n",
      "Epoch: 60 | Training loss: 0.301211 | Validation loss: 0.458135\n",
      "Epoch: 61 | Training loss: 0.298942 | Validation loss: 0.456168\n",
      "Epoch: 62 | Training loss: 0.296718 | Validation loss: 0.454242\n",
      "Epoch: 63 | Training loss: 0.294539 | Validation loss: 0.452358\n",
      "Epoch: 64 | Training loss: 0.292405 | Validation loss: 0.450519\n",
      "Epoch: 65 | Training loss: 0.290310 | Validation loss: 0.448707\n",
      "Epoch: 66 | Training loss: 0.288261 | Validation loss: 0.446949\n",
      "Epoch: 67 | Training loss: 0.286248 | Validation loss: 0.445221\n",
      "Epoch: 68 | Training loss: 0.284274 | Validation loss: 0.443537\n",
      "Epoch: 69 | Training loss: 0.282337 | Validation loss: 0.441883\n",
      "Epoch: 70 | Training loss: 0.280433 | Validation loss: 0.440252\n",
      "Epoch: 71 | Training loss: 0.278569 | Validation loss: 0.438670\n",
      "Epoch: 72 | Training loss: 0.276734 | Validation loss: 0.437121\n",
      "Epoch: 73 | Training loss: 0.274933 | Validation loss: 0.435594\n",
      "Epoch: 74 | Training loss: 0.273163 | Validation loss: 0.434101\n",
      "Epoch: 75 | Training loss: 0.271424 | Validation loss: 0.432635\n",
      "Epoch: 76 | Training loss: 0.269714 | Validation loss: 0.431194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Training loss: 0.268033 | Validation loss: 0.429788\n",
      "Epoch: 78 | Training loss: 0.266380 | Validation loss: 0.428401\n",
      "Epoch: 79 | Training loss: 0.264754 | Validation loss: 0.427046\n",
      "Epoch: 80 | Training loss: 0.263154 | Validation loss: 0.425708\n",
      "Epoch: 81 | Training loss: 0.261581 | Validation loss: 0.424396\n",
      "Epoch: 82 | Training loss: 0.260031 | Validation loss: 0.423106\n",
      "Epoch: 83 | Training loss: 0.258506 | Validation loss: 0.421835\n",
      "Epoch: 84 | Training loss: 0.257005 | Validation loss: 0.420589\n",
      "Epoch: 85 | Training loss: 0.255526 | Validation loss: 0.419363\n",
      "Epoch: 86 | Training loss: 0.254071 | Validation loss: 0.418162\n",
      "Epoch: 87 | Training loss: 0.252637 | Validation loss: 0.416987\n",
      "Epoch: 88 | Training loss: 0.251224 | Validation loss: 0.415823\n",
      "Epoch: 89 | Training loss: 0.249832 | Validation loss: 0.414680\n",
      "Epoch: 90 | Training loss: 0.248459 | Validation loss: 0.413554\n",
      "Epoch: 91 | Training loss: 0.247107 | Validation loss: 0.412450\n",
      "Epoch: 92 | Training loss: 0.245774 | Validation loss: 0.411363\n",
      "Epoch: 93 | Training loss: 0.244459 | Validation loss: 0.410290\n",
      "Epoch: 94 | Training loss: 0.243163 | Validation loss: 0.409236\n",
      "Epoch: 95 | Training loss: 0.241884 | Validation loss: 0.408195\n",
      "Epoch: 96 | Training loss: 0.240623 | Validation loss: 0.407169\n",
      "Epoch: 97 | Training loss: 0.239379 | Validation loss: 0.406163\n",
      "Epoch: 98 | Training loss: 0.238151 | Validation loss: 0.405168\n",
      "lr = 0.000500, alpha= 0.000100; Predict_Class:0.831947\n",
      "Epoch: 0 | Training loss: 1.084223 | Validation loss: 1.083286\n",
      "Epoch: 1 | Training loss: 1.056015 | Validation loss: 1.069044\n",
      "Epoch: 2 | Training loss: 1.030895 | Validation loss: 1.055693\n",
      "Epoch: 3 | Training loss: 1.008225 | Validation loss: 1.043099\n",
      "Epoch: 4 | Training loss: 0.987513 | Validation loss: 1.031132\n",
      "Epoch: 5 | Training loss: 0.968375 | Validation loss: 1.019709\n",
      "Epoch: 6 | Training loss: 0.950555 | Validation loss: 1.008769\n",
      "Epoch: 7 | Training loss: 0.933864 | Validation loss: 0.998255\n",
      "Epoch: 8 | Training loss: 0.918150 | Validation loss: 0.988134\n",
      "Epoch: 9 | Training loss: 0.903300 | Validation loss: 0.978371\n",
      "Epoch: 10 | Training loss: 0.889233 | Validation loss: 0.968942\n",
      "Epoch: 11 | Training loss: 0.875874 | Validation loss: 0.959829\n",
      "Epoch: 12 | Training loss: 0.863159 | Validation loss: 0.951005\n",
      "Epoch: 13 | Training loss: 0.851034 | Validation loss: 0.942460\n",
      "Epoch: 14 | Training loss: 0.839452 | Validation loss: 0.934177\n",
      "Epoch: 15 | Training loss: 0.828372 | Validation loss: 0.926142\n",
      "Epoch: 16 | Training loss: 0.817758 | Validation loss: 0.918344\n",
      "Epoch: 17 | Training loss: 0.807578 | Validation loss: 0.910768\n",
      "Epoch: 18 | Training loss: 0.797804 | Validation loss: 0.903410\n",
      "Epoch: 19 | Training loss: 0.788408 | Validation loss: 0.896256\n",
      "Epoch: 20 | Training loss: 0.779368 | Validation loss: 0.889300\n",
      "Epoch: 21 | Training loss: 0.770660 | Validation loss: 0.882531\n",
      "Epoch: 22 | Training loss: 0.762265 | Validation loss: 0.875942\n",
      "Epoch: 23 | Training loss: 0.754161 | Validation loss: 0.869525\n",
      "Epoch: 24 | Training loss: 0.746337 | Validation loss: 0.863275\n",
      "Epoch: 25 | Training loss: 0.738775 | Validation loss: 0.857184\n",
      "Epoch: 26 | Training loss: 0.731461 | Validation loss: 0.851246\n",
      "Epoch: 27 | Training loss: 0.724381 | Validation loss: 0.845454\n",
      "Epoch: 28 | Training loss: 0.717522 | Validation loss: 0.839803\n",
      "Epoch: 29 | Training loss: 0.710873 | Validation loss: 0.834289\n",
      "Epoch: 30 | Training loss: 0.704425 | Validation loss: 0.828904\n",
      "Epoch: 31 | Training loss: 0.698165 | Validation loss: 0.823647\n",
      "Epoch: 32 | Training loss: 0.692086 | Validation loss: 0.818512\n",
      "Epoch: 33 | Training loss: 0.686179 | Validation loss: 0.813492\n",
      "Epoch: 34 | Training loss: 0.680435 | Validation loss: 0.808586\n",
      "Epoch: 35 | Training loss: 0.674848 | Validation loss: 0.803788\n",
      "Epoch: 36 | Training loss: 0.669410 | Validation loss: 0.799097\n",
      "Epoch: 37 | Training loss: 0.664114 | Validation loss: 0.794507\n",
      "Epoch: 38 | Training loss: 0.658955 | Validation loss: 0.790015\n",
      "Epoch: 39 | Training loss: 0.653926 | Validation loss: 0.785618\n",
      "Epoch: 40 | Training loss: 0.649023 | Validation loss: 0.781314\n",
      "Epoch: 41 | Training loss: 0.644238 | Validation loss: 0.777097\n",
      "Epoch: 42 | Training loss: 0.639568 | Validation loss: 0.772966\n",
      "Epoch: 43 | Training loss: 0.635009 | Validation loss: 0.768919\n",
      "Epoch: 44 | Training loss: 0.630555 | Validation loss: 0.764952\n",
      "Epoch: 45 | Training loss: 0.626203 | Validation loss: 0.761062\n",
      "Epoch: 46 | Training loss: 0.621947 | Validation loss: 0.757248\n",
      "Epoch: 47 | Training loss: 0.617786 | Validation loss: 0.753507\n",
      "Epoch: 48 | Training loss: 0.613715 | Validation loss: 0.749837\n",
      "Epoch: 49 | Training loss: 0.609732 | Validation loss: 0.746235\n",
      "Epoch: 50 | Training loss: 0.605832 | Validation loss: 0.742701\n",
      "Epoch: 51 | Training loss: 0.602013 | Validation loss: 0.739232\n",
      "Epoch: 52 | Training loss: 0.598272 | Validation loss: 0.735824\n",
      "Epoch: 53 | Training loss: 0.594606 | Validation loss: 0.732478\n",
      "Epoch: 54 | Training loss: 0.591014 | Validation loss: 0.729192\n",
      "Epoch: 55 | Training loss: 0.587492 | Validation loss: 0.725963\n",
      "Epoch: 56 | Training loss: 0.584037 | Validation loss: 0.722790\n",
      "Epoch: 57 | Training loss: 0.580648 | Validation loss: 0.719671\n",
      "Epoch: 58 | Training loss: 0.577322 | Validation loss: 0.716605\n",
      "Epoch: 59 | Training loss: 0.574057 | Validation loss: 0.713590\n",
      "Epoch: 60 | Training loss: 0.570853 | Validation loss: 0.710626\n",
      "Epoch: 61 | Training loss: 0.567705 | Validation loss: 0.707710\n",
      "Epoch: 62 | Training loss: 0.564615 | Validation loss: 0.704843\n",
      "Epoch: 63 | Training loss: 0.561578 | Validation loss: 0.702021\n",
      "Epoch: 64 | Training loss: 0.558593 | Validation loss: 0.699244\n",
      "Epoch: 65 | Training loss: 0.555660 | Validation loss: 0.696511\n",
      "Epoch: 66 | Training loss: 0.552776 | Validation loss: 0.693821\n",
      "Epoch: 67 | Training loss: 0.549940 | Validation loss: 0.691173\n",
      "Epoch: 68 | Training loss: 0.547151 | Validation loss: 0.688565\n",
      "Epoch: 69 | Training loss: 0.544407 | Validation loss: 0.685996\n",
      "Epoch: 70 | Training loss: 0.541707 | Validation loss: 0.683467\n",
      "Epoch: 71 | Training loss: 0.539050 | Validation loss: 0.680975\n",
      "Epoch: 72 | Training loss: 0.536434 | Validation loss: 0.678520\n",
      "Epoch: 73 | Training loss: 0.533859 | Validation loss: 0.676100\n",
      "Epoch: 74 | Training loss: 0.531323 | Validation loss: 0.673716\n",
      "Epoch: 75 | Training loss: 0.528825 | Validation loss: 0.671366\n",
      "Epoch: 76 | Training loss: 0.526365 | Validation loss: 0.669050\n",
      "Epoch: 77 | Training loss: 0.523942 | Validation loss: 0.666766\n",
      "Epoch: 78 | Training loss: 0.521553 | Validation loss: 0.664513\n",
      "Epoch: 79 | Training loss: 0.519200 | Validation loss: 0.662292\n",
      "Epoch: 80 | Training loss: 0.516880 | Validation loss: 0.660101\n",
      "Epoch: 81 | Training loss: 0.514593 | Validation loss: 0.657941\n",
      "Epoch: 82 | Training loss: 0.512338 | Validation loss: 0.655809\n",
      "Epoch: 83 | Training loss: 0.510114 | Validation loss: 0.653707\n",
      "Epoch: 84 | Training loss: 0.507921 | Validation loss: 0.651632\n",
      "Epoch: 85 | Training loss: 0.505757 | Validation loss: 0.649584\n",
      "Epoch: 86 | Training loss: 0.503623 | Validation loss: 0.647563\n",
      "Epoch: 87 | Training loss: 0.501516 | Validation loss: 0.645568\n",
      "Epoch: 88 | Training loss: 0.499438 | Validation loss: 0.643599\n",
      "Epoch: 89 | Training loss: 0.497386 | Validation loss: 0.641654\n",
      "Epoch: 90 | Training loss: 0.495361 | Validation loss: 0.639735\n",
      "Epoch: 91 | Training loss: 0.493362 | Validation loss: 0.637839\n",
      "Epoch: 92 | Training loss: 0.491389 | Validation loss: 0.635967\n",
      "Epoch: 93 | Training loss: 0.489440 | Validation loss: 0.634117\n",
      "Epoch: 94 | Training loss: 0.487515 | Validation loss: 0.632291\n",
      "Epoch: 95 | Training loss: 0.485614 | Validation loss: 0.630487\n",
      "Epoch: 96 | Training loss: 0.483736 | Validation loss: 0.628704\n",
      "Epoch: 97 | Training loss: 0.481881 | Validation loss: 0.626942\n",
      "Epoch: 98 | Training loss: 0.480047 | Validation loss: 0.625202\n",
      "Epoch: 99 | Training loss: 0.478236 | Validation loss: 0.623481\n",
      "Epoch: 100 | Training loss: 0.476446 | Validation loss: 0.621782\n",
      "Epoch: 101 | Training loss: 0.474677 | Validation loss: 0.620102\n",
      "Epoch: 102 | Training loss: 0.472928 | Validation loss: 0.618440\n",
      "Epoch: 103 | Training loss: 0.471199 | Validation loss: 0.616799\n",
      "Epoch: 104 | Training loss: 0.469490 | Validation loss: 0.615175\n",
      "Epoch: 105 | Training loss: 0.467799 | Validation loss: 0.613570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106 | Training loss: 0.466128 | Validation loss: 0.611982\n",
      "Epoch: 107 | Training loss: 0.464476 | Validation loss: 0.610413\n",
      "Epoch: 108 | Training loss: 0.462841 | Validation loss: 0.608860\n",
      "Epoch: 109 | Training loss: 0.461224 | Validation loss: 0.607325\n",
      "Epoch: 110 | Training loss: 0.459624 | Validation loss: 0.605806\n",
      "Epoch: 111 | Training loss: 0.458042 | Validation loss: 0.604303\n",
      "Epoch: 112 | Training loss: 0.456476 | Validation loss: 0.602817\n",
      "Epoch: 113 | Training loss: 0.454927 | Validation loss: 0.601347\n",
      "Epoch: 114 | Training loss: 0.453394 | Validation loss: 0.599892\n",
      "Epoch: 115 | Training loss: 0.451877 | Validation loss: 0.598452\n",
      "Epoch: 116 | Training loss: 0.450375 | Validation loss: 0.597028\n",
      "Epoch: 117 | Training loss: 0.448889 | Validation loss: 0.595618\n",
      "Epoch: 118 | Training loss: 0.447418 | Validation loss: 0.594222\n",
      "Epoch: 119 | Training loss: 0.445961 | Validation loss: 0.592840\n",
      "Epoch: 120 | Training loss: 0.444519 | Validation loss: 0.591473\n",
      "Epoch: 121 | Training loss: 0.443091 | Validation loss: 0.590120\n",
      "Epoch: 122 | Training loss: 0.441678 | Validation loss: 0.588780\n",
      "Epoch: 123 | Training loss: 0.440278 | Validation loss: 0.587454\n",
      "Epoch: 124 | Training loss: 0.438891 | Validation loss: 0.586140\n",
      "Epoch: 125 | Training loss: 0.437518 | Validation loss: 0.584840\n",
      "Epoch: 126 | Training loss: 0.436158 | Validation loss: 0.583553\n",
      "Epoch: 127 | Training loss: 0.434811 | Validation loss: 0.582277\n",
      "Epoch: 128 | Training loss: 0.433476 | Validation loss: 0.581014\n",
      "Epoch: 129 | Training loss: 0.432154 | Validation loss: 0.579764\n",
      "Epoch: 130 | Training loss: 0.430844 | Validation loss: 0.578525\n",
      "Epoch: 131 | Training loss: 0.429546 | Validation loss: 0.577298\n",
      "Epoch: 132 | Training loss: 0.428260 | Validation loss: 0.576082\n",
      "Epoch: 133 | Training loss: 0.426986 | Validation loss: 0.574877\n",
      "Epoch: 134 | Training loss: 0.425723 | Validation loss: 0.573684\n",
      "Epoch: 135 | Training loss: 0.424471 | Validation loss: 0.572502\n",
      "Epoch: 136 | Training loss: 0.423230 | Validation loss: 0.571331\n",
      "Epoch: 137 | Training loss: 0.422001 | Validation loss: 0.570171\n",
      "Epoch: 138 | Training loss: 0.420782 | Validation loss: 0.569021\n",
      "Epoch: 139 | Training loss: 0.419574 | Validation loss: 0.567881\n",
      "Epoch: 140 | Training loss: 0.418376 | Validation loss: 0.566752\n",
      "Epoch: 141 | Training loss: 0.417188 | Validation loss: 0.565632\n",
      "Epoch: 142 | Training loss: 0.416011 | Validation loss: 0.564523\n",
      "Epoch: 143 | Training loss: 0.414843 | Validation loss: 0.563423\n",
      "Epoch: 144 | Training loss: 0.413686 | Validation loss: 0.562333\n",
      "Epoch: 145 | Training loss: 0.412538 | Validation loss: 0.561254\n",
      "Epoch: 146 | Training loss: 0.411400 | Validation loss: 0.560182\n",
      "Epoch: 147 | Training loss: 0.410271 | Validation loss: 0.559120\n",
      "Epoch: 148 | Training loss: 0.409151 | Validation loss: 0.558067\n",
      "Epoch: 149 | Training loss: 0.408040 | Validation loss: 0.557023\n",
      "Epoch: 150 | Training loss: 0.406939 | Validation loss: 0.555988\n",
      "Epoch: 151 | Training loss: 0.405846 | Validation loss: 0.554962\n",
      "Epoch: 152 | Training loss: 0.404762 | Validation loss: 0.553944\n",
      "Epoch: 153 | Training loss: 0.403687 | Validation loss: 0.552935\n",
      "Epoch: 154 | Training loss: 0.402620 | Validation loss: 0.551934\n",
      "Epoch: 155 | Training loss: 0.401562 | Validation loss: 0.550942\n",
      "lr = 0.000100, alpha= 0.001000; Predict_Class:0.823333\n",
      "Epoch: 0 | Training loss: 1.084229 | Validation loss: 1.083282\n",
      "Epoch: 1 | Training loss: 1.056039 | Validation loss: 1.069048\n",
      "Epoch: 2 | Training loss: 1.030950 | Validation loss: 1.055704\n",
      "Epoch: 3 | Training loss: 1.008289 | Validation loss: 1.043107\n",
      "Epoch: 4 | Training loss: 0.987562 | Validation loss: 1.031138\n",
      "Epoch: 5 | Training loss: 0.968411 | Validation loss: 1.019712\n",
      "Epoch: 6 | Training loss: 0.950576 | Validation loss: 1.008768\n",
      "Epoch: 7 | Training loss: 0.933872 | Validation loss: 0.998257\n",
      "Epoch: 8 | Training loss: 0.918157 | Validation loss: 0.988137\n",
      "Epoch: 9 | Training loss: 0.903311 | Validation loss: 0.978375\n",
      "Epoch: 10 | Training loss: 0.889243 | Validation loss: 0.968946\n",
      "Epoch: 11 | Training loss: 0.875881 | Validation loss: 0.959830\n",
      "Epoch: 12 | Training loss: 0.863165 | Validation loss: 0.951006\n",
      "Epoch: 13 | Training loss: 0.851039 | Validation loss: 0.942461\n",
      "Epoch: 14 | Training loss: 0.839455 | Validation loss: 0.934177\n",
      "Epoch: 15 | Training loss: 0.828372 | Validation loss: 0.926142\n",
      "Epoch: 16 | Training loss: 0.817757 | Validation loss: 0.918344\n",
      "Epoch: 17 | Training loss: 0.807578 | Validation loss: 0.910770\n",
      "Epoch: 18 | Training loss: 0.797802 | Validation loss: 0.903412\n",
      "Epoch: 19 | Training loss: 0.788404 | Validation loss: 0.896258\n",
      "Epoch: 20 | Training loss: 0.779363 | Validation loss: 0.889301\n",
      "Epoch: 21 | Training loss: 0.770653 | Validation loss: 0.882531\n",
      "Epoch: 22 | Training loss: 0.762256 | Validation loss: 0.875942\n",
      "Epoch: 23 | Training loss: 0.754155 | Validation loss: 0.869526\n",
      "Epoch: 24 | Training loss: 0.746330 | Validation loss: 0.863274\n",
      "Epoch: 25 | Training loss: 0.738767 | Validation loss: 0.857183\n",
      "Epoch: 26 | Training loss: 0.731452 | Validation loss: 0.851245\n",
      "Epoch: 27 | Training loss: 0.724371 | Validation loss: 0.845452\n",
      "Epoch: 28 | Training loss: 0.717510 | Validation loss: 0.839800\n",
      "Epoch: 29 | Training loss: 0.710861 | Validation loss: 0.834286\n",
      "Epoch: 30 | Training loss: 0.704413 | Validation loss: 0.828903\n",
      "Epoch: 31 | Training loss: 0.698154 | Validation loss: 0.823643\n",
      "Epoch: 32 | Training loss: 0.692075 | Validation loss: 0.818507\n",
      "Epoch: 33 | Training loss: 0.686168 | Validation loss: 0.813488\n",
      "Epoch: 34 | Training loss: 0.680425 | Validation loss: 0.808583\n",
      "Epoch: 35 | Training loss: 0.674838 | Validation loss: 0.803786\n",
      "Epoch: 36 | Training loss: 0.669401 | Validation loss: 0.799093\n",
      "Epoch: 37 | Training loss: 0.664105 | Validation loss: 0.794503\n",
      "Epoch: 38 | Training loss: 0.658946 | Validation loss: 0.790010\n",
      "Epoch: 39 | Training loss: 0.653917 | Validation loss: 0.785613\n",
      "Epoch: 40 | Training loss: 0.649013 | Validation loss: 0.781307\n",
      "Epoch: 41 | Training loss: 0.644228 | Validation loss: 0.777091\n",
      "Epoch: 42 | Training loss: 0.639559 | Validation loss: 0.772960\n",
      "Epoch: 43 | Training loss: 0.634999 | Validation loss: 0.768912\n",
      "Epoch: 44 | Training loss: 0.630545 | Validation loss: 0.764945\n",
      "Epoch: 45 | Training loss: 0.626193 | Validation loss: 0.761056\n",
      "Epoch: 46 | Training loss: 0.621939 | Validation loss: 0.757242\n",
      "Epoch: 47 | Training loss: 0.617778 | Validation loss: 0.753501\n",
      "Epoch: 48 | Training loss: 0.613707 | Validation loss: 0.749831\n",
      "Epoch: 49 | Training loss: 0.609724 | Validation loss: 0.746230\n",
      "Epoch: 50 | Training loss: 0.605824 | Validation loss: 0.742696\n",
      "Epoch: 51 | Training loss: 0.602006 | Validation loss: 0.739226\n",
      "Epoch: 52 | Training loss: 0.598265 | Validation loss: 0.735820\n",
      "Epoch: 53 | Training loss: 0.594600 | Validation loss: 0.732474\n",
      "Epoch: 54 | Training loss: 0.591007 | Validation loss: 0.729187\n",
      "Epoch: 55 | Training loss: 0.587485 | Validation loss: 0.725958\n",
      "Epoch: 56 | Training loss: 0.584031 | Validation loss: 0.722785\n",
      "Epoch: 57 | Training loss: 0.580642 | Validation loss: 0.719667\n",
      "Epoch: 58 | Training loss: 0.577316 | Validation loss: 0.716601\n",
      "Epoch: 59 | Training loss: 0.574052 | Validation loss: 0.713587\n",
      "Epoch: 60 | Training loss: 0.570848 | Validation loss: 0.710623\n",
      "Epoch: 61 | Training loss: 0.567701 | Validation loss: 0.707708\n",
      "Epoch: 62 | Training loss: 0.564610 | Validation loss: 0.704840\n",
      "Epoch: 63 | Training loss: 0.561574 | Validation loss: 0.702018\n",
      "Epoch: 64 | Training loss: 0.558590 | Validation loss: 0.699242\n",
      "Epoch: 65 | Training loss: 0.555656 | Validation loss: 0.696509\n",
      "Epoch: 66 | Training loss: 0.552772 | Validation loss: 0.693819\n",
      "Epoch: 67 | Training loss: 0.549937 | Validation loss: 0.691171\n",
      "Epoch: 68 | Training loss: 0.547148 | Validation loss: 0.688563\n",
      "Epoch: 69 | Training loss: 0.544403 | Validation loss: 0.685995\n",
      "Epoch: 70 | Training loss: 0.541703 | Validation loss: 0.683465\n",
      "Epoch: 71 | Training loss: 0.539046 | Validation loss: 0.680972\n",
      "Epoch: 72 | Training loss: 0.536430 | Validation loss: 0.678517\n",
      "Epoch: 73 | Training loss: 0.533855 | Validation loss: 0.676097\n",
      "Epoch: 74 | Training loss: 0.531320 | Validation loss: 0.673713\n",
      "Epoch: 75 | Training loss: 0.528822 | Validation loss: 0.671363\n",
      "Epoch: 76 | Training loss: 0.526362 | Validation loss: 0.669047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Training loss: 0.523939 | Validation loss: 0.666763\n",
      "Epoch: 78 | Training loss: 0.521551 | Validation loss: 0.664511\n",
      "Epoch: 79 | Training loss: 0.519197 | Validation loss: 0.662289\n",
      "Epoch: 80 | Training loss: 0.516877 | Validation loss: 0.660099\n",
      "Epoch: 81 | Training loss: 0.514590 | Validation loss: 0.657939\n",
      "Epoch: 82 | Training loss: 0.512335 | Validation loss: 0.655807\n",
      "Epoch: 83 | Training loss: 0.510111 | Validation loss: 0.653704\n",
      "Epoch: 84 | Training loss: 0.507918 | Validation loss: 0.651629\n",
      "Epoch: 85 | Training loss: 0.505755 | Validation loss: 0.649581\n",
      "Epoch: 86 | Training loss: 0.503620 | Validation loss: 0.647561\n",
      "Epoch: 87 | Training loss: 0.501514 | Validation loss: 0.645566\n",
      "Epoch: 88 | Training loss: 0.499435 | Validation loss: 0.643596\n",
      "Epoch: 89 | Training loss: 0.497384 | Validation loss: 0.641652\n",
      "Epoch: 90 | Training loss: 0.495359 | Validation loss: 0.639733\n",
      "Epoch: 91 | Training loss: 0.493361 | Validation loss: 0.637837\n",
      "Epoch: 92 | Training loss: 0.491387 | Validation loss: 0.635966\n",
      "Epoch: 93 | Training loss: 0.489438 | Validation loss: 0.634117\n",
      "Epoch: 94 | Training loss: 0.487513 | Validation loss: 0.632290\n",
      "Epoch: 95 | Training loss: 0.485612 | Validation loss: 0.630486\n",
      "Epoch: 96 | Training loss: 0.483734 | Validation loss: 0.628704\n",
      "Epoch: 97 | Training loss: 0.481879 | Validation loss: 0.626942\n",
      "Epoch: 98 | Training loss: 0.480046 | Validation loss: 0.625201\n",
      "Epoch: 99 | Training loss: 0.478235 | Validation loss: 0.623481\n",
      "Epoch: 100 | Training loss: 0.476444 | Validation loss: 0.621781\n",
      "Epoch: 101 | Training loss: 0.474675 | Validation loss: 0.620100\n",
      "Epoch: 102 | Training loss: 0.472926 | Validation loss: 0.618440\n",
      "Epoch: 103 | Training loss: 0.471197 | Validation loss: 0.616798\n",
      "Epoch: 104 | Training loss: 0.469488 | Validation loss: 0.615175\n",
      "Epoch: 105 | Training loss: 0.467798 | Validation loss: 0.613570\n",
      "Epoch: 106 | Training loss: 0.466127 | Validation loss: 0.611982\n",
      "Epoch: 107 | Training loss: 0.464474 | Validation loss: 0.610412\n",
      "Epoch: 108 | Training loss: 0.462840 | Validation loss: 0.608860\n",
      "Epoch: 109 | Training loss: 0.461223 | Validation loss: 0.607324\n",
      "Epoch: 110 | Training loss: 0.459623 | Validation loss: 0.605806\n",
      "Epoch: 111 | Training loss: 0.458041 | Validation loss: 0.604303\n",
      "Epoch: 112 | Training loss: 0.456475 | Validation loss: 0.602817\n",
      "Epoch: 113 | Training loss: 0.454926 | Validation loss: 0.601347\n",
      "Epoch: 114 | Training loss: 0.453393 | Validation loss: 0.599892\n",
      "Epoch: 115 | Training loss: 0.451876 | Validation loss: 0.598452\n",
      "Epoch: 116 | Training loss: 0.450374 | Validation loss: 0.597027\n",
      "Epoch: 117 | Training loss: 0.448888 | Validation loss: 0.595617\n",
      "Epoch: 118 | Training loss: 0.447417 | Validation loss: 0.594222\n",
      "Epoch: 119 | Training loss: 0.445960 | Validation loss: 0.592841\n",
      "Epoch: 120 | Training loss: 0.444518 | Validation loss: 0.591473\n",
      "Epoch: 121 | Training loss: 0.443091 | Validation loss: 0.590120\n",
      "Epoch: 122 | Training loss: 0.441677 | Validation loss: 0.588780\n",
      "Epoch: 123 | Training loss: 0.440277 | Validation loss: 0.587454\n",
      "Epoch: 124 | Training loss: 0.438890 | Validation loss: 0.586140\n",
      "Epoch: 125 | Training loss: 0.437517 | Validation loss: 0.584840\n",
      "Epoch: 126 | Training loss: 0.436157 | Validation loss: 0.583553\n",
      "Epoch: 127 | Training loss: 0.434810 | Validation loss: 0.582278\n",
      "Epoch: 128 | Training loss: 0.433475 | Validation loss: 0.581015\n",
      "Epoch: 129 | Training loss: 0.432153 | Validation loss: 0.579764\n",
      "Epoch: 130 | Training loss: 0.430843 | Validation loss: 0.578525\n",
      "Epoch: 131 | Training loss: 0.429545 | Validation loss: 0.577298\n",
      "Epoch: 132 | Training loss: 0.428259 | Validation loss: 0.576082\n",
      "Epoch: 133 | Training loss: 0.426985 | Validation loss: 0.574878\n",
      "Epoch: 134 | Training loss: 0.425722 | Validation loss: 0.573685\n",
      "Epoch: 135 | Training loss: 0.424470 | Validation loss: 0.572503\n",
      "Epoch: 136 | Training loss: 0.423230 | Validation loss: 0.571332\n",
      "Epoch: 137 | Training loss: 0.422000 | Validation loss: 0.570171\n",
      "Epoch: 138 | Training loss: 0.420781 | Validation loss: 0.569022\n",
      "Epoch: 139 | Training loss: 0.419573 | Validation loss: 0.567882\n",
      "Epoch: 140 | Training loss: 0.418375 | Validation loss: 0.566753\n",
      "Epoch: 141 | Training loss: 0.417188 | Validation loss: 0.565634\n",
      "Epoch: 142 | Training loss: 0.416010 | Validation loss: 0.564524\n",
      "Epoch: 143 | Training loss: 0.414843 | Validation loss: 0.563425\n",
      "Epoch: 144 | Training loss: 0.413686 | Validation loss: 0.562335\n",
      "Epoch: 145 | Training loss: 0.412538 | Validation loss: 0.561254\n",
      "Epoch: 146 | Training loss: 0.411399 | Validation loss: 0.560183\n",
      "Epoch: 147 | Training loss: 0.410270 | Validation loss: 0.559121\n",
      "Epoch: 148 | Training loss: 0.409151 | Validation loss: 0.558068\n",
      "Epoch: 149 | Training loss: 0.408040 | Validation loss: 0.557025\n",
      "Epoch: 150 | Training loss: 0.406939 | Validation loss: 0.555990\n",
      "Epoch: 151 | Training loss: 0.405846 | Validation loss: 0.554964\n",
      "Epoch: 152 | Training loss: 0.404762 | Validation loss: 0.553946\n",
      "Epoch: 153 | Training loss: 0.403687 | Validation loss: 0.552937\n",
      "Epoch: 154 | Training loss: 0.402620 | Validation loss: 0.551936\n",
      "Epoch: 155 | Training loss: 0.401562 | Validation loss: 0.550944\n",
      "lr = 0.000100, alpha= 0.000500; Predict_Class:0.823333\n",
      "Epoch: 0 | Training loss: 1.084245 | Validation loss: 1.083293\n",
      "Epoch: 1 | Training loss: 1.056046 | Validation loss: 1.069047\n",
      "Epoch: 2 | Training loss: 1.030937 | Validation loss: 1.055701\n",
      "Epoch: 3 | Training loss: 1.008274 | Validation loss: 1.043103\n",
      "Epoch: 4 | Training loss: 0.987548 | Validation loss: 1.031136\n",
      "Epoch: 5 | Training loss: 0.968402 | Validation loss: 1.019712\n",
      "Epoch: 6 | Training loss: 0.950574 | Validation loss: 1.008768\n",
      "Epoch: 7 | Training loss: 0.933872 | Validation loss: 0.998254\n",
      "Epoch: 8 | Training loss: 0.918150 | Validation loss: 0.988131\n",
      "Epoch: 9 | Training loss: 0.903303 | Validation loss: 0.978367\n",
      "Epoch: 10 | Training loss: 0.889236 | Validation loss: 0.968939\n",
      "Epoch: 11 | Training loss: 0.875876 | Validation loss: 0.959825\n",
      "Epoch: 12 | Training loss: 0.863162 | Validation loss: 0.951001\n",
      "Epoch: 13 | Training loss: 0.851036 | Validation loss: 0.942456\n",
      "Epoch: 14 | Training loss: 0.839454 | Validation loss: 0.934172\n",
      "Epoch: 15 | Training loss: 0.828374 | Validation loss: 0.926136\n",
      "Epoch: 16 | Training loss: 0.817759 | Validation loss: 0.918340\n",
      "Epoch: 17 | Training loss: 0.807580 | Validation loss: 0.910768\n",
      "Epoch: 18 | Training loss: 0.797805 | Validation loss: 0.903408\n",
      "Epoch: 19 | Training loss: 0.788408 | Validation loss: 0.896255\n",
      "Epoch: 20 | Training loss: 0.779367 | Validation loss: 0.889298\n",
      "Epoch: 21 | Training loss: 0.770659 | Validation loss: 0.882529\n",
      "Epoch: 22 | Training loss: 0.762264 | Validation loss: 0.875939\n",
      "Epoch: 23 | Training loss: 0.754162 | Validation loss: 0.869521\n",
      "Epoch: 24 | Training loss: 0.746337 | Validation loss: 0.863271\n",
      "Epoch: 25 | Training loss: 0.738775 | Validation loss: 0.857181\n",
      "Epoch: 26 | Training loss: 0.731460 | Validation loss: 0.851241\n",
      "Epoch: 27 | Training loss: 0.724379 | Validation loss: 0.845449\n",
      "Epoch: 28 | Training loss: 0.717519 | Validation loss: 0.839798\n",
      "Epoch: 29 | Training loss: 0.710871 | Validation loss: 0.834283\n",
      "Epoch: 30 | Training loss: 0.704422 | Validation loss: 0.828900\n",
      "Epoch: 31 | Training loss: 0.698161 | Validation loss: 0.823642\n",
      "Epoch: 32 | Training loss: 0.692082 | Validation loss: 0.818506\n",
      "Epoch: 33 | Training loss: 0.686176 | Validation loss: 0.813487\n",
      "Epoch: 34 | Training loss: 0.680432 | Validation loss: 0.808580\n",
      "Epoch: 35 | Training loss: 0.674845 | Validation loss: 0.803785\n",
      "Epoch: 36 | Training loss: 0.669407 | Validation loss: 0.799093\n",
      "Epoch: 37 | Training loss: 0.664112 | Validation loss: 0.794504\n",
      "Epoch: 38 | Training loss: 0.658953 | Validation loss: 0.790011\n",
      "Epoch: 39 | Training loss: 0.653924 | Validation loss: 0.785614\n",
      "Epoch: 40 | Training loss: 0.649020 | Validation loss: 0.781308\n",
      "Epoch: 41 | Training loss: 0.644235 | Validation loss: 0.777091\n",
      "Epoch: 42 | Training loss: 0.639565 | Validation loss: 0.772960\n",
      "Epoch: 43 | Training loss: 0.635005 | Validation loss: 0.768913\n",
      "Epoch: 44 | Training loss: 0.630551 | Validation loss: 0.764945\n",
      "Epoch: 45 | Training loss: 0.626198 | Validation loss: 0.761055\n",
      "Epoch: 46 | Training loss: 0.621943 | Validation loss: 0.757240\n",
      "Epoch: 47 | Training loss: 0.617781 | Validation loss: 0.753500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Training loss: 0.613711 | Validation loss: 0.749830\n",
      "Epoch: 49 | Training loss: 0.609727 | Validation loss: 0.746229\n",
      "Epoch: 50 | Training loss: 0.605828 | Validation loss: 0.742695\n",
      "Epoch: 51 | Training loss: 0.602009 | Validation loss: 0.739225\n",
      "Epoch: 52 | Training loss: 0.598269 | Validation loss: 0.735819\n",
      "Epoch: 53 | Training loss: 0.594603 | Validation loss: 0.732473\n",
      "Epoch: 54 | Training loss: 0.591010 | Validation loss: 0.729187\n",
      "Epoch: 55 | Training loss: 0.587488 | Validation loss: 0.725959\n",
      "Epoch: 56 | Training loss: 0.584033 | Validation loss: 0.722786\n",
      "Epoch: 57 | Training loss: 0.580644 | Validation loss: 0.719667\n",
      "Epoch: 58 | Training loss: 0.577319 | Validation loss: 0.716602\n",
      "Epoch: 59 | Training loss: 0.574055 | Validation loss: 0.713587\n",
      "Epoch: 60 | Training loss: 0.570850 | Validation loss: 0.710623\n",
      "Epoch: 61 | Training loss: 0.567703 | Validation loss: 0.707707\n",
      "Epoch: 62 | Training loss: 0.564612 | Validation loss: 0.704839\n",
      "Epoch: 63 | Training loss: 0.561575 | Validation loss: 0.702018\n",
      "Epoch: 64 | Training loss: 0.558590 | Validation loss: 0.699241\n",
      "Epoch: 65 | Training loss: 0.555657 | Validation loss: 0.696508\n",
      "Epoch: 66 | Training loss: 0.552773 | Validation loss: 0.693818\n",
      "Epoch: 67 | Training loss: 0.549937 | Validation loss: 0.691170\n",
      "Epoch: 68 | Training loss: 0.547148 | Validation loss: 0.688562\n",
      "Epoch: 69 | Training loss: 0.544404 | Validation loss: 0.685994\n",
      "Epoch: 70 | Training loss: 0.541704 | Validation loss: 0.683463\n",
      "Epoch: 71 | Training loss: 0.539046 | Validation loss: 0.680971\n",
      "Epoch: 72 | Training loss: 0.536431 | Validation loss: 0.678516\n",
      "Epoch: 73 | Training loss: 0.533855 | Validation loss: 0.676096\n",
      "Epoch: 74 | Training loss: 0.531320 | Validation loss: 0.673712\n",
      "Epoch: 75 | Training loss: 0.528823 | Validation loss: 0.671362\n",
      "Epoch: 76 | Training loss: 0.526363 | Validation loss: 0.669045\n",
      "Epoch: 77 | Training loss: 0.523939 | Validation loss: 0.666761\n",
      "Epoch: 78 | Training loss: 0.521551 | Validation loss: 0.664510\n",
      "Epoch: 79 | Training loss: 0.519197 | Validation loss: 0.662289\n",
      "Epoch: 80 | Training loss: 0.516877 | Validation loss: 0.660099\n",
      "Epoch: 81 | Training loss: 0.514590 | Validation loss: 0.657938\n",
      "Epoch: 82 | Training loss: 0.512335 | Validation loss: 0.655807\n",
      "Epoch: 83 | Training loss: 0.510112 | Validation loss: 0.653703\n",
      "Epoch: 84 | Training loss: 0.507918 | Validation loss: 0.651628\n",
      "Epoch: 85 | Training loss: 0.505755 | Validation loss: 0.649581\n",
      "Epoch: 86 | Training loss: 0.503620 | Validation loss: 0.647560\n",
      "Epoch: 87 | Training loss: 0.501514 | Validation loss: 0.645565\n",
      "Epoch: 88 | Training loss: 0.499436 | Validation loss: 0.643596\n",
      "Epoch: 89 | Training loss: 0.497385 | Validation loss: 0.641651\n",
      "Epoch: 90 | Training loss: 0.495360 | Validation loss: 0.639732\n",
      "Epoch: 91 | Training loss: 0.493361 | Validation loss: 0.637837\n",
      "Epoch: 92 | Training loss: 0.491388 | Validation loss: 0.635965\n",
      "Epoch: 93 | Training loss: 0.489439 | Validation loss: 0.634116\n",
      "Epoch: 94 | Training loss: 0.487514 | Validation loss: 0.632290\n",
      "Epoch: 95 | Training loss: 0.485613 | Validation loss: 0.630485\n",
      "Epoch: 96 | Training loss: 0.483735 | Validation loss: 0.628702\n",
      "Epoch: 97 | Training loss: 0.481880 | Validation loss: 0.626941\n",
      "Epoch: 98 | Training loss: 0.480046 | Validation loss: 0.625200\n",
      "Epoch: 99 | Training loss: 0.478235 | Validation loss: 0.623480\n",
      "Epoch: 100 | Training loss: 0.476445 | Validation loss: 0.621780\n",
      "Epoch: 101 | Training loss: 0.474676 | Validation loss: 0.620100\n",
      "Epoch: 102 | Training loss: 0.472927 | Validation loss: 0.618439\n",
      "Epoch: 103 | Training loss: 0.471198 | Validation loss: 0.616797\n",
      "Epoch: 104 | Training loss: 0.469489 | Validation loss: 0.615174\n",
      "Epoch: 105 | Training loss: 0.467798 | Validation loss: 0.613568\n",
      "Epoch: 106 | Training loss: 0.466127 | Validation loss: 0.611981\n",
      "Epoch: 107 | Training loss: 0.464474 | Validation loss: 0.610411\n",
      "Epoch: 108 | Training loss: 0.462840 | Validation loss: 0.608859\n",
      "Epoch: 109 | Training loss: 0.461223 | Validation loss: 0.607324\n",
      "Epoch: 110 | Training loss: 0.459623 | Validation loss: 0.605805\n",
      "Epoch: 111 | Training loss: 0.458041 | Validation loss: 0.604303\n",
      "Epoch: 112 | Training loss: 0.456475 | Validation loss: 0.602818\n",
      "Epoch: 113 | Training loss: 0.454926 | Validation loss: 0.601347\n",
      "Epoch: 114 | Training loss: 0.453393 | Validation loss: 0.599892\n",
      "Epoch: 115 | Training loss: 0.451876 | Validation loss: 0.598452\n",
      "Epoch: 116 | Training loss: 0.450375 | Validation loss: 0.597028\n",
      "Epoch: 117 | Training loss: 0.448888 | Validation loss: 0.595617\n",
      "Epoch: 118 | Training loss: 0.447417 | Validation loss: 0.594222\n",
      "Epoch: 119 | Training loss: 0.445961 | Validation loss: 0.592841\n",
      "Epoch: 120 | Training loss: 0.444519 | Validation loss: 0.591474\n",
      "Epoch: 121 | Training loss: 0.443091 | Validation loss: 0.590120\n",
      "Epoch: 122 | Training loss: 0.441677 | Validation loss: 0.588781\n",
      "Epoch: 123 | Training loss: 0.440277 | Validation loss: 0.587454\n",
      "Epoch: 124 | Training loss: 0.438890 | Validation loss: 0.586141\n",
      "Epoch: 125 | Training loss: 0.437517 | Validation loss: 0.584841\n",
      "Epoch: 126 | Training loss: 0.436157 | Validation loss: 0.583553\n",
      "Epoch: 127 | Training loss: 0.434810 | Validation loss: 0.582278\n",
      "Epoch: 128 | Training loss: 0.433475 | Validation loss: 0.581015\n",
      "Epoch: 129 | Training loss: 0.432153 | Validation loss: 0.579764\n",
      "Epoch: 130 | Training loss: 0.430843 | Validation loss: 0.578525\n",
      "Epoch: 131 | Training loss: 0.429545 | Validation loss: 0.577297\n",
      "Epoch: 132 | Training loss: 0.428259 | Validation loss: 0.576082\n",
      "Epoch: 133 | Training loss: 0.426985 | Validation loss: 0.574878\n",
      "Epoch: 134 | Training loss: 0.425722 | Validation loss: 0.573685\n",
      "Epoch: 135 | Training loss: 0.424470 | Validation loss: 0.572503\n",
      "Epoch: 136 | Training loss: 0.423229 | Validation loss: 0.571332\n",
      "Epoch: 137 | Training loss: 0.422000 | Validation loss: 0.570171\n",
      "Epoch: 138 | Training loss: 0.420781 | Validation loss: 0.569021\n",
      "Epoch: 139 | Training loss: 0.419573 | Validation loss: 0.567881\n",
      "Epoch: 140 | Training loss: 0.418375 | Validation loss: 0.566752\n",
      "Epoch: 141 | Training loss: 0.417188 | Validation loss: 0.565632\n",
      "Epoch: 142 | Training loss: 0.416010 | Validation loss: 0.564523\n",
      "Epoch: 143 | Training loss: 0.414843 | Validation loss: 0.563424\n",
      "Epoch: 144 | Training loss: 0.413685 | Validation loss: 0.562334\n",
      "Epoch: 145 | Training loss: 0.412537 | Validation loss: 0.561253\n",
      "Epoch: 146 | Training loss: 0.411399 | Validation loss: 0.560183\n",
      "Epoch: 147 | Training loss: 0.410270 | Validation loss: 0.559120\n",
      "Epoch: 148 | Training loss: 0.409150 | Validation loss: 0.558067\n",
      "Epoch: 149 | Training loss: 0.408040 | Validation loss: 0.557024\n",
      "Epoch: 150 | Training loss: 0.406938 | Validation loss: 0.555989\n",
      "Epoch: 151 | Training loss: 0.405846 | Validation loss: 0.554963\n",
      "Epoch: 152 | Training loss: 0.404762 | Validation loss: 0.553946\n",
      "Epoch: 153 | Training loss: 0.403687 | Validation loss: 0.552937\n",
      "Epoch: 154 | Training loss: 0.402620 | Validation loss: 0.551936\n",
      "Epoch: 155 | Training loss: 0.401562 | Validation loss: 0.550943\n",
      "lr = 0.000100, alpha= 0.000100; Predict_Class:0.823333\n"
     ]
    }
   ],
   "source": [
    "lr_set = [0.001,0.0005,0.0001]\n",
    "alpha_set = [0.001,0.0005,0.0001]\n",
    "\n",
    "for lr_sid in range(len(lr_set)):\n",
    "    for alpha_sid in range(len(alpha_set)):\n",
    "        w_count, loss_tr_count, dev_loss_count = SGD(X_tr_count, data_tr_label,vocab,\n",
    "                                             X_dev=X_dev_count, \n",
    "                                             Y_dev=data_dev_label,\n",
    "                                             num_classes=3,\n",
    "                                             lr=lr_set[lr_sid], \n",
    "                                             alpha=alpha_set[alpha_sid], \n",
    "                                             epochs=200)\n",
    "        preds_te = predict_class(X = X_test_count, weights = w_count)\n",
    "        print(\"lr = %f, alpha= %f;\"%(lr_set[lr_sid],alpha_set[alpha_sid]),'Predict_Class:%f'%(f1_score(Y_te,preds_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous result, we can see that as decreasing the value of lr and alpha, the F1-Score is increasing.\n",
    "\n",
    "In conclusion,the both lr and alpha has the great impact the training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now evaluate BOW-tfidf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.959970 | Validation loss: 0.957439\n",
      "Epoch: 1 | Training loss: 0.763611 | Validation loss: 0.867650\n",
      "Epoch: 2 | Training loss: 0.650473 | Validation loss: 0.803574\n",
      "Epoch: 3 | Training loss: 0.574929 | Validation loss: 0.754995\n",
      "Epoch: 4 | Training loss: 0.520227 | Validation loss: 0.716595\n",
      "Epoch: 5 | Training loss: 0.478179 | Validation loss: 0.685185\n",
      "Epoch: 6 | Training loss: 0.444663 | Validation loss: 0.659090\n",
      "Epoch: 7 | Training loss: 0.417000 | Validation loss: 0.636840\n",
      "Epoch: 8 | Training loss: 0.393671 | Validation loss: 0.617503\n",
      "Epoch: 9 | Training loss: 0.373643 | Validation loss: 0.600592\n",
      "Epoch: 10 | Training loss: 0.356148 | Validation loss: 0.585546\n",
      "Epoch: 11 | Training loss: 0.340754 | Validation loss: 0.572148\n",
      "Epoch: 12 | Training loss: 0.327017 | Validation loss: 0.560089\n",
      "Epoch: 13 | Training loss: 0.314632 | Validation loss: 0.549119\n",
      "Epoch: 14 | Training loss: 0.303426 | Validation loss: 0.539062\n",
      "Epoch: 15 | Training loss: 0.293215 | Validation loss: 0.529844\n",
      "Epoch: 16 | Training loss: 0.283825 | Validation loss: 0.521364\n",
      "Epoch: 17 | Training loss: 0.275171 | Validation loss: 0.513474\n",
      "Epoch: 18 | Training loss: 0.267169 | Validation loss: 0.506176\n",
      "Epoch: 19 | Training loss: 0.259722 | Validation loss: 0.499357\n",
      "Epoch: 20 | Training loss: 0.252761 | Validation loss: 0.492999\n",
      "Epoch: 21 | Training loss: 0.246258 | Validation loss: 0.487005\n",
      "Epoch: 22 | Training loss: 0.240162 | Validation loss: 0.481363\n",
      "Epoch: 23 | Training loss: 0.234407 | Validation loss: 0.476022\n",
      "Epoch: 24 | Training loss: 0.228987 | Validation loss: 0.471008\n",
      "Epoch: 25 | Training loss: 0.223852 | Validation loss: 0.466253\n",
      "Epoch: 26 | Training loss: 0.218994 | Validation loss: 0.461734\n",
      "Epoch: 27 | Training loss: 0.214367 | Validation loss: 0.457388\n",
      "Epoch: 28 | Training loss: 0.209991 | Validation loss: 0.453303\n",
      "Epoch: 29 | Training loss: 0.205807 | Validation loss: 0.449379\n",
      "Epoch: 30 | Training loss: 0.201817 | Validation loss: 0.445660\n",
      "Epoch: 31 | Training loss: 0.198002 | Validation loss: 0.442101\n",
      "Epoch: 32 | Training loss: 0.194354 | Validation loss: 0.438700\n",
      "Epoch: 33 | Training loss: 0.190864 | Validation loss: 0.435394\n",
      "Epoch: 34 | Training loss: 0.187513 | Validation loss: 0.432245\n",
      "Epoch: 35 | Training loss: 0.184288 | Validation loss: 0.429241\n",
      "Epoch: 36 | Training loss: 0.181199 | Validation loss: 0.426325\n",
      "Epoch: 37 | Training loss: 0.178220 | Validation loss: 0.423520\n",
      "Epoch: 38 | Training loss: 0.175357 | Validation loss: 0.420853\n",
      "Epoch: 39 | Training loss: 0.172589 | Validation loss: 0.418254\n",
      "Epoch: 40 | Training loss: 0.169925 | Validation loss: 0.415731\n",
      "Epoch: 41 | Training loss: 0.167355 | Validation loss: 0.413295\n",
      "Epoch: 42 | Training loss: 0.164866 | Validation loss: 0.410973\n",
      "Epoch: 43 | Training loss: 0.162458 | Validation loss: 0.408677\n",
      "Epoch: 44 | Training loss: 0.160134 | Validation loss: 0.406480\n",
      "Epoch: 45 | Training loss: 0.157879 | Validation loss: 0.404358\n",
      "Epoch: 46 | Training loss: 0.155694 | Validation loss: 0.402284\n",
      "Epoch: 47 | Training loss: 0.153574 | Validation loss: 0.400257\n",
      "Epoch: 48 | Training loss: 0.151522 | Validation loss: 0.398323\n",
      "Epoch: 49 | Training loss: 0.149530 | Validation loss: 0.396438\n",
      "Epoch: 50 | Training loss: 0.147595 | Validation loss: 0.394620\n",
      "Epoch: 51 | Training loss: 0.145705 | Validation loss: 0.392820\n",
      "Epoch: 52 | Training loss: 0.143878 | Validation loss: 0.391112\n",
      "Epoch: 53 | Training loss: 0.142105 | Validation loss: 0.389427\n",
      "Epoch: 54 | Training loss: 0.140370 | Validation loss: 0.387795\n",
      "Epoch: 55 | Training loss: 0.138685 | Validation loss: 0.386205\n",
      "Epoch: 56 | Training loss: 0.137041 | Validation loss: 0.384659\n",
      "Epoch: 57 | Training loss: 0.135442 | Validation loss: 0.383138\n",
      "Epoch: 58 | Training loss: 0.133885 | Validation loss: 0.381675\n",
      "Epoch: 59 | Training loss: 0.132362 | Validation loss: 0.380233\n",
      "Epoch: 60 | Training loss: 0.130876 | Validation loss: 0.378855\n",
      "Epoch: 61 | Training loss: 0.129429 | Validation loss: 0.377465\n",
      "Epoch: 62 | Training loss: 0.128018 | Validation loss: 0.376132\n",
      "Epoch: 63 | Training loss: 0.126639 | Validation loss: 0.374839\n",
      "Epoch: 64 | Training loss: 0.125284 | Validation loss: 0.373554\n",
      "Epoch: 65 | Training loss: 0.123971 | Validation loss: 0.372315\n",
      "Epoch: 66 | Training loss: 0.122680 | Validation loss: 0.371096\n",
      "Epoch: 67 | Training loss: 0.121423 | Validation loss: 0.369901\n",
      "Epoch: 68 | Training loss: 0.120193 | Validation loss: 0.368746\n",
      "Epoch: 69 | Training loss: 0.118986 | Validation loss: 0.367622\n",
      "Epoch: 70 | Training loss: 0.117807 | Validation loss: 0.366534\n",
      "Epoch: 71 | Training loss: 0.116650 | Validation loss: 0.365446\n",
      "Epoch: 72 | Training loss: 0.115521 | Validation loss: 0.364399\n",
      "Epoch: 73 | Training loss: 0.114408 | Validation loss: 0.363356\n",
      "Epoch: 74 | Training loss: 0.113325 | Validation loss: 0.362338\n",
      "Epoch: 75 | Training loss: 0.112265 | Validation loss: 0.361331\n",
      "Epoch: 76 | Training loss: 0.111224 | Validation loss: 0.360346\n"
     ]
    }
   ],
   "source": [
    "w_count, loss_tr_count, dev_loss_count = SGD(X_tr_tfidf, data_tr_label,vocab,\n",
    "                                             X_dev=X_dev_tfidf, \n",
    "                                             Y_dev=data_dev_label,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.0001, \n",
    "                                             alpha=0.001, \n",
    "                                             epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,len(loss_tr_count),len(loss_tr_count))\n",
    "y1, y2 = loss_tr_count, dev_loss_count\n",
    " \n",
    "plt.plot(x, y1,label='Training loss')\n",
    "plt.plot(x, y2, label='Validation loss')\n",
    " \n",
    "plt.title('Training Monitoring')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8755555555555555\n",
      "Precision: 0.8417508417508418\n",
      "Recall: 0.8333333333333334\n",
      "F1-Score: 0.8375209380234506\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(X = X_test_count, weights = w_count)\n",
    "Y_te = data_test_label\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te_count))\n",
    "print('Precision:', precision_score(Y_te,preds_te_count))\n",
    "print('Recall:', recall_score(Y_te,preds_te_count))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:16:42.567569Z",
     "start_time": "2020-02-15T14:16:42.562560Z"
    }
   },
   "source": [
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |  0.8233 |  0.8233 | 0.8233 |\n",
    "| BOW-tfidf  | 0.8417  |  0.8333 | 0.8375  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
