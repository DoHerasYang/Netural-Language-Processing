{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [COM4513-6513] Assignment 2: Text Classification with a Feedforward Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.read_csv('./data_topic/train.csv',header=None,names=[\"label\",\"text\"])\n",
    "dev_data=pd.read_csv('./data_topic/dev.csv',header=None,names=[\"label\",\"text\"])\n",
    "test_data=pd.read_csv('./data_topic/test.csv',header=None,names=[\"label\",\"text\"])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Make the raw texts into lists and their corresponding labels into  np.arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.753874Z",
     "start_time": "2020-04-02T14:26:39.749647Z"
    }
   },
   "outputs": [],
   "source": [
    "def creat_list_array(data_text,data_label):\n",
    "    x=data_text.tolist()\n",
    "    y=np.array(data_label)\n",
    "    return x,y\n",
    "def lower_F(data):\n",
    "    lower_list=[]\n",
    "    for i in range(len(data)):\n",
    "        lower_data=str.lower(data[i])\n",
    "        lower_list.append(lower_data)\n",
    "    return lower_list\n",
    "\n",
    "# Transform train data\n",
    "data_train_x_raw,data_train_y=creat_list_array(train_data['text'],train_data['label']) #len 2400\n",
    "# Transform validation data\n",
    "data_dev_x_raw,data_dev_y=creat_list_array(dev_data['text'],dev_data['label']) #len 150\n",
    "# Transform test data\n",
    "data_test_x_raw,data_test_y=creat_list_array(test_data['text'],test_data['label']) #len 900\n",
    "\n",
    "# lower data\n",
    "data_train_x=lower_F(data_train_x_raw)\n",
    "data_dev_x=lower_F(data_dev_x_raw)\n",
    "data_test_x=lower_F(data_test_x_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:41.505459Z",
     "start_time": "2020-04-02T14:26:41.498388Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']\n",
    "\n",
    "# tokenise, create unigrams, using stop-words\n",
    "def tokenise(data,token_pattern,stop_words):\n",
    "    token_data=[]\n",
    "    token_list=re.findall(token_pattern,data)\n",
    "    for word in token_list:\n",
    "        if word not in stop_words:\n",
    "            token_data.append(word)\n",
    "    return token_data\n",
    "\n",
    "# based on the tokenised data(unigrams), create bigrams or trigrams\n",
    "def ngrams_generate(data,n):\n",
    "    result_list=[]\n",
    "    ngrams = zip(*[data[i:] for i in range(n)])\n",
    "    for ngram in ngrams:\n",
    "        result_list.append((ngram))\n",
    "    return result_list\n",
    "\n",
    "# extract ngrams function\n",
    "def extract_ngrams(x_raw,ngram_range=(1,3),token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',stop_words=[],vocab=set()):\n",
    "    # tokenise data\n",
    "    token_data=tokenise(x_raw,token_pattern=token_pattern,stop_words=stop_words)\n",
    "    # create ngrams list which save ngrams result\n",
    "    result_ngrams=[]\n",
    "    result_vocab=[]\n",
    "    # Extract ngrams based on the ngram_range\n",
    "    if ngram_range == 1:\n",
    "        result_ngrams = token_data\n",
    "    elif ngram_range[0]==1:\n",
    "        result_ngrams=token_data\n",
    "        for i in range(ngram_range[0],ngram_range[1]):\n",
    "            ngrams=ngrams_generate(token_data,i+1)\n",
    "            result_ngrams=result_ngrams+ngrams\n",
    "    else:\n",
    "        result_ngrams=ngrams_generate(token_data,ngram_range[0])\n",
    "        for i in range(ngram_range[0],ngram_range[1]):\n",
    "            ngrams=ngrams_generate(token_data,ngram_range[0]+1)\n",
    "            result_ngrams=result_ngrams+ngrams\n",
    "    # Extract specific vocab based on the vocab set()\n",
    "    if len(vocab)==0:\n",
    "        return result_ngrams\n",
    "    else:\n",
    "        for word in vocab:\n",
    "            if word in result_ngrams:\n",
    "                result_vocab.append(word)\n",
    "        return result_vocab\n",
    "    \n",
    "# Extract ngrams on the complete data set, for dev and test sets\n",
    "def extract_ngrams_for_test(X_data,ngram_range,stop_words=stop_words):\n",
    "    # Extract ngrams from raw data\n",
    "    ngrams_list_without_Ded=[]\n",
    "    for i in range(len(X_data)):\n",
    "        ngrams_data=extract_ngrams(X_data[i],ngram_range=ngram_range,stop_words=stop_words)\n",
    "        ngrams_list_without_Ded.append(ngrams_data)\n",
    "    return ngrams_list_without_Ded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
    "    ngrams_list = []\n",
    "    ngrams_list_without_Ded = []\n",
    "    for i in range(len(X_raw)):\n",
    "        ngrams_data=extract_ngrams(X_raw[i],ngram_range=ngram_range,stop_words=stop_words)\n",
    "        ngrams_list_without_Ded.append(ngrams_data)\n",
    "        # Deduplication\n",
    "        ngrams_data_Ded=sorted(set(ngrams_data),key=ngrams_data.index)\n",
    "        ngrams_list.append(ngrams_data_Ded)\n",
    "    \n",
    "    # create vocab dictionary\n",
    "    vocab_dict = {}\n",
    "    for i in range(len(ngrams_list)):\n",
    "        for word in ngrams_list[i]:\n",
    "            if word in vocab_dict:\n",
    "                vocab_dict[word]+=1\n",
    "            else:\n",
    "                vocab_dict[word]=1\n",
    "                \n",
    "    # keep ngrams with a minimun df\n",
    "    for word in list(vocab_dict.keys()):\n",
    "        if vocab_dict[word] < min_df:\n",
    "            del vocab_dict[word]\n",
    "    \n",
    "    # sorted then keep only topN\n",
    "    vocab_sorted=sorted(vocab_dict.items(),key=lambda item:item[1],reverse=True)\n",
    "    if keep_topN == 0:\n",
    "        vocab_topN = vocab_sorted\n",
    "    else:\n",
    "        vocab_topN = vocab_sorted[:keep_topN]\n",
    "        \n",
    "    vocab = []\n",
    "    for i in range(len(vocab_topN)):\n",
    "        vocab.append(vocab_topN[i][0])\n",
    "        \n",
    "    return vocab,vocab_topN,ngrams_list_without_Ded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract vocab, df——train, train token data\n",
    "vocab,df_tr,ngrams_without_Ded_tr=get_vocab(data_train_x,ngram_range=(1),  \n",
    "                                            min_df=0,keep_topN=0, stop_words=stop_words)\n",
    "# extract dev token\n",
    "ngrams_without_Ded_dev=extract_ngrams_for_test(data_dev_x,ngram_range=(1),stop_words=stop_words)\n",
    "# extract test token\n",
    "ngrams_without_Ded_test=extract_ngrams_for_test(data_test_x,ngram_range=(1),stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8931\n",
      "\n",
      "['stores', 'jan', 'renewed', 'coached', 'quarterfinals', 'scheduled', 'refugees', 'la', 'jns', 'tourists', 'advisers', 'call', 'embarrassment', 'mate', 'valuable', 'benazir', 'relinquish', 'revenues', 'one', 'scholarship', 'senate', 'equities', 'mistakes', 'left', 'erupts', 'marco', 'throw', 'opened', 'disciplinary', 'lance', 'surprise', 'career', 'downturn', 'conjunctivitis', 'usatoday', 'instruction', 'deflated', 'euro', 'gravely', 'occupation', 'warehouses', 'blows', 'alcoa', 'protecting', 'grip', 'co', 'nicol', 'woes', 'tilt', 'des', 'penned', 'casting', 'denmark', 'older', 'strengthen', 'injury', 'brussels', 'declare', 'categories', 'glorious', 'earl', 'dom', 'charts', 'arsenal', 'debut', 'vulnerable', 'smudged', 'seals', 'murder', 'computers', 'shrugged', 'slammed', 'through', 'papal', 'lethal', 'luck', 'slapped', 'evaluation', 'wife', 'prv', 'cbi', 'organisers', 'breakingviews', 'astorga', 'counter', 'argentina', 'treading', 'squares', 'robbing', 'shenzhen', 'increase', 'chicago', 'monitoring', 'cantered', 'domain', 'ite', 'ideal', 'sergei', 'pitching', 'forehead']\n",
      "\n",
      "[('reuters', 631), ('said', 432), ('tuesday', 413), ('wednesday', 344), ('new', 325), ('after', 295), ('ap', 275), ('athens', 245), ('monday', 221), ('first', 210)]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print()\n",
    "print(random.sample(vocab,100))\n",
    "print()\n",
    "print(df_tr[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and id -> word dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dict(vocab):\n",
    "    id_word_dict = {}\n",
    "    word_id_dict = {}\n",
    "    for i in range(len(vocab)):\n",
    "        id_word_dict[i] = vocab[i]\n",
    "        word_id_dict[vocab[i]] = i\n",
    "    return id_word_dict,word_id_dict\n",
    "id_word_dict,word_id_dict = create_dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reuters': 0,\n",
       " 'said': 1,\n",
       " 'tuesday': 2,\n",
       " 'wednesday': 3,\n",
       " 'new': 4,\n",
       " 'after': 5,\n",
       " 'ap': 6,\n",
       " 'athens': 7,\n",
       " 'monday': 8,\n",
       " 'first': 9,\n",
       " 'two': 10,\n",
       " 'york': 11,\n",
       " 'over': 12,\n",
       " 'us': 13,\n",
       " 'olympic': 14,\n",
       " 'inc': 15,\n",
       " 'more': 16,\n",
       " 'year': 17,\n",
       " 'oil': 18,\n",
       " 'prices': 19,\n",
       " 'company': 20,\n",
       " 'world': 21,\n",
       " 'than': 22,\n",
       " 'aug': 23,\n",
       " 'about': 24,\n",
       " 'had': 25,\n",
       " 'united': 26,\n",
       " 'one': 27,\n",
       " 'out': 28,\n",
       " 'sunday': 29,\n",
       " 'into': 30,\n",
       " 'against': 31,\n",
       " 'up': 32,\n",
       " 'second': 33,\n",
       " 'last': 34,\n",
       " 'president': 35,\n",
       " 'stocks': 36,\n",
       " 'gold': 37,\n",
       " 'team': 38,\n",
       " 'when': 39,\n",
       " 'three': 40,\n",
       " 'night': 41,\n",
       " 'time': 42,\n",
       " 'yesterday': 43,\n",
       " 'games': 44,\n",
       " 'olympics': 45,\n",
       " 'states': 46,\n",
       " 'greece': 47,\n",
       " 'off': 48,\n",
       " 'iraq': 49,\n",
       " 'washington': 50,\n",
       " 'percent': 51,\n",
       " 'home': 52,\n",
       " 'day': 53,\n",
       " 'google': 54,\n",
       " 'public': 55,\n",
       " 'record': 56,\n",
       " 'week': 57,\n",
       " 'men': 58,\n",
       " 'government': 59,\n",
       " 'win': 60,\n",
       " 'american': 61,\n",
       " 'won': 62,\n",
       " 'years': 63,\n",
       " 'all': 64,\n",
       " 'billion': 65,\n",
       " 'shares': 66,\n",
       " 'city': 67,\n",
       " 'offering': 68,\n",
       " 'officials': 69,\n",
       " 'would': 70,\n",
       " 'today': 71,\n",
       " 'final': 72,\n",
       " 'afp': 73,\n",
       " 'gt': 74,\n",
       " 'people': 75,\n",
       " 'lt': 76,\n",
       " 'medal': 77,\n",
       " 'corp': 78,\n",
       " 'sales': 79,\n",
       " 'country': 80,\n",
       " 'back': 81,\n",
       " 'four': 82,\n",
       " 'high': 83,\n",
       " 'investor': 84,\n",
       " 'com': 85,\n",
       " 'minister': 86,\n",
       " 'reported': 87,\n",
       " 'month': 88,\n",
       " 'initial': 89,\n",
       " 'profit': 90,\n",
       " 'ticker': 91,\n",
       " 'al': 92,\n",
       " 'million': 93,\n",
       " 'top': 94,\n",
       " 'before': 95,\n",
       " 'china': 96,\n",
       " 'him': 97,\n",
       " 'national': 98,\n",
       " 'najaf': 99,\n",
       " 'victory': 100,\n",
       " 'end': 101,\n",
       " 'third': 102,\n",
       " 'target': 103,\n",
       " 'troops': 104,\n",
       " 'michael': 105,\n",
       " 'largest': 106,\n",
       " 'game': 107,\n",
       " 'left': 108,\n",
       " 'group': 109,\n",
       " 'price': 110,\n",
       " 'international': 111,\n",
       " 'http': 112,\n",
       " 'hit': 113,\n",
       " 'eight': 114,\n",
       " 'state': 115,\n",
       " 'expected': 116,\n",
       " 'plans': 117,\n",
       " 'href': 118,\n",
       " 'www': 119,\n",
       " 'fullquote': 120,\n",
       " 'aspx': 121,\n",
       " 'quickinfo': 122,\n",
       " 'most': 123,\n",
       " 'july': 124,\n",
       " 'london': 125,\n",
       " 'season': 126,\n",
       " 'while': 127,\n",
       " 'down': 128,\n",
       " 'least': 129,\n",
       " 'iraqi': 130,\n",
       " 'some': 131,\n",
       " 'may': 132,\n",
       " 'bank': 133,\n",
       " 'quarter': 134,\n",
       " 'john': 135,\n",
       " 'just': 136,\n",
       " 'round': 137,\n",
       " 'rose': 138,\n",
       " 'since': 139,\n",
       " 'between': 140,\n",
       " 'other': 141,\n",
       " 'crude': 142,\n",
       " 'another': 143,\n",
       " 'stock': 144,\n",
       " 'earnings': 145,\n",
       " 'market': 146,\n",
       " 'during': 147,\n",
       " 'run': 148,\n",
       " 'women': 149,\n",
       " 'former': 150,\n",
       " 'quot': 151,\n",
       " 'five': 152,\n",
       " 'prime': 153,\n",
       " 'set': 154,\n",
       " 'higher': 155,\n",
       " 'costs': 156,\n",
       " 'investors': 157,\n",
       " 'police': 158,\n",
       " 'australia': 159,\n",
       " 'half': 160,\n",
       " 'south': 161,\n",
       " 'biggest': 162,\n",
       " 'quarterly': 163,\n",
       " 'court': 164,\n",
       " 'fell': 165,\n",
       " 'chicago': 166,\n",
       " 'san': 167,\n",
       " 'phelps': 168,\n",
       " 'near': 169,\n",
       " 'days': 170,\n",
       " 'financial': 171,\n",
       " 'hugo': 172,\n",
       " 'next': 173,\n",
       " 'made': 174,\n",
       " 'chavez': 175,\n",
       " 'israeli': 176,\n",
       " 'west': 177,\n",
       " 'hurricane': 178,\n",
       " 'because': 179,\n",
       " 'british': 180,\n",
       " 'leader': 181,\n",
       " 'according': 182,\n",
       " 'street': 183,\n",
       " 'business': 184,\n",
       " 'plan': 185,\n",
       " 'earlier': 186,\n",
       " 'news': 187,\n",
       " 'search': 188,\n",
       " 'wall': 189,\n",
       " 'exchange': 190,\n",
       " 'strong': 191,\n",
       " 'federal': 192,\n",
       " 'where': 193,\n",
       " 'paul': 194,\n",
       " 'nearly': 195,\n",
       " 'basketball': 196,\n",
       " 'loss': 197,\n",
       " 'major': 198,\n",
       " 'champion': 199,\n",
       " 'charley': 200,\n",
       " 'cut': 201,\n",
       " 'economy': 202,\n",
       " 'killed': 203,\n",
       " 'shot': 204,\n",
       " 'co': 205,\n",
       " 'announced': 206,\n",
       " 'giant': 207,\n",
       " 'early': 208,\n",
       " 'cleric': 209,\n",
       " 'through': 210,\n",
       " 'only': 211,\n",
       " 'official': 212,\n",
       " 'start': 213,\n",
       " 'report': 214,\n",
       " 'england': 215,\n",
       " 'bloomberg': 216,\n",
       " 'right': 217,\n",
       " 'talks': 218,\n",
       " 'bush': 219,\n",
       " 'mark': 220,\n",
       " 'even': 221,\n",
       " 'securities': 222,\n",
       " 'commission': 223,\n",
       " 'put': 224,\n",
       " 'took': 225,\n",
       " 'lead': 226,\n",
       " 'under': 227,\n",
       " 'freestyle': 228,\n",
       " 'man': 229,\n",
       " 'meter': 230,\n",
       " 'canadian': 231,\n",
       " 'fourth': 232,\n",
       " 'economic': 233,\n",
       " 'region': 234,\n",
       " 'long': 235,\n",
       " 'militia': 236,\n",
       " 'homes': 237,\n",
       " 'medals': 238,\n",
       " 'lost': 239,\n",
       " 'agreed': 240,\n",
       " 'now': 241,\n",
       " 'six': 242,\n",
       " 'based': 243,\n",
       " 'consumer': 244,\n",
       " 'ahead': 245,\n",
       " 'demand': 246,\n",
       " 'research': 247,\n",
       " 'profile': 248,\n",
       " 'share': 249,\n",
       " 'seven': 250,\n",
       " 'military': 251,\n",
       " 'service': 252,\n",
       " 'nation': 253,\n",
       " 'sadr': 254,\n",
       " 'venezuela': 255,\n",
       " 'results': 256,\n",
       " 'fighting': 257,\n",
       " 'peace': 258,\n",
       " 'coach': 259,\n",
       " 'months': 260,\n",
       " 'posted': 261,\n",
       " 'referendum': 262,\n",
       " 'including': 263,\n",
       " 'florida': 264,\n",
       " 'baghdad': 265,\n",
       " 'war': 266,\n",
       " 'murder': 267,\n",
       " 'close': 268,\n",
       " 'players': 269,\n",
       " 'maker': 270,\n",
       " 'western': 271,\n",
       " 'despite': 272,\n",
       " 'says': 273,\n",
       " 'old': 274,\n",
       " 'leading': 275,\n",
       " 'competition': 276,\n",
       " 'part': 277,\n",
       " 'francisco': 278,\n",
       " 'still': 279,\n",
       " 'buy': 280,\n",
       " 'league': 281,\n",
       " 'amid': 282,\n",
       " 'army': 283,\n",
       " 'forces': 284,\n",
       " 'holy': 285,\n",
       " 'press': 286,\n",
       " 'nations': 287,\n",
       " 'beat': 288,\n",
       " 'political': 289,\n",
       " 'department': 290,\n",
       " 'helped': 291,\n",
       " 'data': 292,\n",
       " 'lower': 293,\n",
       " 'per': 294,\n",
       " 'quote': 295,\n",
       " 'race': 296,\n",
       " 'radical': 297,\n",
       " 'go': 298,\n",
       " 'capital': 299,\n",
       " 'trial': 300,\n",
       " 'europe': 301,\n",
       " 'championship': 302,\n",
       " 'inflation': 303,\n",
       " 'trading': 304,\n",
       " 'estimates': 305,\n",
       " 'greek': 306,\n",
       " 'winning': 307,\n",
       " 'vote': 308,\n",
       " 'decision': 309,\n",
       " 'whether': 310,\n",
       " 'history': 311,\n",
       " 'say': 312,\n",
       " 'many': 313,\n",
       " 'past': 314,\n",
       " 'make': 315,\n",
       " 'tournament': 316,\n",
       " 'thousands': 317,\n",
       " 'security': 318,\n",
       " 'north': 319,\n",
       " 'way': 320,\n",
       " 'help': 321,\n",
       " 'reports': 322,\n",
       " 'being': 323,\n",
       " 'play': 324,\n",
       " 'kenteris': 325,\n",
       " 'party': 326,\n",
       " 'making': 327,\n",
       " 'late': 328,\n",
       " 'drop': 329,\n",
       " 'shi': 330,\n",
       " 'much': 331,\n",
       " 'dollar': 332,\n",
       " 'sox': 333,\n",
       " 'here': 334,\n",
       " 'should': 335,\n",
       " 'election': 336,\n",
       " 'authorities': 337,\n",
       " 'conference': 338,\n",
       " 'held': 339,\n",
       " 'internet': 340,\n",
       " 'growth': 341,\n",
       " 'global': 342,\n",
       " 'services': 343,\n",
       " 'ite': 344,\n",
       " 'showed': 345,\n",
       " 'web': 346,\n",
       " 'career': 347,\n",
       " 'israel': 348,\n",
       " 'killing': 349,\n",
       " 'saturday': 350,\n",
       " 'running': 351,\n",
       " 'general': 352,\n",
       " 'behind': 353,\n",
       " 'russian': 354,\n",
       " 'sports': 355,\n",
       " 'industry': 356,\n",
       " 'led': 357,\n",
       " 'missed': 358,\n",
       " 'big': 359,\n",
       " 'summer': 360,\n",
       " 'take': 361,\n",
       " 'sprinters': 362,\n",
       " 'get': 363,\n",
       " 'contract': 364,\n",
       " 'tennis': 365,\n",
       " 'inning': 366,\n",
       " 'retailer': 367,\n",
       " 'darfur': 368,\n",
       " 'foreign': 369,\n",
       " 'key': 370,\n",
       " 'air': 371,\n",
       " 'japan': 372,\n",
       " 'thursday': 373,\n",
       " 'ian': 374,\n",
       " 'recall': 375,\n",
       " 'also': 376,\n",
       " 'august': 377,\n",
       " 'good': 378,\n",
       " 'opposition': 379,\n",
       " 'again': 380,\n",
       " 'around': 381,\n",
       " 'charged': 382,\n",
       " 'open': 383,\n",
       " 'manager': 384,\n",
       " 'double': 385,\n",
       " 'weeks': 386,\n",
       " 'drugs': 387,\n",
       " 'player': 388,\n",
       " 'cup': 389,\n",
       " 'little': 390,\n",
       " 'energy': 391,\n",
       " 'regulators': 392,\n",
       " 'case': 393,\n",
       " 'how': 394,\n",
       " 'engine': 395,\n",
       " 'title': 396,\n",
       " 'network': 397,\n",
       " 'pay': 398,\n",
       " 'korea': 399,\n",
       " 'leaders': 400,\n",
       " 'ago': 401,\n",
       " 'later': 402,\n",
       " 'canada': 403,\n",
       " 'interest': 404,\n",
       " 'los': 405,\n",
       " 'following': 406,\n",
       " 'americans': 407,\n",
       " 'cash': 408,\n",
       " 'pool': 409,\n",
       " 'companies': 410,\n",
       " 'number': 411,\n",
       " 'food': 412,\n",
       " 'beijing': 413,\n",
       " 'gaza': 414,\n",
       " 'firm': 415,\n",
       " 'atlanta': 416,\n",
       " 'began': 417,\n",
       " 'place': 418,\n",
       " 'soldiers': 419,\n",
       " 'sudan': 420,\n",
       " 'shrine': 421,\n",
       " 'thorpe': 422,\n",
       " 'center': 423,\n",
       " 'singh': 424,\n",
       " 'shiite': 425,\n",
       " 'cost': 426,\n",
       " 'committee': 427,\n",
       " 'ever': 428,\n",
       " 'angeles': 429,\n",
       " 'heavy': 430,\n",
       " 'thanou': 431,\n",
       " 'bid': 432,\n",
       " 'britain': 433,\n",
       " 'taking': 434,\n",
       " 'life': 435,\n",
       " 'toronto': 436,\n",
       " 'latest': 437,\n",
       " 'sharply': 438,\n",
       " 'head': 439,\n",
       " 'labor': 440,\n",
       " 'republic': 441,\n",
       " 'net': 442,\n",
       " 'deal': 443,\n",
       " 'slashed': 444,\n",
       " 'hamm': 445,\n",
       " 'store': 446,\n",
       " 'palestinian': 447,\n",
       " 'rebels': 448,\n",
       " 'democratic': 449,\n",
       " 'drug': 450,\n",
       " 'across': 451,\n",
       " 'tokyo': 452,\n",
       " 'afghanistan': 453,\n",
       " 'hours': 454,\n",
       " 'found': 455,\n",
       " 'field': 456,\n",
       " 'white': 457,\n",
       " 'militants': 458,\n",
       " 'such': 459,\n",
       " 'australian': 460,\n",
       " 'morning': 461,\n",
       " 'face': 462,\n",
       " 'ended': 463,\n",
       " 'germany': 464,\n",
       " 'hospital': 465,\n",
       " 'materials': 466,\n",
       " 'xinhuanet': 467,\n",
       " 'roger': 468,\n",
       " 'range': 469,\n",
       " 'boston': 470,\n",
       " 'red': 471,\n",
       " 'nuclear': 472,\n",
       " 'efforts': 473,\n",
       " 'friday': 474,\n",
       " 'due': 475,\n",
       " 'fresh': 476,\n",
       " 'both': 477,\n",
       " 'venezuelan': 478,\n",
       " 'starting': 479,\n",
       " 'relay': 480,\n",
       " 'pressure': 481,\n",
       " 'dropped': 482,\n",
       " 'straight': 483,\n",
       " 'released': 484,\n",
       " 'terror': 485,\n",
       " 'told': 486,\n",
       " 'agreement': 487,\n",
       " 'senior': 488,\n",
       " 'silver': 489,\n",
       " 'andy': 490,\n",
       " 'selling': 491,\n",
       " 'gymnastics': 492,\n",
       " 'office': 493,\n",
       " 'water': 494,\n",
       " 'central': 495,\n",
       " 'militiamen': 496,\n",
       " 'bomb': 497,\n",
       " 'india': 498,\n",
       " 'called': 499,\n",
       " 'saying': 500,\n",
       " 'caracas': 501,\n",
       " 'like': 502,\n",
       " 'money': 503,\n",
       " 'jerusalem': 504,\n",
       " 'accused': 505,\n",
       " 'america': 506,\n",
       " 'burundi': 507,\n",
       " 'defense': 508,\n",
       " 'soaring': 509,\n",
       " 'kostas': 510,\n",
       " 'test': 511,\n",
       " 'break': 512,\n",
       " 'delegation': 513,\n",
       " 'pakistan': 514,\n",
       " 'drove': 515,\n",
       " 'pga': 516,\n",
       " 'outlook': 517,\n",
       " 'jones': 518,\n",
       " 'fears': 519,\n",
       " 'sell': 520,\n",
       " 'injury': 521,\n",
       " 'filed': 522,\n",
       " 'approval': 523,\n",
       " 'fall': 524,\n",
       " 'ariel': 525,\n",
       " 'sharon': 526,\n",
       " 'construction': 527,\n",
       " 'low': 528,\n",
       " 'chief': 529,\n",
       " 'olympia': 530,\n",
       " 'awaited': 531,\n",
       " 'barrel': 532,\n",
       " 'funds': 533,\n",
       " 'halliburton': 534,\n",
       " 'used': 535,\n",
       " 'attack': 536,\n",
       " 'visit': 537,\n",
       " 'kerry': 538,\n",
       " 'release': 539,\n",
       " 'nine': 540,\n",
       " 'rival': 541,\n",
       " 'moqtada': 542,\n",
       " 'asked': 543,\n",
       " 'finally': 544,\n",
       " 'opening': 545,\n",
       " 'match': 546,\n",
       " 'caused': 547,\n",
       " 'rates': 548,\n",
       " 'russia': 549,\n",
       " 'charges': 550,\n",
       " 'try': 551,\n",
       " 'children': 552,\n",
       " 'vijay': 553,\n",
       " 'return': 554,\n",
       " 'operator': 555,\n",
       " 'less': 556,\n",
       " 'miss': 557,\n",
       " 'leave': 558,\n",
       " 'action': 559,\n",
       " 'fifth': 560,\n",
       " 'gains': 561,\n",
       " 'sources': 562,\n",
       " 'event': 563,\n",
       " 'june': 564,\n",
       " 'become': 565,\n",
       " 'individual': 566,\n",
       " 'housing': 567,\n",
       " 'soccer': 568,\n",
       " 'future': 569,\n",
       " 'pulled': 570,\n",
       " 'club': 571,\n",
       " 'philadelphia': 572,\n",
       " 'send': 573,\n",
       " 'presidential': 574,\n",
       " 'agency': 575,\n",
       " 'possible': 576,\n",
       " 'short': 577,\n",
       " 'any': 578,\n",
       " 'keep': 579,\n",
       " 'power': 580,\n",
       " 'france': 581,\n",
       " 'failed': 582,\n",
       " 'calif': 583,\n",
       " 'going': 584,\n",
       " 'hundreds': 585,\n",
       " 'impact': 586,\n",
       " 'katerina': 587,\n",
       " 'rule': 588,\n",
       " 'own': 589,\n",
       " 'same': 590,\n",
       " 'captain': 591,\n",
       " 'texas': 592,\n",
       " 'got': 593,\n",
       " 'claims': 594,\n",
       " 'blue': 595,\n",
       " 'boost': 596,\n",
       " 'suspects': 597,\n",
       " 'showing': 598,\n",
       " 'investment': 599,\n",
       " 'tony': 600,\n",
       " 'highs': 601,\n",
       " 'euro': 602,\n",
       " 'need': 603,\n",
       " 'card': 604,\n",
       " 'golf': 605,\n",
       " 'fund': 606,\n",
       " 'ipo': 607,\n",
       " 'didn': 608,\n",
       " 'injured': 609,\n",
       " 'democracy': 610,\n",
       " 'term': 611,\n",
       " 'continue': 612,\n",
       " 'violence': 613,\n",
       " 'french': 614,\n",
       " 'came': 615,\n",
       " 'full': 616,\n",
       " 'residents': 617,\n",
       " 'then': 618,\n",
       " 'st': 619,\n",
       " 'battle': 620,\n",
       " 'members': 621,\n",
       " 'holding': 622,\n",
       " 'southern': 623,\n",
       " 'worries': 624,\n",
       " 'refugees': 625,\n",
       " 'camp': 626,\n",
       " 'rise': 627,\n",
       " 'begin': 628,\n",
       " 'went': 629,\n",
       " 'trying': 630,\n",
       " 'private': 631,\n",
       " 'became': 632,\n",
       " 'points': 633,\n",
       " 'star': 634,\n",
       " 'chain': 635,\n",
       " 'tax': 636,\n",
       " 'might': 637,\n",
       " 'bill': 638,\n",
       " 'european': 639,\n",
       " 'approved': 640,\n",
       " 'work': 641,\n",
       " 'host': 642,\n",
       " 'continued': 643,\n",
       " 'yukos': 644,\n",
       " 'road': 645,\n",
       " 'investigation': 646,\n",
       " 'runs': 647,\n",
       " 'beating': 648,\n",
       " 'meters': 649,\n",
       " 'innings': 650,\n",
       " 'stadium': 651,\n",
       " 'performance': 652,\n",
       " 'credit': 653,\n",
       " 'baseball': 654,\n",
       " 'depot': 655,\n",
       " 'check': 656,\n",
       " 'death': 657,\n",
       " 'fire': 658,\n",
       " 'edwards': 659,\n",
       " 'avoid': 660,\n",
       " 'meeting': 661,\n",
       " 'toward': 662,\n",
       " 'japanese': 663,\n",
       " 'move': 664,\n",
       " 'pushed': 665,\n",
       " 'fans': 666,\n",
       " 'getting': 667,\n",
       " 'urged': 668,\n",
       " 'reserve': 669,\n",
       " 'congolese': 670,\n",
       " 'weekend': 671,\n",
       " 'likely': 672,\n",
       " 'survived': 673,\n",
       " 'appeared': 674,\n",
       " 'islamic': 675,\n",
       " 'away': 676,\n",
       " 'rights': 677,\n",
       " 'without': 678,\n",
       " 'uprising': 679,\n",
       " 'well': 680,\n",
       " 'appeal': 681,\n",
       " 'workers': 682,\n",
       " 'surged': 683,\n",
       " 'ancient': 684,\n",
       " 'recent': 685,\n",
       " 'executive': 686,\n",
       " 'green': 687,\n",
       " 'eased': 688,\n",
       " 'real': 689,\n",
       " 'phone': 690,\n",
       " 'trade': 691,\n",
       " 'baltimore': 692,\n",
       " 'tests': 693,\n",
       " 'sold': 694,\n",
       " 'williams': 695,\n",
       " 'forecast': 696,\n",
       " 'giants': 697,\n",
       " 'mutual': 698,\n",
       " 'operating': 699,\n",
       " 'amp': 700,\n",
       " 'give': 701,\n",
       " 'better': 702,\n",
       " 'ossetia': 703,\n",
       " 'campaign': 704,\n",
       " 'giving': 705,\n",
       " 'course': 706,\n",
       " 'concern': 707,\n",
       " 'fla': 708,\n",
       " 'insurance': 709,\n",
       " 'reach': 710,\n",
       " 'qualifying': 711,\n",
       " 'van': 712,\n",
       " 'free': 713,\n",
       " 'hour': 714,\n",
       " 'family': 715,\n",
       " 'given': 716,\n",
       " 'puerto': 717,\n",
       " 'rico': 718,\n",
       " 'opened': 719,\n",
       " 'province': 720,\n",
       " 'storm': 721,\n",
       " 'asia': 722,\n",
       " 'increase': 723,\n",
       " 'sent': 724,\n",
       " 'missing': 725,\n",
       " 'fraud': 726,\n",
       " 'car': 727,\n",
       " 'decided': 728,\n",
       " 'rising': 729,\n",
       " 'lowe': 730,\n",
       " 'africa': 731,\n",
       " 'bronze': 732,\n",
       " 'union': 733,\n",
       " 'meet': 734,\n",
       " 'conflict': 735,\n",
       " 'boosted': 736,\n",
       " 'backed': 737,\n",
       " 'pitched': 738,\n",
       " 'gasoline': 739,\n",
       " 'federer': 740,\n",
       " 'czech': 741,\n",
       " 'cardinals': 742,\n",
       " 'blair': 743,\n",
       " 'italian': 744,\n",
       " 'alleged': 745,\n",
       " 'concerns': 746,\n",
       " 'far': 747,\n",
       " 'yet': 748,\n",
       " 'potential': 749,\n",
       " 'cleveland': 750,\n",
       " 'indians': 751,\n",
       " 'doping': 752,\n",
       " 'practice': 753,\n",
       " 'rebounded': 754,\n",
       " 'debt': 755,\n",
       " 'nortel': 756,\n",
       " 'mortgage': 757,\n",
       " 'turned': 758,\n",
       " 'wing': 759,\n",
       " 'sydney': 760,\n",
       " 'swimming': 761,\n",
       " 'georgia': 762,\n",
       " 'un': 763,\n",
       " 'sixth': 764,\n",
       " 'poor': 765,\n",
       " 'punta': 766,\n",
       " 'gorda': 767,\n",
       " 'look': 768,\n",
       " 'terrorism': 769,\n",
       " 'cp': 770,\n",
       " 'intelligence': 771,\n",
       " 'television': 772,\n",
       " 'haven': 773,\n",
       " 'republican': 774,\n",
       " 'charge': 775,\n",
       " 'silvio': 776,\n",
       " 'berlusconi': 777,\n",
       " 'until': 778,\n",
       " 'allen': 779,\n",
       " 'level': 780,\n",
       " 'popular': 781,\n",
       " 're': 782,\n",
       " 'received': 783,\n",
       " 'hard': 784,\n",
       " 'program': 785,\n",
       " 'quest': 786,\n",
       " 'raised': 787,\n",
       " 'foot': 788,\n",
       " 'anticipated': 789,\n",
       " 'growing': 790,\n",
       " 'nfl': 791,\n",
       " 'association': 792,\n",
       " 'dallas': 793,\n",
       " 'businesses': 794,\n",
       " 'media': 795,\n",
       " 'sharp': 796,\n",
       " 'markets': 797,\n",
       " 'twins': 798,\n",
       " 'withdrew': 799,\n",
       " 'losing': 800,\n",
       " 'jump': 801,\n",
       " 'defensive': 802,\n",
       " 'gave': 803,\n",
       " 'david': 804,\n",
       " 'having': 805,\n",
       " 'equity': 806,\n",
       " 'consecutive': 807,\n",
       " 'yankees': 808,\n",
       " 'montreal': 809,\n",
       " 'jumped': 810,\n",
       " 'exports': 811,\n",
       " 'equipment': 812,\n",
       " 'registration': 813,\n",
       " 'networks': 814,\n",
       " 'wholesale': 815,\n",
       " 'historic': 816,\n",
       " 'strike': 817,\n",
       " 'care': 818,\n",
       " 'african': 819,\n",
       " 'show': 820,\n",
       " 'among': 821,\n",
       " 'interim': 822,\n",
       " 'setting': 823,\n",
       " 'pope': 824,\n",
       " 'mass': 825,\n",
       " 'kabul': 826,\n",
       " 'huge': 827,\n",
       " 'cause': 828,\n",
       " 'insurers': 829,\n",
       " 'evening': 830,\n",
       " 'teenager': 831,\n",
       " 'town': 832,\n",
       " 'delegates': 833,\n",
       " 'wis': 834,\n",
       " 'grand': 835,\n",
       " 'best': 836,\n",
       " 'facing': 837,\n",
       " 'support': 838,\n",
       " 'qaeda': 839,\n",
       " 'pull': 840,\n",
       " 'iverson': 841,\n",
       " 'arrived': 842,\n",
       " 'chinese': 843,\n",
       " 'factories': 844,\n",
       " 'building': 845,\n",
       " 'exporters': 846,\n",
       " 'hearing': 847,\n",
       " 'negotiations': 848,\n",
       " 'teams': 849,\n",
       " 'call': 850,\n",
       " 'great': 851,\n",
       " 'wanted': 852,\n",
       " 'health': 853,\n",
       " 'bought': 854,\n",
       " 'wal': 855,\n",
       " 'mart': 856,\n",
       " 'stores': 857,\n",
       " 'dow': 858,\n",
       " 'complete': 859,\n",
       " 'products': 860,\n",
       " 'anti': 861,\n",
       " 'supplies': 862,\n",
       " 'see': 863,\n",
       " 'series': 864,\n",
       " 'religious': 865,\n",
       " 'income': 866,\n",
       " 'claim': 867,\n",
       " 'island': 868,\n",
       " 'jewish': 869,\n",
       " 'statement': 870,\n",
       " 'football': 871,\n",
       " 'september': 872,\n",
       " 'applied': 873,\n",
       " 'profits': 874,\n",
       " 'medical': 875,\n",
       " 'done': 876,\n",
       " 'advanced': 877,\n",
       " 'athletes': 878,\n",
       " 'bhp': 879,\n",
       " 'billiton': 880,\n",
       " 'closely': 881,\n",
       " 'watched': 882,\n",
       " 'computer': 883,\n",
       " 'futures': 884,\n",
       " 'shooting': 885,\n",
       " 'cuts': 886,\n",
       " 'come': 887,\n",
       " 'roddick': 888,\n",
       " 'started': 889,\n",
       " 'palestinians': 890,\n",
       " 'mike': 891,\n",
       " 'pro': 892,\n",
       " 'grew': 893,\n",
       " 'relief': 894,\n",
       " 'homered': 895,\n",
       " 'judo': 896,\n",
       " 'defending': 897,\n",
       " 'compete': 898,\n",
       " 'looking': 899,\n",
       " 'sprinter': 900,\n",
       " 'torri': 901,\n",
       " 'auction': 902,\n",
       " 'technology': 903,\n",
       " 'lawsuit': 904,\n",
       " 'improvement': 905,\n",
       " 'ltd': 906,\n",
       " 'accounting': 907,\n",
       " 'effective': 908,\n",
       " 'inflationary': 909,\n",
       " 'force': 910,\n",
       " 'area': 911,\n",
       " 'dead': 912,\n",
       " 'fired': 913,\n",
       " 'pick': 914,\n",
       " 'asian': 915,\n",
       " 'clashes': 916,\n",
       " 'iran': 917,\n",
       " 'crisis': 918,\n",
       " 'secretary': 919,\n",
       " 'hoping': 920,\n",
       " 'den': 921,\n",
       " 'associated': 922,\n",
       " 'outside': 923,\n",
       " 'guard': 924,\n",
       " 'cents': 925,\n",
       " 'jobs': 926,\n",
       " 'reduced': 927,\n",
       " 'others': 928,\n",
       " 'spitz': 929,\n",
       " 'hotel': 930,\n",
       " 'lines': 931,\n",
       " 'jersey': 932,\n",
       " 'closer': 933,\n",
       " 'convention': 934,\n",
       " 'afghan': 935,\n",
       " 'supply': 936,\n",
       " 'nikkei': 937,\n",
       " 'knocked': 938,\n",
       " 'holiest': 939,\n",
       " 'enough': 940,\n",
       " 'uk': 941,\n",
       " 'chance': 942,\n",
       " 'annual': 943,\n",
       " 'information': 944,\n",
       " 'died': 945,\n",
       " 'average': 946,\n",
       " 'firms': 947,\n",
       " 'march': 948,\n",
       " 'northern': 949,\n",
       " 'find': 950,\n",
       " 'sept': 951,\n",
       " 'voted': 952,\n",
       " 'cincinnati': 953,\n",
       " 'events': 954,\n",
       " 'increased': 955,\n",
       " 'line': 956,\n",
       " 'arrested': 957,\n",
       " 'refused': 958,\n",
       " 'rebound': 959,\n",
       " 'terrorist': 960,\n",
       " 'settlements': 961,\n",
       " 'moved': 962,\n",
       " 'picked': 963,\n",
       " 'knew': 964,\n",
       " 'explosion': 965,\n",
       " 'chip': 966,\n",
       " 'process': 967,\n",
       " 'track': 968,\n",
       " 'corporate': 969,\n",
       " 'singapore': 970,\n",
       " 'decline': 971,\n",
       " 'mobile': 972,\n",
       " 'conspiracy': 973,\n",
       " 'allowed': 974,\n",
       " 'sun': 975,\n",
       " 'rebel': 976,\n",
       " 'kong': 977,\n",
       " 'wife': 978,\n",
       " 'louis': 979,\n",
       " 'changed': 980,\n",
       " 'tie': 981,\n",
       " 'draw': 982,\n",
       " 'houston': 983,\n",
       " 'ninth': 984,\n",
       " 'hitter': 985,\n",
       " 'bay': 986,\n",
       " 'figures': 987,\n",
       " 'offer': 988,\n",
       " 'jose': 989,\n",
       " 'panel': 990,\n",
       " 'crash': 991,\n",
       " 'hold': 992,\n",
       " 'vault': 993,\n",
       " 'minnesota': 994,\n",
       " 'suspension': 995,\n",
       " 'surgery': 996,\n",
       " 'orioles': 997,\n",
       " 'rangers': 998,\n",
       " 'swimmer': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(data,word_id_dict):\n",
    "    X_uni_tr = data\n",
    "    X_tr = []\n",
    "    for i in range(len(X_uni_tr)):\n",
    "        list_a = []\n",
    "        for word in X_uni_tr[i]:\n",
    "            if word in word_id_dict:\n",
    "                word_id = word_id_dict[word]\n",
    "            else:\n",
    "                pass\n",
    "            list_a.append(word_id)\n",
    "        X_tr.append(list_a)\n",
    "    return X_uni_tr,X_tr\n",
    "# represent train set\n",
    "X_uni_tr,X_tr = create_index(ngrams_without_Ded_tr,word_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuters',\n",
       " 'venezuelans',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'early',\n",
       " 'large',\n",
       " 'numbers',\n",
       " 'sunday',\n",
       " 'vote',\n",
       " 'historic',\n",
       " 'referendum',\n",
       " 'either',\n",
       " 'remove',\n",
       " 'left',\n",
       " 'wing',\n",
       " 'president',\n",
       " 'hugo',\n",
       " 'chavez',\n",
       " 'office',\n",
       " 'give',\n",
       " 'him',\n",
       " 'new',\n",
       " 'mandate',\n",
       " 'govern',\n",
       " 'next',\n",
       " 'two',\n",
       " 'years']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_uni_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1011,\n",
       " 758,\n",
       " 28,\n",
       " 208,\n",
       " 1103,\n",
       " 1367,\n",
       " 29,\n",
       " 308,\n",
       " 816,\n",
       " 262,\n",
       " 1586,\n",
       " 2704,\n",
       " 108,\n",
       " 759,\n",
       " 35,\n",
       " 172,\n",
       " 175,\n",
       " 493,\n",
       " 701,\n",
       " 97,\n",
       " 4,\n",
       " 1221,\n",
       " 2203,\n",
       " 173,\n",
       " 10,\n",
       " 63]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uni_dev,X_dev = create_index(ngrams_without_Ded_dev,word_id_dict)\n",
    "X_uni_test,X_test = create_index(ngrams_without_Ded_test,word_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W^T) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i^T $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n",
    "\n",
    "\n",
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers (for the Bonus). Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_clusses`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "See the examples below for expected outputs. Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:10.086665Z",
     "start_time": "2020-04-02T15:09:10.083429Z"
    }
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.5):\n",
    "    dict_num_list = hidden_dim\n",
    "    dict_num_list.insert(0,vocab_size)\n",
    "    dict_num_list.insert(1,embedding_dim)\n",
    "    dict_num_list.append(num_classes)\n",
    "    W={}\n",
    "    for i in range(len(dict_num_list)):\n",
    "        if i == len(dict_num_list)-1:\n",
    "            break\n",
    "        else:\n",
    "            np.random.seed(2020)\n",
    "            W[i] = np.random.uniform(-init_val,init_val,(dict_num_list[i],dict_num_list[i+1])).astype('float32')\n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.143236Z",
     "start_time": "2020-04-02T14:26:48.139381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_emb: (5, 10)\n",
      "W_out: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=5,embedding_dim=10,hidden_dim=[], num_classes=2)\n",
    "\n",
    "print('W_emb:', W[0].shape)\n",
    "print('W_out:', W[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    }
   },
   "outputs": [],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[2], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:49.086112Z",
     "start_time": "2020-04-02T14:26:49.082225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_emb: (3, 4)\n",
      "W_h1: (4, 2)\n",
      "W_out: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print('W_emb:', W[0].shape)\n",
    "print('W_h1:', W[1].shape)\n",
    "print('W_out:', W[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48627684,  0.37339196,  0.00974553, -0.22816429],\n",
       "       [-0.16308127, -0.28304574, -0.22352286, -0.15668441],\n",
       "       [ 0.36215892, -0.34330034, -0.35911277,  0.2570803 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:50.504086Z",
     "start_time": "2020-04-02T14:26:50.500686Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sig = (np.exp(z).T/np.sum(np.exp(z),axis=1)).T\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    loss = -np.log(y_preds[y])\n",
    "    #l2_regularization = (alpha/2)*(np.sum(np.square(weights)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.762676Z",
     "start_time": "2020-04-02T14:26:51.758210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_preds:  [0.01217919 0.27035308 0.24462558 0.02710529 0.44573687]\n",
      "loss: 1.40802648485675\n"
     ]
    }
   ],
   "source": [
    "# example for 5 classes\n",
    "\n",
    "y = 2 #true label\n",
    "y_preds = softmax(np.array([[-2.1,1.,0.9,-1.3,1.5]]))[0]\n",
    "\n",
    "print('y_preds: ',y_preds)\n",
    "print('loss:', categorical_loss(y, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network (during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{relu_derivative}(z_i)=\\begin{cases}\n",
    "    0, & \\text{if $z_i<=0$}.\\\\\n",
    "    1, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    }
   },
   "outputs": [],
   "source": [
    "#def relu(z):\n",
    "#    return z*(z>0)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z>0)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    }
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    dropout_vec = np.ones(size)\n",
    "    num = int(size*dropout_rate)\n",
    "    dropout_vec[:num] = 0\n",
    "    np.random.shuffle(dropout_vec)\n",
    "    return dropout_vec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 1. 1. 1. 0. 1. 1. 1.]\n",
      "[0. 1. 1. 1. 0. 1. 1. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    out_vals = {}\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "    \n",
    "    x_vecs = [W[0][x_num] for x_num in x]\n",
    "    # h0 = 1/x * sum.(list_vec)\n",
    "    h0 = np.expand_dims(1/len(x)*np.sum(x_vecs,axis = 0).T,axis = 0) # (,4) -> (1,4)\n",
    "    a0 = relu(h0)\n",
    "    d0 = dropout_mask(a0.shape[1],dropout_rate)\n",
    "    #output_0 = a0*d0\n",
    "    output_0 = (a0*d0)/dropout_rate\n",
    "    \n",
    "    # add h, a, dropout array to list\n",
    "    h_vecs.append(h0.squeeze())\n",
    "    a_vecs.append(a0.squeeze())\n",
    "    dropout_vecs.append(d0.squeeze())\n",
    "    \n",
    "    if len(W) == 2:\n",
    "        y = softmax(output_0@W[2])\n",
    "    else:\n",
    "        output = output_0\n",
    "        for i in range(len(W)):\n",
    "            h = output@W[i+1]\n",
    "            a = relu(h)\n",
    "            d = dropout_mask(a.shape[1],dropout_rate)\n",
    "            #output = a*d\n",
    "            output = (a*d)/dropout_rate\n",
    "            # add h, a, dropout array to list\n",
    "            h_vecs.append(h.squeeze())\n",
    "            a_vecs.append(a.squeeze())\n",
    "            dropout_vecs.append(d.squeeze())\n",
    "            \n",
    "            if i == len(W)-3:\n",
    "                break\n",
    "        y = softmax(output@W[len(W)-1])\n",
    "    \n",
    "    # output result to dictionary\n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['dropout_vecs'] = dropout_vecs\n",
    "    out_vals['y'] = y.squeeze()\n",
    "   \n",
    "    return out_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (3, 4)\n",
      "Shape W1 (4, 5)\n",
      "Shape W2 (5, 2)\n",
      "\n",
      "{'h': [array([ 0.09953883, -0.31317306, -0.29131782,  0.05019794], dtype=float32), array([ 0.01674634, -0.02840193,  0.00616702, -0.0377309 , -0.01809771])], 'a': [array([0.09953883, 0.        , 0.        , 0.05019794], dtype=float32), array([0.01674634, 0.        , 0.00616702, 0.        , 0.        ])], 'dropout_vecs': [array([0., 0., 1., 1.]), array([0., 1., 1., 1., 0.])], 'y': array([0.50036991, 0.49963009])}\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[5], num_classes=2)\n",
    " \n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "print()\n",
    "print(forward_pass([2,1], W, dropout_rate=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x = [2,1]\n",
    "dropout_rate = 0.5\n",
    "list_vecs = [W[0][i] for i in x]\n",
    "h0 = np.expand_dims(1/len(x)*np.sum(list_vecs,axis = 0).T,axis = 0) # (,4) -> (1,4)\n",
    "a0 = relu(h0)\n",
    "dropout_0 = dropout_mask(a0.shape[1],dropout_rate)\n",
    "output_0 = a0*dropout_0\n",
    "h1 = output_0@W[1] # (1,4)@(4,5) = (1,5)\n",
    "a1 = relu(h1)\n",
    "dropout_1 = dropout_mask(a1.shape[1],dropout_rate)\n",
    "output_1 = a1*dropout_1\n",
    "y = softmax(output_1@W[2]) # (1,5)@(5,2) = (1,2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:55.162854Z",
     "start_time": "2020-04-02T14:26:55.156122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (3, 4)\n",
      "Shape W1 (4, 5)\n",
      "Shape W2 (5, 2)\n",
      "\n",
      "{'h': [array([-0.04668263, -0.12518334,  0.17532286, -0.32932055], dtype=float32), array([0., 0., 0., 0., 0.])], 'a': [array([0.        , 0.        , 0.17532286, 0.        ], dtype=float32), array([0., 0., 0., 0., 0.])], 'dropout_vec': [array([1., 0., 0., 1.]), array([0., 0., 1., 1., 1.])], 'y': array([0.5, 0.5])}\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[5], num_classes=2)\n",
    " \n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "print()\n",
    "print(forward_pass([2,1], W, dropout_rate=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and update the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4,5])\n",
    "b = np.array([1,2,3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  9, 16, 25])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:56.225630Z",
     "start_time": "2020-04-02T14:26:56.216508Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n",
    "    \n",
    "    \n",
    "    return W\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, print_progress=True):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return W, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    }
   },
   "outputs": [],
   "source": [
    "W = network_weights(vocab_size=len(vocab),embedding_dim=300,hidden_dim=[], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr, Y_tr,\n",
    "                            W,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.01,\n",
    "                            epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in vocab:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:28:54.548613Z",
     "start_time": "2020-04-02T14:27:32.780248Z"
    }
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:30:11.121198Z",
     "start_time": "2020-04-02T14:29:24.946124Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures (Bonus)\n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  |   |   |   |   |\n",
    "| Average Embedding (Pre-trained)  |   |   |   |   |\n",
    "| Average Embedding (Pre-trained) + X hidden layers (BONUS)   |   |   |   |   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
