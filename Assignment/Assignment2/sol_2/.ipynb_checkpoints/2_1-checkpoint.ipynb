{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [COM4513-6513] Assignment 2: Text Classification with a Feedforward Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.read_csv('./data_topic/train.csv',header=None,names=[\"label\",\"text\"])\n",
    "dev_data=pd.read_csv('./data_topic/dev.csv',header=None,names=[\"label\",\"text\"])\n",
    "test_data=pd.read_csv('./data_topic/test.csv',header=None,names=[\"label\",\"text\"])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Make the raw texts into lists and their corresponding labels into  np.arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.753874Z",
     "start_time": "2020-04-02T14:26:39.749647Z"
    }
   },
   "outputs": [],
   "source": [
    "def creat_list_array(data_text,data_label):\n",
    "    x=data_text.tolist()\n",
    "    y=np.array(data_label)\n",
    "    return x,y\n",
    "def lower_F(data):\n",
    "    lower_list=[]\n",
    "    for i in range(len(data)):\n",
    "        lower_data=str.lower(data[i])\n",
    "        lower_list.append(lower_data)\n",
    "    return lower_list\n",
    "\n",
    "# Transform train data\n",
    "data_train_x_raw,data_train_y=creat_list_array(train_data['text'],train_data['label']) #len 2400\n",
    "# Transform validation data\n",
    "data_dev_x_raw,data_dev_y=creat_list_array(dev_data['text'],dev_data['label']) #len 150\n",
    "# Transform test data\n",
    "data_test_x_raw,data_test_y=creat_list_array(test_data['text'],test_data['label']) #len 900\n",
    "\n",
    "# lower data\n",
    "data_train_x=lower_F(data_train_x_raw)\n",
    "data_dev_x=lower_F(data_dev_x_raw)\n",
    "data_test_x=lower_F(data_test_x_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:41.505459Z",
     "start_time": "2020-04-02T14:26:41.498388Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']\n",
    "\n",
    "# tokenise, create unigrams, using stop-words\n",
    "def tokenise(data,token_pattern,stop_words):\n",
    "    token_data=[]\n",
    "    token_list=re.findall(token_pattern,data)\n",
    "    for word in token_list:\n",
    "        if word not in stop_words:\n",
    "            token_data.append(word)\n",
    "    return token_data\n",
    "\n",
    "# based on the tokenised data(unigrams), create bigrams or trigrams\n",
    "def ngrams_generate(data,n):\n",
    "    result_list=[]\n",
    "    ngrams = zip(*[data[i:] for i in range(n)])\n",
    "    for ngram in ngrams:\n",
    "        result_list.append((ngram))\n",
    "    return result_list\n",
    "\n",
    "# extract ngrams function\n",
    "def extract_ngrams(x_raw,ngram_range=(1,3),token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',stop_words=[],vocab=set()):\n",
    "    # tokenise data\n",
    "    token_data=tokenise(x_raw,token_pattern=token_pattern,stop_words=stop_words)\n",
    "    # create ngrams list which save ngrams result\n",
    "    result_ngrams=[]\n",
    "    result_vocab=[]\n",
    "    # Extract ngrams based on the ngram_range\n",
    "    if ngram_range == 1:\n",
    "        result_ngrams = token_data\n",
    "    elif ngram_range[0]==1:\n",
    "        result_ngrams=token_data\n",
    "        for i in range(ngram_range[0],ngram_range[1]):\n",
    "            ngrams=ngrams_generate(token_data,i+1)\n",
    "            result_ngrams=result_ngrams+ngrams\n",
    "    else:\n",
    "        result_ngrams=ngrams_generate(token_data,ngram_range[0])\n",
    "        for i in range(ngram_range[0],ngram_range[1]):\n",
    "            ngrams=ngrams_generate(token_data,ngram_range[0]+1)\n",
    "            result_ngrams=result_ngrams+ngrams\n",
    "    # Extract specific vocab based on the vocab set()\n",
    "    if len(vocab)==0:\n",
    "        return result_ngrams\n",
    "    else:\n",
    "        for word in vocab:\n",
    "            if word in result_ngrams:\n",
    "                result_vocab.append(word)\n",
    "        return result_vocab\n",
    "    \n",
    "# Extract ngrams on the complete data set, for dev and test sets\n",
    "def extract_ngrams_for_test(X_data,ngram_range,stop_words=stop_words):\n",
    "    # Extract ngrams from raw data\n",
    "    ngrams_list_without_Ded=[]\n",
    "    for i in range(len(X_data)):\n",
    "        ngrams_data=extract_ngrams(X_data[i],ngram_range=ngram_range,stop_words=stop_words)\n",
    "        ngrams_list_without_Ded.append(ngrams_data)\n",
    "    return ngrams_list_without_Ded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=0, keep_topN=0, stop_words=[]):\n",
    "    ngrams_list = []\n",
    "    ngrams_list_without_Ded = []\n",
    "    for i in range(len(X_raw)):\n",
    "        ngrams_data=extract_ngrams(X_raw[i],ngram_range=ngram_range,stop_words=stop_words)\n",
    "        ngrams_list_without_Ded.append(ngrams_data)\n",
    "        # Deduplication\n",
    "        ngrams_data_Ded=sorted(set(ngrams_data),key=ngrams_data.index)\n",
    "        ngrams_list.append(ngrams_data_Ded)\n",
    "    \n",
    "    # create vocab dictionary\n",
    "    vocab_dict = {}\n",
    "    for i in range(len(ngrams_list)):\n",
    "        for word in ngrams_list[i]:\n",
    "            if word in vocab_dict:\n",
    "                vocab_dict[word]+=1\n",
    "            else:\n",
    "                vocab_dict[word]=1\n",
    "                \n",
    "    # keep ngrams with a minimun df\n",
    "    for word in list(vocab_dict.keys()):\n",
    "        if vocab_dict[word] < min_df:\n",
    "            del vocab_dict[word]\n",
    "    \n",
    "    # sorted then keep only topN\n",
    "    vocab_sorted=sorted(vocab_dict.items(),key=lambda item:item[1],reverse=True)\n",
    "    if keep_topN == 0:\n",
    "        vocab_topN = vocab_sorted\n",
    "    else:\n",
    "        vocab_topN = vocab_sorted[:keep_topN]\n",
    "        \n",
    "    vocab = []\n",
    "    for i in range(len(vocab_topN)):\n",
    "        vocab.append(vocab_topN[i][0])\n",
    "        \n",
    "    return vocab,vocab_topN,ngrams_list_without_Ded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract vocab, df——train, train token data\n",
    "vocab_all,df_tr,ngrams_without_Ded_tr=get_vocab(data_train_x,ngram_range=(1),  \n",
    "                                            min_df=0,keep_topN=0, stop_words=stop_words)\n",
    "# extract dev token\n",
    "ngrams_without_Ded_dev=extract_ngrams_for_test(data_dev_x,ngram_range=(1),stop_words=stop_words)\n",
    "# extract test token\n",
    "ngrams_without_Ded_test=extract_ngrams_for_test(data_test_x,ngram_range=(1),stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab_all[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "\n",
      "['report', 'system', 'led', 'appealed', 'starts', 'republic', 'costs', 'brian', 'narrower', 'blamed', 'kuwait', 'october', 'official', 'credit', 'vijay', 'drew', 'lanka', 'numbers', 'jimmy', 'retail', 'congolese', 'ap', 'outfielder', 'missed', 'wondering', 'kosuke', 'press', 'one', 'believes', 'missiles', 'protect', 'began', 'because', 'opposition', 'fireworks', 'failed', 'outlook', 'minister', 'oakland', 'looks', 'gatumba', 'ditch', 'finished', 'backpacker', 'former', 'violent', 'increasing', 'bundled', 'buyers', 'hearing', 'previously', 'medley', 'invitational', 'decades', 'blow', 'percent', 'boy', 'customer', 'successive', 'leg', 'consortium', 'gunfire', 'dark', 'often', 'medics', 'occupied', 'owen', 'delhi', 'plotting', 'champions', 'remaining', 'iraqi', 'finances', 'blair', 'geneva', 'conference', 'karzai', 'raw', 'surging', 'forecast', 'remote', 'law', 'homeless', 'wake', 'trial', 'imported', 'billions', 'protesters', 'arabia', 'prescription', 'shares', 'republican', 'atlantic', 'take', 'athlete', 'tore', 'home', 'boys', 'knocked', 'beginning']\n",
      "\n",
      "[('reuters', 631), ('said', 432), ('tuesday', 413), ('wednesday', 344), ('new', 325), ('after', 295), ('ap', 275), ('athens', 245), ('monday', 221), ('first', 210)]\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print()\n",
    "print(random.sample(vocab,100))\n",
    "print()\n",
    "print(df_tr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dict(vocab):\n",
    "    id_word_dict = {}\n",
    "    word_id_dict = {}\n",
    "    for i in range(len(vocab)):\n",
    "        id_word_dict[i] = vocab[i]\n",
    "        word_id_dict[vocab[i]] = i\n",
    "    return id_word_dict,word_id_dict\n",
    "id_word_dict,word_id_dict = create_dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(data,word_id_dict):\n",
    "    X_uni_tr = data\n",
    "    X_tr = []\n",
    "    for i in range(len(X_uni_tr)):\n",
    "        list_a = []\n",
    "        for word in X_uni_tr[i]:\n",
    "            if word in word_id_dict:\n",
    "                word_id = word_id_dict[word]\n",
    "            else:\n",
    "                pass\n",
    "            list_a.append(word_id)\n",
    "        X_tr.append(list_a)\n",
    "    return X_uni_tr,X_tr\n",
    "# represent train set\n",
    "X_uni_tr,X_tr = create_index(ngrams_without_Ded_tr,word_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuters',\n",
       " 'rwandan',\n",
       " 'troops',\n",
       " 'airlifted',\n",
       " 'sunday',\n",
       " 'sudan',\n",
       " 'darfur',\n",
       " 'first',\n",
       " 'foreign',\n",
       " 'force',\n",
       " 'mandated',\n",
       " 'protect',\n",
       " 'observers',\n",
       " 'monitoring',\n",
       " 'cease',\n",
       " 'fire',\n",
       " 'between',\n",
       " 'sudanese',\n",
       " 'government',\n",
       " 'rebels',\n",
       " 'troubled',\n",
       " 'western',\n",
       " 'region']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_uni_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1011,\n",
       " 758,\n",
       " 28,\n",
       " 208,\n",
       " 1103,\n",
       " 1367,\n",
       " 29,\n",
       " 308,\n",
       " 816,\n",
       " 262,\n",
       " 1586,\n",
       " 2704,\n",
       " 108,\n",
       " 759,\n",
       " 35,\n",
       " 172,\n",
       " 175,\n",
       " 493,\n",
       " 701,\n",
       " 97,\n",
       " 4,\n",
       " 1221,\n",
       " 2203,\n",
       " 173,\n",
       " 10,\n",
       " 63]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dev and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uni_dev,X_dev = create_index(ngrams_without_Ded_dev,word_id_dict)\n",
    "X_uni_test,X_test = create_index(ngrams_without_Ded_test,word_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:10.086665Z",
     "start_time": "2020-04-02T15:09:10.083429Z"
    }
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.5):\n",
    "    dict_num_list = hidden_dim\n",
    "    dict_num_list.insert(0,vocab_size)\n",
    "    dict_num_list.insert(1,embedding_dim)\n",
    "    dict_num_list.append(num_classes)\n",
    "    W={}\n",
    "    for i in range(len(dict_num_list)):\n",
    "        if i == len(dict_num_list)-1:\n",
    "            break\n",
    "        else:\n",
    "            np.random.seed(2020)\n",
    "            W[i] = np.random.uniform(-init_val,init_val,(dict_num_list[i],dict_num_list[i+1])).astype('float32')\n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.143236Z",
     "start_time": "2020-04-02T14:26:48.139381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_emb: (5, 10)\n",
      "W_out: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=5,embedding_dim=10,hidden_dim=[], num_classes=2)\n",
    "\n",
    "print('W_emb:', W[0].shape)\n",
    "print('W_out:', W[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8811052dc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W_emb:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W_h1:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W_out:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'network_weights' is not defined"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[2], num_classes=2)\n",
    "                    \n",
    "print('W_emb:', W[0].shape)\n",
    "print('W_h1:', W[1].shape)\n",
    "print('W_out:', W[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:50.504086Z",
     "start_time": "2020-04-02T14:26:50.500686Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    sig = (np.exp(z).T/np.sum(np.exp(z),axis=1)).T\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    loss = -np.log(y_preds[y])\n",
    "    #l2_regularization = (alpha/2)*(np.sum(np.square(weights)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.762676Z",
     "start_time": "2020-04-02T14:26:51.758210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_preds:  [0.01217919 0.27035308 0.24462558 0.02710529 0.44573687]\n",
      "loss: 1.40802648485675\n"
     ]
    }
   ],
   "source": [
    "# example for 5 classes\n",
    "\n",
    "y = 2 #true label\n",
    "y_preds = softmax(np.array([[-2.1,1.,0.9,-1.3,1.5]]))[0]\n",
    "\n",
    "print('y_preds: ',y_preds)\n",
    "print('loss:', categorical_loss(y, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu and relu_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    }
   },
   "outputs": [],
   "source": [
    "#def relu(z):\n",
    "#    return z*(z>0)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z>0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    }
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    dropout_vec = np.ones(size)\n",
    "    num = int(size*dropout_rate)\n",
    "    dropout_vec[:num] = 0\n",
    "    np.random.shuffle(dropout_vec)\n",
    "    return dropout_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 0. 0. 1. 1.]\n",
      "[1. 1. 1. 0. 1. 1. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    out_vals = {}\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "    \n",
    "    x_vecs = [W[0][x_num] for x_num in x]\n",
    "    # h0 = 1/x * sum.(list_vec)\n",
    "    h0 = np.expand_dims(1/len(x)*np.sum(x_vecs,axis = 0).T,axis = 0) # (,4) -> (1,4)\n",
    "    a0 = relu(h0)\n",
    "    d0 = dropout_mask(a0.shape[1],dropout_rate)\n",
    "    #output_0 = a0*d0\n",
    "    output_0 = (a0*d0)/dropout_rate\n",
    "    \n",
    "    # add h, a, dropout array to list\n",
    "    h_vecs.append(h0.squeeze())\n",
    "    a_vecs.append(a0.squeeze())\n",
    "    dropout_vecs.append(d0.squeeze())\n",
    "    \n",
    "    if len(W) == 2:\n",
    "        y = softmax(output_0@W[1])\n",
    "    else:\n",
    "        output = output_0\n",
    "        for i in range(len(W)):\n",
    "            h = output@W[i+1]\n",
    "            a = relu(h)\n",
    "            d = dropout_mask(a.shape[1],dropout_rate)\n",
    "            #output = a*d\n",
    "            output = (a*d)/dropout_rate\n",
    "            # add h, a, dropout array to list\n",
    "            h_vecs.append(h.squeeze())\n",
    "            a_vecs.append(a.squeeze())\n",
    "            dropout_vecs.append(d.squeeze())\n",
    "            \n",
    "            if i == len(W)-3:\n",
    "                break\n",
    "        y = softmax(output@W[len(W)-1])\n",
    "    \n",
    "    # output result to dictionary\n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['dropout_vecs'] = dropout_vecs\n",
    "    out_vals['y'] = y.squeeze()\n",
    "   \n",
    "    return out_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (3, 4)\n",
      "Shape W1 (4, 5)\n",
      "Shape W2 (5, 2)\n",
      "\n",
      "{'h': [array([ 0.09953883, -0.31317306, -0.29131782,  0.05019794], dtype=float32), array([ 0.01674634, -0.02840193,  0.00616702, -0.0377309 , -0.01809771])], 'a': [array([0.09953883, 0.        , 0.        , 0.05019794], dtype=float32), array([0.01674634, 0.        , 0.00616702, 0.        , 0.        ])], 'dropout_vecs': [array([0., 0., 1., 1.]), array([0., 1., 1., 1., 0.])], 'y': array([0.50036991, 0.49963009])}\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[5], num_classes=2)\n",
    " \n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "print()\n",
    "print(forward_pass([2,1], W, dropout_rate=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_x(x,vocab_len):\n",
    "    result = []\n",
    "    for i in range(vocab_len):\n",
    "        if i in x:\n",
    "            a = 1\n",
    "            result.append(a)\n",
    "        else:\n",
    "            a = 0\n",
    "            result.append(a)\n",
    "    return result\n",
    "\n",
    "def one_hot_y(y,class_num):\n",
    "    a=np.eye(class_num+1)[y]\n",
    "    return np.delete(a,0,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:56.225630Z",
     "start_time": "2020-04-02T14:26:56.216508Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1 \n",
    "def backward_pass(x, y, W, out_vals, dropout_rate, lr=0.001, freeze_emb=False):\n",
    "    W_new = W.copy()\n",
    "    W_num = len(W)\n",
    "    \n",
    "    W_first = W[0].shape\n",
    "    vocab_size = W_first[0]\n",
    "    \n",
    "    W_last = W[list(W.keys())[-1]].shape\n",
    "    class_num = W_last[1]\n",
    "    \n",
    "    x_onehot = one_hot_x(x,vocab_size)\n",
    "    y_onehot = one_hot_y(y,class_num)\n",
    "    p = out_vals['y']\n",
    "    \n",
    "    # first\n",
    "    g = np.expand_dims(p - y,axis = 0)\n",
    "    out = np.expand_dims(out_vals['a'][W_num-2] * out_vals['dropout_vecs'][W_num-2],axis = 0)\n",
    "    dw = out.T @ g # [5,2]\n",
    "    W_new[W_num-1] = W_new[W_num-1] - lr*dw\n",
    "    d_out = ((g @ W[W_num-1].T) * out_vals['dropout_vecs'][W_num-2])/dropout_rate\n",
    "    #d_out = (g @ W[W_num-1].T) * out_vals['dropout_vecs'][W_num-2]\n",
    "    \n",
    "    if W_num == 2:\n",
    "        if freeze_emb == False:\n",
    "            g0 = d_out * relu_derivative(out_vals['h'][0])\n",
    "            dw0 = np.expand_dims(np.array(x_onehot),axis = 1) @ g0\n",
    "            W_new[0] = W_new[0] - lr*dw0\n",
    "\n",
    "        elif freeze_emb == True:\n",
    "            pass\n",
    "    else:\n",
    "        for i in range(W_num-2):\n",
    "            g = d_out * relu_derivative(out_vals['h'][W_num-2-i])\n",
    "            out = np.expand_dims(out_vals['a'][W_num-3-i] * out_vals['dropout_vecs'][W_num-3-i],axis = 0)\n",
    "            dw = out.T @ g\n",
    "            W_new[W_num-2-i] = W_new[W_num-2-i] - lr*dw\n",
    "            d_out = ((g @ W[W_num-2-i].T)*out_vals['dropout_vecs'][W_num-3-i])/dropout_rate\n",
    "            #d_out = (g @ W[W_num-2-i].T)*out_vals['dropout_vecs'][W_num-3-i]\n",
    "            \n",
    "        if freeze_emb == False:\n",
    "            g0 = d_out * relu_derivative(out_vals['h'][0])\n",
    "            dw0 = np.expand_dims(np.array(x_onehot),axis = 1) @ g0\n",
    "            W_new[0] = W_new[0] - lr*dw0\n",
    "\n",
    "        elif freeze_emb == True:\n",
    "            pass\n",
    "        \n",
    "    return W_new\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def backward_pass(x, y, W, out_vals, dropout_rate, lr=0.001, freeze_emb=False):\n",
    "    W_num = len(W)\n",
    "    W_first = W[0].shape\n",
    "    vocab_size = W_first[0]\n",
    "    \n",
    "    W_last = W[list(W.keys())[-1]].shape\n",
    "    class_num = W_last[1]\n",
    "    \n",
    "    x_onehot = one_hot_x(x,vocab_size)\n",
    "    y_onehot = one_hot_y(y,class_num)\n",
    "    p = out_vals['y']\n",
    "    \n",
    "    # first\n",
    "    g = np.expand_dims(p - y,axis = 0)\n",
    "    out = np.expand_dims(out_vals['a'][W_num-2] * out_vals['dropout_vecs'][W_num-2],axis = 0)\n",
    "    dw = out.T @ np.float32(g) # [5,2]\n",
    "    W[W_num-1] = W[W_num-1] - lr*dw\n",
    "    d_out = ((g @ W[W_num-1].T) * out_vals['dropout_vecs'][W_num-2])/dropout_rate\n",
    "    #d_out = (g @ W[W_num-1].T) * out_vals['dropout_vecs'][W_num-2]\n",
    "    \n",
    "    if W_num == 2:\n",
    "        if freeze_emb == False:\n",
    "            g0 = d_out * relu_derivative(out_vals['h'][0])\n",
    "            dw0 = np.expand_dims(np.array(x_onehot),axis = 1) @ g0\n",
    "            W[0] = W[0] - lr*dw0\n",
    "\n",
    "        elif freeze_emb == True:\n",
    "            pass\n",
    "    else:\n",
    "        for i in range(W_num-2):\n",
    "            g = d_out * relu_derivative(out_vals['h'][W_num-2-i])\n",
    "            out = np.expand_dims(out_vals['a'][W_num-3-i] * out_vals['dropout_vecs'][W_num-3-i],axis = 0)\n",
    "            dw = out.T @ g\n",
    "            W[W_num-2-i] = W[W_num-2-i] - lr*dw\n",
    "            d_out = ((g @ W[W_num-2-i].T)*out_vals['dropout_vecs'][W_num-3-i])/dropout_rate\n",
    "            #d_out = (g @ W[W_num-2-i].T)*out_vals['dropout_vecs'][W_num-3-i]\n",
    "            \n",
    "        if freeze_emb == False:\n",
    "            g0 = d_out * relu_derivative(out_vals['h'][0])\n",
    "            dw0 = np.expand_dims(np.array(x_onehot),axis = 1) @ g0\n",
    "            W[0] = W[0] - lr*dw0\n",
    "\n",
    "        elif freeze_emb == True:\n",
    "            pass\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "def backward_pass(x, y, W, out_vals,lr=0.001, freeze_emb=False):\n",
    "    W_num = len(W)\n",
    "    \n",
    "    #y_array = np.zeros(W[list(W.keys())[-1]].shape[-1])\n",
    "    y_array = np.zeros([W[W_num-1].shape[1]])\n",
    "    y_array[y-1] = 1\n",
    "    temp = out_vals['a'][-1]*out_vals['dropout_vecs'][-1]\n",
    "    current_loss = np.float32((out_vals['y'] - y_array))\n",
    "    last_layer_gradient = np.dot(temp.reshape([W[W_num-1].shape[0],1]),current_loss.reshape([1,W[W_num-1].shape[1]]))\n",
    "    W[W_num-1] = W[W_num-1] - lr*last_layer_gradient\n",
    "    current_loss = np.dot(W[W_num-1],current_loss).reshape([W[W_num-1].shape[0],1])\n",
    "    layercount = len(W)-1\n",
    "    \n",
    "    while layercount>1 and freeze_emb ==False:\n",
    "        current_loss = current_loss*relu_derivative(out_vals['h'][layercount-1]).reshape([W[layercount].shape[0],1])\n",
    "        W_grad = np.dot((out_vals['a'][layercount-2]*out_vals['dropout_vecs'][layercount-2]).reshape([W[layercount-1].shape[0],1]),W[layercount-2])\n",
    "        W[layercount-1] = W[layercount-1] - lr*W_grad\n",
    "        current_loss = np.dot(W[layercount-1],current_loss).reshape([W[layercount-1].shape[0],1])\n",
    "        layercount = layercount-1\n",
    "    if freeze_emb ==False:\n",
    "        x_array = np.zeros([W[0].shape[0],1])\n",
    "        x_array[x] = 1\n",
    "        current_loss = current_loss*relu_derivative(out_vals['h'][0]).reshape([W[0].shape[1],1])\n",
    "        W_grad = np.dot(x_array,current_loss.T)\n",
    "        W[0] = W[0] - lr*W_grad\n",
    "    return W\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(data, y_true, W, dropout_rate):\n",
    "    loss_result = 0.0\n",
    "    for i in range(len(data)):\n",
    "        x = data[i]\n",
    "        prob_dict = forward_pass(x, W, dropout_rate)\n",
    "        prob_pred = prob_dict['y']\n",
    "        loss = categorical_loss(y_true[i]-1,prob_pred)\n",
    "        loss_result += loss\n",
    "        \n",
    "    return loss_result/(len(data))\n",
    "\n",
    "def randmoise_data(data,label):\n",
    "    index=[i for i in range(len(data))]\n",
    "    data_array = np.array(data)\n",
    "    random.shuffle(index)\n",
    "    data1=data_array[index]\n",
    "    label1=label[index]\n",
    "    return data1,label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, print_progress=True):\n",
    "    \n",
    "    cur_loss_tr = 1.\n",
    "    cur_loss_dev = 1.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    W_curr = W\n",
    "    for i in range(epochs):\n",
    "        cur_loss_dev = compute_loss(X_dev,Y_dev,W_curr,dropout_rate=dropout)\n",
    "        if print_progress==True:\n",
    "            validation_loss_history.append(cur_loss_dev) \n",
    "            \n",
    "        #shuffle train dataset\n",
    "        X_tr_ran,Y_tr_ran = randmoise_data(X_tr,Y_tr)\n",
    "        for n in range(len(X_tr_ran)):\n",
    "            out_vals = forward_pass(X_tr_ran[n], W_curr, dropout_rate=dropout)\n",
    "            W_curr = backward_pass(X_tr_ran[n], Y_tr_ran[n], W_curr, out_vals,dropout_rate=dropout,lr=lr,freeze_emb=freeze_emb)\n",
    "            \n",
    "        cur_loss_tr = compute_loss(X_tr_ran, Y_tr_ran, W_curr, dropout_rate=dropout)\n",
    "        if print_progress==True:\n",
    "            training_loss_history.append(cur_loss_tr) \n",
    "            \n",
    "        print('Epoch:{0}|Training Loss{1}|Validation Loss{2}'.format(i,cur_loss_tr,cur_loss_dev))\n",
    "        \n",
    "        # if diff smaller than tolerance then break iteration\n",
    "        if i==0:\n",
    "            pass\n",
    "        else:\n",
    "            #diff = previous validation loss − current validation loss\n",
    "            diff=validation_loss_history[-2]-cur_loss_dev\n",
    "            if abs(diff)<tolerance:\n",
    "                break\n",
    "    return W_curr, training_loss_history, validation_loss_history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, print_progress=True):\n",
    "    \n",
    "    cur_loss_tr = 1.\n",
    "    cur_loss_dev = 1.\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    W_curr = W\n",
    "    for i in range(epochs):\n",
    "        cur_loss_dev = compute_loss(X_dev,Y_dev,W_curr,dropout_rate=dropout)\n",
    "        if print_progress==True:\n",
    "            validation_loss_history.append(cur_loss_dev) \n",
    "            \n",
    "        #shuffle train dataset\n",
    "        X_tr_ran,Y_tr_ran = randmoise_data(X_tr,Y_tr)\n",
    "        for n in range(len(X_tr_ran)):\n",
    "            out_vals = forward_pass(X_tr_ran[n], W_curr, dropout_rate=dropout)\n",
    "            W_curr = backward_pass(X_tr_ran[n], Y_tr_ran[n], W_curr, out_vals,lr=lr,freeze_emb=freeze_emb)\n",
    "            \n",
    "        cur_loss_tr = compute_loss(X_tr_ran, Y_tr_ran, W_curr, dropout_rate=dropout)\n",
    "        if print_progress==True:\n",
    "            training_loss_history.append(cur_loss_tr) \n",
    "            \n",
    "        print('Epoch:{0}|Training Loss{1}|Validation Loss{2}'.format(i,cur_loss_tr,cur_loss_dev))\n",
    "        \n",
    "        # if diff smaller than tolerance then break iteration\n",
    "        if i==0:\n",
    "            pass\n",
    "        else:\n",
    "            #diff = previous validation loss − current validation loss\n",
    "            diff=validation_loss_history[-2]-cur_loss_dev\n",
    "            if abs(diff)<tolerance:\n",
    "                break\n",
    "    return W_curr, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0|Training Loss1.092803670097112|Validation Loss1.2372955300402892\n",
      "Epoch:1|Training Loss1.023120550919523|Validation Loss1.0463838767055296\n",
      "Epoch:2|Training Loss0.9572358353754123|Validation Loss0.9753945066596079\n",
      "Epoch:3|Training Loss0.9014522964248384|Validation Loss1.0380198981911153\n",
      "Epoch:4|Training Loss0.8518479853579349|Validation Loss0.9625816109792835\n",
      "Epoch:5|Training Loss0.8199724129508498|Validation Loss0.928956299699012\n",
      "Epoch:6|Training Loss0.7786143492002672|Validation Loss0.8306734188246467\n",
      "Epoch:7|Training Loss0.7597428097309898|Validation Loss0.834575478181543\n",
      "Epoch:8|Training Loss0.7259993507176548|Validation Loss0.8012532578251269\n",
      "Epoch:9|Training Loss0.6920639036467784|Validation Loss0.8093084969666376\n",
      "Epoch:10|Training Loss0.6758387704606004|Validation Loss0.7010034529768256\n",
      "Epoch:11|Training Loss0.6522882392637098|Validation Loss0.6497622190511457\n",
      "Epoch:12|Training Loss0.6263531233808146|Validation Loss0.680780040507636\n",
      "Epoch:13|Training Loss0.6118465197824523|Validation Loss0.6268700557512785\n",
      "Epoch:14|Training Loss0.5906241566557667|Validation Loss0.6997532636729247\n",
      "Epoch:15|Training Loss0.5699965699846867|Validation Loss0.6975108120666482\n",
      "Epoch:16|Training Loss0.5581888860299186|Validation Loss0.6521465244241046\n",
      "Epoch:17|Training Loss0.5431963214908521|Validation Loss0.6880961489264117\n",
      "Epoch:18|Training Loss0.539130272048609|Validation Loss0.5834654451391568\n",
      "Epoch:19|Training Loss0.5125271492724183|Validation Loss0.5996024874202652\n",
      "Epoch:20|Training Loss0.49955983671978577|Validation Loss0.5118562445031912\n",
      "Epoch:21|Training Loss0.4950103718515404|Validation Loss0.539989336622263\n",
      "Epoch:22|Training Loss0.48246010124318234|Validation Loss0.5415305618191342\n",
      "Epoch:23|Training Loss0.47409142317577924|Validation Loss0.5324495530507412\n",
      "Epoch:24|Training Loss0.4601284513062562|Validation Loss0.5183805341044213\n",
      "Epoch:25|Training Loss0.44750040883787917|Validation Loss0.507927329502077\n",
      "Epoch:26|Training Loss0.4433563142381702|Validation Loss0.5280192455302785\n",
      "Epoch:27|Training Loss0.43306630729848394|Validation Loss0.5123448841980761\n",
      "Epoch:28|Training Loss0.4224679086219184|Validation Loss0.591148516684914\n",
      "Epoch:29|Training Loss0.4070707774679059|Validation Loss0.4961289342699181\n"
     ]
    }
   ],
   "source": [
    "X = X_tr[:2000]\n",
    "Y = data_train_y[:2000]\n",
    "X_d = X_dev[:20]\n",
    "Y_d = data_dev_y[0:20]\n",
    "W = network_weights(vocab_size=len(vocab),embedding_dim=100,hidden_dim=[], num_classes=3)\n",
    "\n",
    "W_1, training_loss_history, validation_loss_history = SGD(X, Y, W, X_d,Y_d,lr=0.001,\\\n",
    "                                                        dropout=0.2, epochs=30, tolerance=0.0001, freeze_emb=False, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0|Training Loss1.0795401952919148|Validation Loss1.1982577008409576\n",
      "Epoch:1|Training Loss1.0176519518334153|Validation Loss1.1155264588119949\n",
      "Epoch:2|Training Loss0.9679001260696922|Validation Loss1.1119778865634027\n",
      "Epoch:3|Training Loss0.9189293604277594|Validation Loss1.0759394717601192\n",
      "Epoch:4|Training Loss0.8748979904090465|Validation Loss1.0375396082431818\n",
      "Epoch:5|Training Loss0.8489015709036999|Validation Loss1.009369252884199\n",
      "Epoch:6|Training Loss0.809540582485053|Validation Loss0.9699869347801869\n",
      "Epoch:7|Training Loss0.7726917124228524|Validation Loss0.9736472342373607\n",
      "Epoch:8|Training Loss0.7462820452891763|Validation Loss0.9744335607976861\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),embedding_dim=100,hidden_dim=[], num_classes=3)\n",
    "\n",
    "W_1, training_loss_history, validation_loss_history=SGD(X_tr, data_train_y, W, X_dev,data_dev_y,lr=0.001, \n",
    "        dropout=0.3, epochs=30, tolerance=0.001, freeze_emb=False, print_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    }
   },
   "outputs": [],
   "source": [
    "W = network_weights(vocab_size=len(vocab),embedding_dim=300,hidden_dim=[], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr, Y_tr,\n",
    "                            W,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.01,\n",
    "                            epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    }
   },
   "outputs": [],
   "source": [
    "def plt_loss(y1,y2,epochs):\n",
    "    x=np.linspace(1,epochs,epochs)\n",
    "    plt.title('Training Monitoring')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.ylim(0.2, 0.7)\n",
    "    plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\",label='training loss')\n",
    "    plt.plot(x, y2, color=\"red\", linewidth=1.0, linestyle=\"-\",label='validation loss')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(0.6, 0.95))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_loss(training_loss_history, validation_loss_history,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.1111111111111111\n",
      "Recall: 0.3333333333333333\n",
      "F1-Score: 0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/l/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  del sys.path[0]\n",
      "/Users/l/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n",
      "/Users/l/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W_1, dropout_rate=0.0)['y'])+1 for x,y in zip(X_test,data_test_y)]\n",
    "print('Accuracy:', accuracy_score(data_test_y,preds_te))\n",
    "print('Precision:', precision_score(data_test_y,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(data_test_y,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(data_test_y,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in vocab:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:28:54.548613Z",
     "start_time": "2020-04-02T14:27:32.780248Z"
    }
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:30:11.121198Z",
     "start_time": "2020-04-02T14:29:24.946124Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test\n",
    "data_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33555555555555555\n",
      "Precision: 0.3465338791854702\n",
      "Recall: 0.33555555555555555\n",
      "F1-Score: 0.32819034409358355\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y'])+1 for x,y in zip(X_test,data_test_y)]\n",
    "print('Accuracy:', accuracy_score(data_test_y,preds_te))\n",
    "print('Precision:', precision_score(data_test_y,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(data_test_y,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(data_test_y,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures (Bonus)\n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  |   |   |   |   |\n",
    "| Average Embedding (Pre-trained)  |   |   |   |   |\n",
    "| Average Embedding (Pre-trained) + X hidden layers (BONUS)   |   |   |   |   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
